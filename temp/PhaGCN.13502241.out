rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
error length < 2000bp
DTR_019451
error length < 2000bp
DTR_029350
error length < 2000bp
DTR_019424
error length < 2000bp
DTR_019335
error length < 2000bp
DTR_004245
error length < 2000bp
DTR_019506
error length < 2000bp
DTR_017136
error length < 2000bp
DTR_019421
error length < 2000bp
DTR_004161
error length < 2000bp
DTR_004120
error length < 2000bp
DTR_016434
error length < 2000bp
DTR_019457
error length < 2000bp
DTR_021738
error length < 2000bp
DTR_019547
error length < 2000bp
DTR_019556
error length < 2000bp
DTR_019420
error length < 2000bp
DTR_017087
error length < 2000bp
DTR_004117
error length < 2000bp
DTR_019483
error length < 2000bp
DTR_004113
error length < 2000bp
DTR_004119
error length < 2000bp
DTR_019370
error length < 2000bp
DTR_019623
error length < 2000bp
DTR_019372
error length < 2000bp
DTR_019449
error length < 2000bp
DTR_019333
error length < 2000bp
DTR_019471
error length < 2000bp
DTR_017137
error length < 2000bp
DTR_016447
error length < 2000bp
DTR_021663
error length < 2000bp
DTR_019681
error length < 2000bp
DTR_019446
error length < 2000bp
DTR_019294
error length < 2000bp
DTR_019480
error length < 2000bp
DTR_019321
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_117.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_30.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_410.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_411.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_43.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_438.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_439.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_44.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_440.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_45.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_46.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_51.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_57.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_588.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_589.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_590.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_591.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_592.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_593.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_594.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_595.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_596.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_597.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_598.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_599.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_600.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_601.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_602.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_603.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_604.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_605.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_606.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_608.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_610.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_653.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_654.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_848.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_932.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_965.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_975.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_976.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_039427
error length < 2000bp
DTR_039020
error length < 2000bp
DTR_043972
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_122.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_141.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_159.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_160.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_163.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_164.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_170.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_174.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_185.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_196.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_199.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_234.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_242.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_550.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_81.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_064416
error length < 2000bp
DTR_070494
error length < 2000bp
DTR_064534
error length < 2000bp
DTR_073593
error length < 2000bp
DTR_070880
error length < 2000bp
DTR_064525
error length < 2000bp
DTR_064575
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_506.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_509.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_510.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_513.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_821.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_843.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_904.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_506.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_091535
error length < 2000bp
DTR_083581
error length < 2000bp
DTR_083573
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_243.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_244.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_422.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_243.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_114097
error length < 2000bp
DTR_110349
error length < 2000bp
DTR_120893
error length < 2000bp
DTR_113012
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_350.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_587.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_655.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_884.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_350.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_129121
error length < 2000bp
DTR_140390
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_117.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_420.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_117.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_165674
error length < 2000bp
DTR_169911
error length < 2000bp
DTR_155951
error length < 2000bp
DTR_163363
error length < 2000bp
DTR_164983
error length < 2000bp
DTR_165577
error length < 2000bp
DTR_169909
error length < 2000bp
DTR_165573
error length < 2000bp
DTR_165672
error length < 2000bp
DTR_163182
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_254.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_549.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_554.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_616.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_621.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_622.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_624.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_625.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_918.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_919.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_254.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_180314
error length < 2000bp
DTR_174117
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_271.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_591.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_271.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_187762
error length < 2000bp
DTR_187760
error length < 2000bp
DTR_186114
error length < 2000bp
DTR_197989
error length < 2000bp
DTR_186115
error length < 2000bp
DTR_197543
error length < 2000bp
DTR_186108
error length < 2000bp
DTR_197545
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_157.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_158.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_575.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_576.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_585.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_90.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_91.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_92.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_208522
error length < 2000bp
DTR_217783
error length < 2000bp
DTR_217506
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_163.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_341.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_344.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_163.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_241250
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_753.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_753.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_247136
error length < 2000bp
DTR_250600
error length < 2000bp
DTR_254766
error length < 2000bp
DTR_249732
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_308.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_513.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_621.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_840.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_308.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.048s]
Loading sequences...  [0.387s]
Masking sequences...  [0.447s]
Writing sequences...  [0.063s]
Hashing sequences...  [0.023s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 0.988s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.053s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.027s]
Opening the output file...  [0.001s]
Loading query sequences...  [0.567s]
Masking queries...  [0.749s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.648s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.13s]
Masking reference...  [0.493s]
Initializing temporary storage...  [0.029s]
Building reference histograms...  [1.539s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.128s]
Building query seed array...  [0.165s]
Computing hash join...  [0.11s]
Building seed filter...  [0.015s]
Searching alignments...  [0.249s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.31s]
Building query seed array...  [0.224s]
Computing hash join...  [0.102s]
Building seed filter...  [0.012s]
Searching alignments...  [0.304s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.297s]
Building query seed array...  [0.243s]
Computing hash join...  [0.092s]
Building seed filter...  [0.013s]
Searching alignments...  [0.26s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.173s]
Building query seed array...  [0.205s]
Computing hash join...  [0.066s]
Building seed filter...  [0.008s]
Searching alignments...  [0.269s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.152s]
Building query seed array...  [0.199s]
Computing hash join...  [0.055s]
Building seed filter...  [0.014s]
Searching alignments...  [0.258s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.188s]
Building query seed array...  [0.196s]
Computing hash join...  [0.058s]
Building seed filter...  [0.022s]
Searching alignments...  [0.233s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.215s]
Computing hash join...  [0.06s]
Building seed filter...  [0.01s]
Searching alignments...  [0.28s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.206s]
Building query seed array...  [0.175s]
Computing hash join...  [0.067s]
Building seed filter...  [0.013s]
Searching alignments...  [0.236s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.174s]
Building query seed array...  [0.17s]
Computing hash join...  [0.091s]
Building seed filter...  [0.015s]
Searching alignments...  [0.302s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.167s]
Building query seed array...  [0.158s]
Computing hash join...  [0.079s]
Building seed filter...  [0.014s]
Searching alignments...  [0.278s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.242s]
Building query seed array...  [0.299s]
Computing hash join...  [0.153s]
Building seed filter...  [0.023s]
Searching alignments...  [0.267s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.283s]
Building query seed array...  [0.207s]
Computing hash join...  [0.122s]
Building seed filter...  [0.017s]
Searching alignments...  [0.279s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.137s]
Building query seed array...  [0.144s]
Computing hash join...  [0.089s]
Building seed filter...  [0.024s]
Searching alignments...  [0.288s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.217s]
Building query seed array...  [0.222s]
Computing hash join...  [0.072s]
Building seed filter...  [0.023s]
Searching alignments...  [0.34s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.32s]
Building query seed array...  [0.226s]
Computing hash join...  [0.088s]
Building seed filter...  [0.012s]
Searching alignments...  [0.259s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.169s]
Building query seed array...  [0.233s]
Computing hash join...  [0.114s]
Building seed filter...  [0.012s]
Searching alignments...  [0.256s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.179s]
Building query seed array...  [0.251s]
Computing hash join...  [0.066s]
Building seed filter...  [0.022s]
Searching alignments...  [0.27s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.161s]
Building query seed array...  [0.185s]
Computing hash join...  [0.084s]
Building seed filter...  [0.012s]
Searching alignments...  [0.276s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.304s]
Building query seed array...  [0.226s]
Computing hash join...  [0.085s]
Building seed filter...  [0.022s]
Searching alignments...  [0.228s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.206s]
Computing hash join...  [0.099s]
Building seed filter...  [0.023s]
Searching alignments...  [0.287s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.201s]
Building query seed array...  [0.193s]
Computing hash join...  [0.108s]
Building seed filter...  [0.023s]
Searching alignments...  [0.377s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.273s]
Building query seed array...  [0.246s]
Computing hash join...  [0.088s]
Building seed filter...  [0.014s]
Searching alignments...  [0.22s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.199s]
Building query seed array...  [0.371s]
Computing hash join...  [0.078s]
Building seed filter...  [0.011s]
Searching alignments...  [0.261s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.147s]
Building query seed array...  [0.109s]
Computing hash join...  [0.088s]
Building seed filter...  [0.023s]
Searching alignments...  [0.34s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.154s]
Building query seed array...  [0.208s]
Computing hash join...  [0.055s]
Building seed filter...  [0.009s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.185s]
Building query seed array...  [0.236s]
Computing hash join...  [0.08s]
Building seed filter...  [0.017s]
Searching alignments...  [0.21s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.212s]
Building query seed array...  [0.138s]
Computing hash join...  [0.067s]
Building seed filter...  [0.013s]
Searching alignments...  [0.276s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.171s]
Computing hash join...  [0.082s]
Building seed filter...  [0.012s]
Searching alignments...  [0.255s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.172s]
Building query seed array...  [0.142s]
Computing hash join...  [0.098s]
Building seed filter...  [0.017s]
Searching alignments...  [0.239s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.196s]
Building query seed array...  [0.223s]
Computing hash join...  [0.074s]
Building seed filter...  [0.011s]
Searching alignments...  [0.267s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.156s]
Building query seed array...  [0.215s]
Computing hash join...  [0.061s]
Building seed filter...  [0.012s]
Searching alignments...  [0.298s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.225s]
Building query seed array...  [0.138s]
Computing hash join...  [0.068s]
Building seed filter...  [0.015s]
Searching alignments...  [0.34s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.137s]
Building query seed array...  [0.24s]
Computing hash join...  [0.058s]
Building seed filter...  [0.013s]
Searching alignments...  [0.341s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.243s]
Building query seed array...  [0.181s]
Computing hash join...  [0.064s]
Building seed filter...  [0.019s]
Searching alignments...  [0.228s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.204s]
Building query seed array...  [0.326s]
Computing hash join...  [0.076s]
Building seed filter...  [0.016s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.184s]
Building query seed array...  [0.216s]
Computing hash join...  [0.07s]
Building seed filter...  [0.012s]
Searching alignments...  [0.261s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.111s]
Building query seed array...  [0.137s]
Computing hash join...  [0.088s]
Building seed filter...  [0.018s]
Searching alignments...  [0.206s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.208s]
Building query seed array...  [0.19s]
Computing hash join...  [0.091s]
Building seed filter...  [0.012s]
Searching alignments...  [0.201s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.203s]
Building query seed array...  [0.225s]
Computing hash join...  [0.09s]
Building seed filter...  [0.014s]
Searching alignments...  [0.205s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.16s]
Building query seed array...  [0.148s]
Computing hash join...  [0.077s]
Building seed filter...  [0.015s]
Searching alignments...  [0.282s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.172s]
Building query seed array...  [0.237s]
Computing hash join...  [0.057s]
Building seed filter...  [0.011s]
Searching alignments...  [0.255s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.201s]
Building query seed array...  [0.231s]
Computing hash join...  [0.076s]
Building seed filter...  [0.018s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.239s]
Building query seed array...  [0.237s]
Computing hash join...  [0.096s]
Building seed filter...  [0.016s]
Searching alignments...  [0.316s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.242s]
Computing hash join...  [0.065s]
Building seed filter...  [0.018s]
Searching alignments...  [0.286s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.183s]
Building query seed array...  [0.137s]
Computing hash join...  [0.091s]
Building seed filter...  [0.011s]
Searching alignments...  [0.203s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.14s]
Building query seed array...  [0.134s]
Computing hash join...  [0.08s]
Building seed filter...  [0.031s]
Searching alignments...  [0.319s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.236s]
Building query seed array...  [0.134s]
Computing hash join...  [0.11s]
Building seed filter...  [0.009s]
Searching alignments...  [0.258s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.161s]
Computing hash join...  [0.101s]
Building seed filter...  [0.017s]
Searching alignments...  [0.231s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.126s]
Building query seed array...  [0.117s]
Computing hash join...  [0.135s]
Building seed filter...  [0.016s]
Searching alignments...  [0.201s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.232s]
Building query seed array...  [0.15s]
Computing hash join...  [0.082s]
Building seed filter...  [0.022s]
Searching alignments...  [0.298s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.162s]
Building query seed array...  [0.263s]
Computing hash join...  [0.106s]
Building seed filter...  [0.022s]
Searching alignments...  [0.302s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.123s]
Building query seed array...  [0.18s]
Computing hash join...  [0.073s]
Building seed filter...  [0.014s]
Searching alignments...  [0.283s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.235s]
Building query seed array...  [0.175s]
Computing hash join...  [0.144s]
Building seed filter...  [0.023s]
Searching alignments...  [0.281s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.154s]
Building query seed array...  [0.197s]
Computing hash join...  [0.103s]
Building seed filter...  [0.023s]
Searching alignments...  [0.269s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.291s]
Computing hash join...  [0.09s]
Building seed filter...  [0.015s]
Searching alignments...  [0.247s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.185s]
Building query seed array...  [0.183s]
Computing hash join...  [0.091s]
Building seed filter...  [0.023s]
Searching alignments...  [0.241s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.117s]
Building query seed array...  [0.166s]
Computing hash join...  [0.099s]
Building seed filter...  [0.016s]
Searching alignments...  [0.276s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.234s]
Building query seed array...  [0.167s]
Computing hash join...  [0.084s]
Building seed filter...  [0.024s]
Searching alignments...  [0.255s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.221s]
Building query seed array...  [0.321s]
Computing hash join...  [0.07s]
Building seed filter...  [0.02s]
Searching alignments...  [0.239s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.174s]
Computing hash join...  [0.078s]
Building seed filter...  [0.014s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.201s]
Building query seed array...  [0.101s]
Computing hash join...  [0.12s]
Building seed filter...  [0.014s]
Searching alignments...  [0.255s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.176s]
Building query seed array...  [0.197s]
Computing hash join...  [0.066s]
Building seed filter...  [0.019s]
Searching alignments...  [0.28s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.141s]
Building query seed array...  [0.228s]
Computing hash join...  [0.062s]
Building seed filter...  [0.019s]
Searching alignments...  [0.24s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.146s]
Computing hash join...  [0.09s]
Building seed filter...  [0.02s]
Searching alignments...  [0.221s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.051s]
Computing alignments...  [8.251s]
Deallocating reference...  [0.001s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0s]
Deallocating queries...  [0.002s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.012s]
Closing the database file...  [0.001s]
Deallocating taxonomy...  [0s]
Total time = 62.677s
Reported 275613 pairwise alignments, 275613 HSPs.
21437 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
......................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 282853x282853 matrix with 5732068 entries to stream <out/merged.mci>
[mclIO] wrote 282853 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 282853 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 282853x282853 matrix with 5732068 entries
[mcl] pid 16138
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  54.18  2.92 0.98/0.05/4.25 2.65 2.46 2.46   0
  2  ...................  53.56 13.69 0.87/0.12/4.58 4.03 0.86 2.11   3
  3  ...................  42.36  7.84 0.83/0.12/7.05 2.49 0.71 1.50   1
  4  ...................  24.75  3.14 0.83/0.12/11.69 1.55 0.72 1.08   0
  5  ...................  17.39  1.53 0.82/0.09/8.63 1.22 0.71 0.77   0
  6  ...................   9.96  0.82 0.82/0.12/4.13 1.09 0.73 0.56   0
  7  ...................  11.94  0.53 0.82/0.14/3.16 1.03 0.78 0.43   0
  8  ...................   5.98  0.39 0.83/0.20/2.75 1.01 0.81 0.35   0
  9  ...................   4.69  0.32 0.86/0.20/1.39 1.01 0.81 0.29   0
 10  ...................   5.10  0.27 0.89/0.26/1.70 1.00 0.81 0.23   0
 11  ...................   4.31  0.24 0.93/0.23/1.11 1.00 0.82 0.19   0
 12  ...................   4.77  0.20 0.95/0.22/1.18 1.00 0.83 0.16   0
 13  ...................   4.94  0.18 0.97/0.27/1.01 1.00 0.86 0.13   0
 14  ...................   4.27  0.16 0.98/0.20/1.00 1.00 0.90 0.12   0
 15  ...................   4.64  0.15 0.99/0.23/1.00 1.00 0.92 0.11   0
 16  ...................   3.13  0.14 0.99/0.28/1.00 1.00 0.96 0.11   0
 17  ...................   3.61  0.14 1.00/0.36/1.00 1.00 0.96 0.10   0
 18  ...................   1.73  0.14 1.00/0.33/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.13 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.05  0.13 1.00/0.46/1.00 1.00 1.00 0.10   0
 21  ...................   1.11  0.13 1.00/0.57/1.00 1.00 1.00 0.10   0
 22  ...................   0.43  0.14 1.00/0.69/1.00 1.00 1.00 0.10   0
 23  ...................   0.46  0.14 1.00/0.66/1.00 1.00 1.00 0.10   0
 24  ...................   0.49  0.14 1.00/0.64/1.00 1.00 1.00 0.10   0
 25  ...................   0.25  0.13 1.00/0.76/1.00 1.00 1.00 0.10   0
 26  ...................   0.16  0.14 1.00/0.84/1.00 1.00 1.00 0.10   0
 27  ...................   0.02  0.12 1.00/0.98/1.00 1.00 1.00 0.10   0
 28  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
 29  ...................   0.00  0.12 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 34290 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3567 entries from out/pcs_contigs.csv
Read 275141 entries (dropped 2282 singletons) from out/Cyber_profiles.csv
.......... 0.89% 10000/1119780.0
.......... 1.79% 20000/1119780.0
.......... 2.68% 30000/1119780.0
.......... 3.57% 40000/1119780.0
.......... 4.47% 50000/1119780.0
.......... 5.36% 60000/1119780.0
.......... 6.25% 70000/1119780.0
.......... 7.14% 80000/1119780.0
.......... 8.04% 90000/1119780.0
.......... 8.93% 100000/1119780.0
.......... 9.82% 110000/1119780.0
..........10.72% 120000/1119780.0
..........11.61% 130000/1119780.0
..........12.50% 140000/1119780.0
..........13.40% 150000/1119780.0
..........14.29% 160000/1119780.0
..........15.18% 170000/1119780.0
..........16.07% 180000/1119780.0
..........16.97% 190000/1119780.0
..........17.86% 200000/1119780.0
..........18.75% 210000/1119780.0
..........19.65% 220000/1119780.0
..........20.54% 230000/1119780.0
..........21.43% 240000/1119780.0
..........22.33% 250000/1119780.0
..........23.22% 260000/1119780.0
..........24.11% 270000/1119780.0
..........25.00% 280000/1119780.0
..........25.90% 290000/1119780.0
..........26.79% 300000/1119780.0
..........27.68% 310000/1119780.0
..........28.58% 320000/1119780.0
..........29.47% 330000/1119780.0
..........30.36% 340000/1119780.0
..........31.26% 350000/1119780.0
..........32.15% 360000/1119780.0
..........33.04% 370000/1119780.0
..........33.94% 380000/1119780.0
..........34.83% 390000/1119780.0
..........35.72% 400000/1119780.0
..........36.61% 410000/1119780.0
..........37.51% 420000/1119780.0
..........38.40% 430000/1119780.0
..........39.29% 440000/1119780.0
..........40.19% 450000/1119780.0
..........41.08% 460000/1119780.0
..........41.97% 470000/1119780.0
..........42.87% 480000/1119780.0
..........43.76% 490000/1119780.0
..........44.65% 500000/1119780.0
..........45.54% 510000/1119780.0
..........46.44% 520000/1119780.0
..........47.33% 530000/1119780.0
..........48.22% 540000/1119780.0
..........49.12% 550000/1119780.0
..........50.01% 560000/1119780.0
..........50.90% 570000/1119780.0
..........51.80% 580000/1119780.0
..........52.69% 590000/1119780.0
..........53.58% 600000/1119780.0
..........54.47% 610000/1119780.0
..........55.37% 620000/1119780.0
..........56.26% 630000/1119780.0
..........57.15% 640000/1119780.0
..........58.05% 650000/1119780.0
..........58.94% 660000/1119780.0
..........59.83% 670000/1119780.0
..........60.73% 680000/1119780.0
..........61.62% 690000/1119780.0
..........62.51% 700000/1119780.0
..........63.41% 710000/1119780.0
..........64.30% 720000/1119780.0
..........65.19% 730000/1119780.0
..........66.08% 740000/1119780.0
..........66.98% 750000/1119780.0
..........67.87% 760000/1119780.0
..........68.76% 770000/1119780.0
..........69.66% 780000/1119780.0
..........70.55% 790000/1119780.0
..........71.44% 800000/1119780.0
..........72.34% 810000/1119780.0
..........73.23% 820000/1119780.0
..........74.12% 830000/1119780.0
..........75.01% 840000/1119780.0
..........75.91% 850000/1119780.0
..........76.80% 860000/1119780.0
..........77.69% 870000/1119780.0
..........78.59% 880000/1119780.0
..........79.48% 890000/1119780.0
..........80.37% 900000/1119780.0
..........81.27% 910000/1119780.0
..........82.16% 920000/1119780.0
..........83.05% 930000/1119780.0
..........83.95% 940000/1119780.0
..........84.84% 950000/1119780.0
..........85.73% 960000/1119780.0
..........86.62% 970000/1119780.0
..........87.52% 980000/1119780.0
..........88.41% 990000/1119780.0
..........89.30% 1000000/1119780.0
..........90.20% 1010000/1119780.0
..........91.09% 1020000/1119780.0
..........91.98% 1030000/1119780.0
..........92.88% 1040000/1119780.0
..........93.77% 1050000/1119780.0
..........94.66% 1060000/1119780.0
..........95.55% 1070000/1119780.0
..........96.45% 1080000/1119780.0
..........97.34% 1090000/1119780.0
..........98.23% 1100000/1119780.0
..........99.13% 1110000/1119780.0
......Hypergeometric contig-similarity network:
       3567 contigs,
     131426 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (131426 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3451, 3451)
features: (3451, 512)
y: (3451,) (3451,)
mask: (3451,) (3451,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3450, 3450, 3450],
                       [ 463,  450,  447,  ...,   34,   32,    9]]),
       values=tensor([0.0860, 0.0195, 0.0005,  ..., 0.0053, 0.0633, 0.0090]),
       device='cuda:0', size=(3451, 512), nnz=83075, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    2,  ..., 2380, 2381, 3450],
                       [   0,    0,    0,  ..., 3450, 3450, 3450]]),
       values=tensor([0.3333, 0.3333, 0.3333,  ..., 0.0687, 0.0687, 0.2500]),
       device='cuda:0', size=(3451, 3451), nnz=135547, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 83075
0 20.048717498779297 0.14078751857355126
10 16.232460021972656 0.638187221396731
20 13.36449909210205 0.8677563150074294
30 11.129932403564453 0.887444279346211
40 9.340434074401855 0.9115898959881129
50 8.034104347229004 0.9320208023774146
60 6.865005016326904 0.9472511144130757
70 5.907668590545654 0.9520802377414561
80 5.134776592254639 0.9606240713224369
90 4.442383766174316 0.961738484398217
100 3.874121904373169 0.9654531946508172
110 3.406036376953125 0.962109955423477
120 2.93047833442688 0.9673105497771174
130 2.570650339126587 0.9650817236255572
140 2.2989330291748047 0.9673105497771174
150 1.9764201641082764 0.9684249628528975
160 1.7833491563796997 0.9691679049034175
170 1.5944552421569824 0.9687964338781575
180 1.4402310848236084 0.9702823179791976
190 1.2748408317565918 0.9691679049034175
200 1.1305875778198242 0.9702823179791976
210 1.0356276035308838 0.9687964338781575
220 0.956114649772644 0.9706537890044576
230 0.8805632591247559 0.9680534918276374
240 0.7896021604537964 0.9673105497771174
250 0.7460297346115112 0.9647102526002972
260 0.6778857111930847 0.9661961367013373
270 0.6064742207527161 0.9687964338781575
280 0.6024757623672485 0.9591381872213968
290 0.5365928411483765 0.9661961367013373
300 0.47942501306533813 0.9643387815750372
310 0.4630110561847687 0.9680534918276374
320 0.45580464601516724 0.9676820208023774
330 0.4405038356781006 0.9665676077265973
340 0.3912178874015808 0.9695393759286776
350 0.40091466903686523 0.9721396731054978
360 0.3801814019680023 0.9732540861812778
370 0.34287452697753906 0.9736255572065379
380 0.3572244942188263 0.9725111441307578
390 0.3478879928588867 0.9717682020802377
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
error length < 2000bp
DTR_281728
error length < 2000bp
DTR_278507
error length < 2000bp
DTR_271350
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_355.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_711.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_886.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_355.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_291912
error length < 2000bp
DTR_293182
error length < 2000bp
DTR_293183
error length < 2000bp
DTR_283557
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_29.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_632.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_762.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_763.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_29.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_312921
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_754.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_754.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_324163
error length < 2000bp
DTR_317853
error length < 2000bp
DTR_317852
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_383.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_384.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_604.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_383.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_331687
error length < 2000bp
DTR_331662
error length < 2000bp
DTR_331680
error length < 2000bp
DTR_344019
error length < 2000bp
DTR_331643
error length < 2000bp
DTR_331641
error length < 2000bp
DTR_331869
error length < 2000bp
DTR_331870
error length < 2000bp
DTR_331868
error length < 2000bp
DTR_331657
error length < 2000bp
DTR_331672
error length < 2000bp
DTR_331163
error length < 2000bp
DTR_331668
error length < 2000bp
DTR_344066
error length < 2000bp
DTR_331864
error length < 2000bp
DTR_331882
error length < 2000bp
DTR_331859
error length < 2000bp
DTR_344966
error length < 2000bp
DTR_331656
error length < 2000bp
DTR_331673
error length < 2000bp
DTR_331661
error length < 2000bp
DTR_331646
error length < 2000bp
DTR_331637
error length < 2000bp
DTR_331676
error length < 2000bp
DTR_331640
error length < 2000bp
DTR_331885
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_155.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_156.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_157.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_158.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_159.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_160.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_161.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_162.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_163.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_164.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_165.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_166.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_167.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_168.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_169.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_206.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_207.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_208.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_209.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_210.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_211.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_212.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_353.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_354.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_441.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_61.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_155.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_360544
error length < 2000bp
DTR_363464
error length < 2000bp
DTR_362820
error length < 2000bp
DTR_364178
error length < 2000bp
DTR_357606
error length < 2000bp
DTR_356010
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_312.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_417.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_599.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_710.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_741.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_764.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_312.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.072s]
Loading sequences...  [0.431s]
Masking sequences...  [0.449s]
Writing sequences...  [0.066s]
Hashing sequences...  [0.024s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.062s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.063s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.027s]
Opening the output file...  [0.001s]
Loading query sequences...  [0.636s]
Masking queries...  [0.895s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.699s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.113s]
Masking reference...  [0.464s]
Initializing temporary storage...  [0.028s]
Building reference histograms...  [1.406s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.157s]
Building query seed array...  [0.151s]
Computing hash join...  [0.147s]
Building seed filter...  [0.014s]
Searching alignments...  [0.296s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.141s]
Building query seed array...  [0.356s]
Computing hash join...  [0.127s]
Building seed filter...  [0.025s]
Searching alignments...  [0.293s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.254s]
Building query seed array...  [0.284s]
Computing hash join...  [0.073s]
Building seed filter...  [0.013s]
Searching alignments...  [0.362s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.174s]
Building query seed array...  [0.175s]
Computing hash join...  [0.062s]
Building seed filter...  [0.017s]
Searching alignments...  [0.34s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.23s]
Building query seed array...  [0.188s]
Computing hash join...  [0.083s]
Building seed filter...  [0.024s]
Searching alignments...  [0.305s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.195s]
Building query seed array...  [0.246s]
Computing hash join...  [0.141s]
Building seed filter...  [0.024s]
Searching alignments...  [0.349s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.224s]
Computing hash join...  [0.105s]
Building seed filter...  [0.01s]
Searching alignments...  [0.255s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.167s]
Computing hash join...  [0.089s]
Building seed filter...  [0.019s]
Searching alignments...  [0.306s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.167s]
Building query seed array...  [0.15s]
Computing hash join...  [0.093s]
Building seed filter...  [0.015s]
Searching alignments...  [0.462s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.174s]
Computing hash join...  [0.162s]
Building seed filter...  [0.014s]
Searching alignments...  [0.322s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.203s]
Building query seed array...  [0.196s]
Computing hash join...  [0.063s]
Building seed filter...  [0.021s]
Searching alignments...  [0.278s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.218s]
Building query seed array...  [0.167s]
Computing hash join...  [0.06s]
Building seed filter...  [0.014s]
Searching alignments...  [0.327s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.232s]
Building query seed array...  [0.135s]
Computing hash join...  [0.075s]
Building seed filter...  [0.011s]
Searching alignments...  [0.311s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.295s]
Building query seed array...  [0.176s]
Computing hash join...  [0.12s]
Building seed filter...  [0.016s]
Searching alignments...  [0.351s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.209s]
Building query seed array...  [0.222s]
Computing hash join...  [0.105s]
Building seed filter...  [0.012s]
Searching alignments...  [0.309s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.184s]
Building query seed array...  [0.134s]
Computing hash join...  [0.059s]
Building seed filter...  [0.014s]
Searching alignments...  [0.412s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.22s]
Building query seed array...  [0.136s]
Computing hash join...  [0.065s]
Building seed filter...  [0.02s]
Searching alignments...  [0.3s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.233s]
Building query seed array...  [0.357s]
Computing hash join...  [0.126s]
Building seed filter...  [0.015s]
Searching alignments...  [0.283s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.18s]
Building query seed array...  [0.178s]
Computing hash join...  [0.057s]
Building seed filter...  [0.02s]
Searching alignments...  [0.318s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.22s]
Building query seed array...  [0.166s]
Computing hash join...  [0.068s]
Building seed filter...  [0.02s]
Searching alignments...  [0.366s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.127s]
Building query seed array...  [0.211s]
Computing hash join...  [0.058s]
Building seed filter...  [0.009s]
Searching alignments...  [0.306s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.187s]
Building query seed array...  [0.253s]
Computing hash join...  [0.059s]
Building seed filter...  [0.019s]
Searching alignments...  [0.263s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.213s]
Building query seed array...  [0.175s]
Computing hash join...  [0.096s]
Building seed filter...  [0.012s]
Searching alignments...  [0.288s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.115s]
Building query seed array...  [0.231s]
Computing hash join...  [0.06s]
Building seed filter...  [0.029s]
Searching alignments...  [0.311s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.17s]
Building query seed array...  [0.209s]
Computing hash join...  [0.08s]
Building seed filter...  [0.014s]
Searching alignments...  [0.313s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.21s]
Building query seed array...  [0.194s]
Computing hash join...  [0.116s]
Building seed filter...  [0.014s]
Searching alignments...  [0.273s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.223s]
Building query seed array...  [0.188s]
Computing hash join...  [0.077s]
Building seed filter...  [0.016s]
Searching alignments...  [0.307s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.305s]
Building query seed array...  [0.265s]
Computing hash join...  [0.181s]
Building seed filter...  [0.024s]
Searching alignments...  [0.29s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.135s]
Building query seed array...  [0.335s]
Computing hash join...  [0.123s]
Building seed filter...  [0.025s]
Searching alignments...  [0.437s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.229s]
Building query seed array...  [0.35s]
Computing hash join...  [0.102s]
Building seed filter...  [0.025s]
Searching alignments...  [0.361s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.241s]
Building query seed array...  [0.248s]
Computing hash join...  [0.103s]
Building seed filter...  [0.025s]
Searching alignments...  [0.387s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.169s]
Building query seed array...  [0.196s]
Computing hash join...  [0.065s]
Building seed filter...  [0.025s]
Searching alignments...  [0.302s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.197s]
Building query seed array...  [0.259s]
Computing hash join...  [0.136s]
Building seed filter...  [0.023s]
Searching alignments...  [0.373s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.315s]
Building query seed array...  [0.261s]
Computing hash join...  [0.143s]
Building seed filter...  [0.032s]
Searching alignments...  [0.369s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.201s]
Computing hash join...  [0.099s]
Building seed filter...  [0.025s]
Searching alignments...  [0.284s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.16s]
Building query seed array...  [0.16s]
Computing hash join...  [0.068s]
Building seed filter...  [0.033s]
Searching alignments...  [0.3s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.158s]
Computing hash join...  [0.093s]
Building seed filter...  [0.022s]
Searching alignments...  [0.253s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.184s]
Building query seed array...  [0.252s]
Computing hash join...  [0.072s]
Building seed filter...  [0.009s]
Searching alignments...  [0.287s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.195s]
Building query seed array...  [0.33s]
Computing hash join...  [0.105s]
Building seed filter...  [0.024s]
Searching alignments...  [0.294s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.233s]
Building query seed array...  [0.255s]
Computing hash join...  [0.076s]
Building seed filter...  [0.032s]
Searching alignments...  [0.264s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.177s]
Building query seed array...  [0.329s]
Computing hash join...  [0.115s]
Building seed filter...  [0.032s]
Searching alignments...  [0.386s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.386s]
Building query seed array...  [0.229s]
Computing hash join...  [0.12s]
Building seed filter...  [0.027s]
Searching alignments...  [0.399s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.242s]
Building query seed array...  [0.294s]
Computing hash join...  [0.196s]
Building seed filter...  [0.027s]
Searching alignments...  [0.337s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.258s]
Building query seed array...  [0.146s]
Computing hash join...  [0.077s]
Building seed filter...  [0.011s]
Searching alignments...  [0.29s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.1s]
Building query seed array...  [0.134s]
Computing hash join...  [0.078s]
Building seed filter...  [0.01s]
Searching alignments...  [0.289s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.269s]
Building query seed array...  [0.163s]
Computing hash join...  [0.065s]
Building seed filter...  [0.019s]
Searching alignments...  [0.35s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.342s]
Building query seed array...  [0.29s]
Computing hash join...  [0.189s]
Building seed filter...  [0.02s]
Searching alignments...  [0.278s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.154s]
Building query seed array...  [0.197s]
Computing hash join...  [0.071s]
Building seed filter...  [0.011s]
Searching alignments...  [0.249s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.188s]
Building query seed array...  [0.149s]
Computing hash join...  [0.092s]
Building seed filter...  [0.011s]
Searching alignments...  [0.276s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.247s]
Building query seed array...  [0.467s]
Computing hash join...  [0.117s]
Building seed filter...  [0.025s]
Searching alignments...  [0.311s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.22s]
Building query seed array...  [0.24s]
Computing hash join...  [0.152s]
Building seed filter...  [0.017s]
Searching alignments...  [0.334s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.199s]
Building query seed array...  [0.297s]
Computing hash join...  [0.072s]
Building seed filter...  [0.025s]
Searching alignments...  [0.287s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.119s]
Building query seed array...  [0.194s]
Computing hash join...  [0.063s]
Building seed filter...  [0.013s]
Searching alignments...  [0.324s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.206s]
Building query seed array...  [0.247s]
Computing hash join...  [0.089s]
Building seed filter...  [0.019s]
Searching alignments...  [0.274s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.163s]
Building query seed array...  [0.274s]
Computing hash join...  [0.068s]
Building seed filter...  [0.012s]
Searching alignments...  [0.277s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.2s]
Building query seed array...  [0.26s]
Computing hash join...  [0.1s]
Building seed filter...  [0.017s]
Searching alignments...  [0.377s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.178s]
Building query seed array...  [0.228s]
Computing hash join...  [0.086s]
Building seed filter...  [0.022s]
Searching alignments...  [0.257s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.208s]
Building query seed array...  [0.181s]
Computing hash join...  [0.067s]
Building seed filter...  [0.017s]
Searching alignments...  [0.273s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.238s]
Building query seed array...  [0.288s]
Computing hash join...  [0.08s]
Building seed filter...  [0.017s]
Searching alignments...  [0.307s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.168s]
Building query seed array...  [0.207s]
Computing hash join...  [0.118s]
Building seed filter...  [0.023s]
Searching alignments...  [0.319s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.179s]
Building query seed array...  [0.242s]
Computing hash join...  [0.128s]
Building seed filter...  [0.015s]
Searching alignments...  [0.333s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.292s]
Building query seed array...  [0.221s]
Computing hash join...  [0.093s]
Building seed filter...  [0.017s]
Searching alignments...  [0.362s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.193s]
Building query seed array...  [0.218s]
Computing hash join...  [0.06s]
Building seed filter...  [0.019s]
Searching alignments...  [0.313s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.133s]
Computing hash join...  [0.097s]
Building seed filter...  [0.021s]
Searching alignments...  [0.284s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.059s]
Computing alignments...  [10.25s]
Deallocating reference...  [0s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0.001s]
Deallocating queries...  [0.001s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.014s]
Closing the database file...  [0.001s]
Deallocating taxonomy...  [0s]
Total time = 71.44s
Reported 384821 pairwise alignments, 384821 HSPs.
28720 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
...........................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 289868x289868 matrix with 5950484 entries to stream <out/merged.mci>
[mclIO] wrote 289868 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 289868 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 289868x289868 matrix with 5950484 entries
[mcl] pid 5889
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  61.55  3.45 1.00/0.01/4.80 2.99 2.75 2.75   0
  2  ...................  89.56 16.56 0.86/0.07/4.58 4.01 0.83 2.29   6
  3  ...................  42.55  9.91 0.83/0.08/7.83 2.61 0.66 1.50   2
  4  ...................  27.60  3.35 0.82/0.09/10.57 1.58 0.71 1.06   0
  5  ...................  17.12  1.55 0.82/0.11/6.62 1.23 0.71 0.75   0
  6  ...................  14.65  0.82 0.82/0.12/4.12 1.09 0.73 0.55   0
  7  ...................   9.16  0.54 0.82/0.14/2.52 1.04 0.78 0.42   0
  8  ...................   6.19  0.39 0.83/0.19/2.47 1.01 0.81 0.34   0
  9  ...................   4.69  0.32 0.86/0.20/1.33 1.01 0.81 0.28   0
 10  ...................   5.10  0.28 0.89/0.26/1.32 1.00 0.81 0.23   0
 11  ...................   4.31  0.21 0.93/0.23/1.11 1.00 0.82 0.18   0
 12  ...................   4.48  0.20 0.95/0.22/1.18 1.00 0.83 0.15   0
 13  ...................   4.96  0.17 0.97/0.24/1.01 1.00 0.86 0.13   0
 14  ...................   4.70  0.16 0.98/0.19/1.00 1.00 0.90 0.12   0
 15  ...................   4.66  0.15 0.99/0.24/1.00 1.00 0.93 0.11   0
 16  ...................   3.57  0.14 0.99/0.28/1.00 1.00 0.96 0.10   0
 17  ...................   2.19  0.14 1.00/0.29/1.00 1.00 0.97 0.10   0
 18  ...................   4.67  0.13 1.00/0.26/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.13 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.05  0.13 1.00/0.46/1.00 1.00 1.00 0.10   0
 21  ...................   1.11  0.13 1.00/0.57/1.00 1.00 1.00 0.10   0
 22  ...................   0.43  0.14 1.00/0.70/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.13 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.15  0.14 1.00/0.85/1.00 1.00 1.00 0.10   0
 25  ...................   0.03  0.12 1.00/0.96/1.00 1.00 1.00 0.10   0
 26  ...................   0.05  0.13 1.00/0.99/1.00 1.00 1.00 0.10   0
 27  ...................   0.09  0.13 1.00/0.97/1.00 1.00 1.00 0.10   0
 28  ...................   0.16  0.12 1.00/0.90/1.00 1.00 1.00 0.10   0
 29  ...................   0.24  0.13 1.00/0.79/1.00 1.00 1.00 0.10   0
 30  ...................   0.22  0.12 1.00/0.79/1.00 1.00 1.00 0.10   0
 31  ...................   0.06  0.12 1.00/0.94/1.00 1.00 1.00 0.10   0
 32  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
 33  ...................   0.00  0.12 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 34050 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3653 entries from out/pcs_contigs.csv
Read 281668 entries (dropped 2286 singletons) from out/Cyber_profiles.csv
.......... 0.68% 10000/1468678.0
.......... 1.36% 20000/1468678.0
.......... 2.04% 30000/1468678.0
.......... 2.72% 40000/1468678.0
.......... 3.40% 50000/1468678.0
.......... 4.09% 60000/1468678.0
.......... 4.77% 70000/1468678.0
.......... 5.45% 80000/1468678.0
.......... 6.13% 90000/1468678.0
.......... 6.81% 100000/1468678.0
.......... 7.49% 110000/1468678.0
.......... 8.17% 120000/1468678.0
.......... 8.85% 130000/1468678.0
.......... 9.53% 140000/1468678.0
..........10.21% 150000/1468678.0
..........10.89% 160000/1468678.0
..........11.58% 170000/1468678.0
..........12.26% 180000/1468678.0
..........12.94% 190000/1468678.0
..........13.62% 200000/1468678.0
..........14.30% 210000/1468678.0
..........14.98% 220000/1468678.0
..........15.66% 230000/1468678.0
..........16.34% 240000/1468678.0
..........17.02% 250000/1468678.0
..........17.70% 260000/1468678.0
..........18.38% 270000/1468678.0
..........19.06% 280000/1468678.0
..........19.75% 290000/1468678.0
..........20.43% 300000/1468678.0
..........21.11% 310000/1468678.0
..........21.79% 320000/1468678.0
..........22.47% 330000/1468678.0
..........23.15% 340000/1468678.0
..........23.83% 350000/1468678.0
..........24.51% 360000/1468678.0
..........25.19% 370000/1468678.0
..........25.87% 380000/1468678.0
..........26.55% 390000/1468678.0
..........27.24% 400000/1468678.0
..........27.92% 410000/1468678.0
..........28.60% 420000/1468678.0
..........29.28% 430000/1468678.0
..........29.96% 440000/1468678.0
..........30.64% 450000/1468678.0
..........31.32% 460000/1468678.0
..........32.00% 470000/1468678.0
..........32.68% 480000/1468678.0
..........33.36% 490000/1468678.0
..........34.04% 500000/1468678.0
..........34.73% 510000/1468678.0
..........35.41% 520000/1468678.0
..........36.09% 530000/1468678.0
..........36.77% 540000/1468678.0
..........37.45% 550000/1468678.0
..........38.13% 560000/1468678.0
..........38.81% 570000/1468678.0
..........39.49% 580000/1468678.0
..........40.17% 590000/1468678.0
..........40.85% 600000/1468678.0
..........41.53% 610000/1468678.0
..........42.21% 620000/1468678.0
..........42.90% 630000/1468678.0
..........43.58% 640000/1468678.0
..........44.26% 650000/1468678.0
..........44.94% 660000/1468678.0
..........45.62% 670000/1468678.0
..........46.30% 680000/1468678.0
..........46.98% 690000/1468678.0
..........47.66% 700000/1468678.0
..........48.34% 710000/1468678.0
..........49.02% 720000/1468678.0
..........49.70% 730000/1468678.0
..........50.39% 740000/1468678.0
..........51.07% 750000/1468678.0
..........51.75% 760000/1468678.0
..........52.43% 770000/1468678.0
..........53.11% 780000/1468678.0
..........53.79% 790000/1468678.0
..........54.47% 800000/1468678.0
..........55.15% 810000/1468678.0
..........55.83% 820000/1468678.0
..........56.51% 830000/1468678.0
..........57.19% 840000/1468678.0
..........57.88% 850000/1468678.0
..........58.56% 860000/1468678.0
..........59.24% 870000/1468678.0
..........59.92% 880000/1468678.0
..........60.60% 890000/1468678.0
..........61.28% 900000/1468678.0
..........61.96% 910000/1468678.0
..........62.64% 920000/1468678.0
..........63.32% 930000/1468678.0
..........64.00% 940000/1468678.0
..........64.68% 950000/1468678.0
..........65.36% 960000/1468678.0
..........66.05% 970000/1468678.0
..........66.73% 980000/1468678.0
..........67.41% 990000/1468678.0
..........68.09% 1000000/1468678.0
..........68.77% 1010000/1468678.0
..........69.45% 1020000/1468678.0
..........70.13% 1030000/1468678.0
..........70.81% 1040000/1468678.0
..........71.49% 1050000/1468678.0
..........72.17% 1060000/1468678.0
..........72.85% 1070000/1468678.0
..........73.54% 1080000/1468678.0
..........74.22% 1090000/1468678.0
..........74.90% 1100000/1468678.0
..........75.58% 1110000/1468678.0
..........76.26% 1120000/1468678.0
..........76.94% 1130000/1468678.0
..........77.62% 1140000/1468678.0
..........78.30% 1150000/1468678.0
..........78.98% 1160000/1468678.0
..........79.66% 1170000/1468678.0
..........80.34% 1180000/1468678.0
..........81.03% 1190000/1468678.0
..........81.71% 1200000/1468678.0
..........82.39% 1210000/1468678.0
..........83.07% 1220000/1468678.0
..........83.75% 1230000/1468678.0
..........84.43% 1240000/1468678.0
..........85.11% 1250000/1468678.0
..........85.79% 1260000/1468678.0
..........86.47% 1270000/1468678.0
..........87.15% 1280000/1468678.0
..........87.83% 1290000/1468678.0
..........88.51% 1300000/1468678.0
..........89.20% 1310000/1468678.0
..........89.88% 1320000/1468678.0
..........90.56% 1330000/1468678.0
..........91.24% 1340000/1468678.0
..........91.92% 1350000/1468678.0
..........92.60% 1360000/1468678.0
..........93.28% 1370000/1468678.0
..........93.96% 1380000/1468678.0
..........94.64% 1390000/1468678.0
..........95.32% 1400000/1468678.0
..........96.00% 1410000/1468678.0
..........96.69% 1420000/1468678.0
..........97.37% 1430000/1468678.0
..........98.05% 1440000/1468678.0
..........98.73% 1450000/1468678.0
..........99.41% 1460000/1468678.0
.....Hypergeometric contig-similarity network:
       3653 contigs,
     179200 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (179200 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3635, 3635)
features: (3635, 512)
y: (3635,) (3635,)
mask: (3635,) (3635,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3634, 3634, 3634],
                       [ 466,  463,  447,  ...,   52,   34,   32]]),
       values=tensor([0.0009, 0.0690, 0.0415,  ..., 0.0825, 0.1177, 0.0333]),
       device='cuda:0', size=(3635, 512), nnz=88443, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    2,  ..., 3370, 3374, 3634],
                       [   0,    0,    0,  ..., 3634, 3634, 3634]]),
       values=tensor([0.0182, 0.0182, 0.0182,  ..., 0.1240, 0.1240, 0.2000]),
       device='cuda:0', size=(3635, 3635), nnz=182859, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 88443
0 20.029979705810547 0.115570419918246
10 16.203372955322266 0.5916016350798959
20 13.360257148742676 0.8290598290598291
30 11.071854591369629 0.89223337049424
40 9.356101989746094 0.9141583054626533
50 7.980109691619873 0.9342251950947603
60 6.9220757484436035 0.9516908212560387
70 5.9606032371521 0.9464882943143813
80 5.1425933837890625 0.9502043849869937
90 4.504070281982422 0.9561501300631735
100 3.8664047718048096 0.9594946116685247
110 3.3954100608825684 0.9620958751393534
120 2.9728634357452393 0.9632107023411371
130 2.6354875564575195 0.9617242660720922
140 2.2460856437683105 0.961352657004831
150 2.0323433876037598 0.966183574879227
160 1.8196529150009155 0.9620958751393534
170 1.5872993469238281 0.9624674842066147
180 1.4369193315505981 0.9639539204756596
190 1.2767099142074585 0.9650687476774433
200 1.1557555198669434 0.9632107023411371
210 1.055325984954834 0.9669267930137495
220 0.9224573373794556 0.9620958751393534
230 0.8734616041183472 0.9646971386101821
240 0.7628480195999146 0.9620958751393534
250 0.70865797996521 0.9665551839464883
260 0.6818870306015015 0.9646971386101821
270 0.6456629037857056 0.966183574879227
280 0.5663094520568848 0.967670011148272
290 0.5296055674552917 0.9624674842066147
300 0.5334694385528564 0.9687848383500557
310 0.47928810119628906 0.9606094388703085
320 0.47066834568977356 0.9695280564845782
330 0.4252215623855591 0.9669267930137495
340 0.41432633996009827 0.969156447417317
350 0.40460342168807983 0.9609810479375697
360 0.38034600019454956 0.970642883686362
370 0.3866257667541504 0.9672984020810108
380 0.41485536098480225 0.9669267930137495
390 0.35911041498184204 0.9702712746191007
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.042s]
Loading sequences...  [0.396s]
Masking sequences...  [0.446s]
Writing sequences...  [0.062s]
Hashing sequences...  [0.022s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0.001s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 0.989s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.086s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.029s]
Opening the output file...  [0.001s]
Loading query sequences...  [0.742s]
Masking queries...  [0.935s]
Building query seed set...  [0.004s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.562s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.138s]
Masking reference...  [0.489s]
Initializing temporary storage...  [0.033s]
Building reference histograms...  [1.283s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.176s]
Building query seed array...  [0.218s]
Computing hash join...  [0.091s]
Building seed filter...  [0.024s]
Searching alignments...  [0.403s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.27s]
Building query seed array...  [0.206s]
Computing hash join...  [0.066s]
Building seed filter...  [0.033s]
Searching alignments...  [0.415s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.225s]
Building query seed array...  [0.211s]
Computing hash join...  [0.067s]
Building seed filter...  [0.011s]
Searching alignments...  [0.4s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.164s]
Building query seed array...  [0.205s]
Computing hash join...  [0.086s]
Building seed filter...  [0.022s]
Searching alignments...  [0.399s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.167s]
Building query seed array...  [0.161s]
Computing hash join...  [0.064s]
Building seed filter...  [0.012s]
Searching alignments...  [0.351s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.229s]
Building query seed array...  [0.226s]
Computing hash join...  [0.088s]
Building seed filter...  [0.023s]
Searching alignments...  [0.297s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.212s]
Building query seed array...  [0.277s]
Computing hash join...  [0.123s]
Building seed filter...  [0.009s]
Searching alignments...  [0.296s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.174s]
Building query seed array...  [0.173s]
Computing hash join...  [0.063s]
Building seed filter...  [0.013s]
Searching alignments...  [0.44s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.169s]
Building query seed array...  [0.262s]
Computing hash join...  [0.066s]
Building seed filter...  [0.015s]
Searching alignments...  [0.395s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.169s]
Building query seed array...  [0.269s]
Computing hash join...  [0.081s]
Building seed filter...  [0.016s]
Searching alignments...  [0.384s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.185s]
Building query seed array...  [0.221s]
Computing hash join...  [0.114s]
Building seed filter...  [0.026s]
Searching alignments...  [0.447s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.182s]
Building query seed array...  [0.148s]
Computing hash join...  [0.092s]
Building seed filter...  [0.019s]
Searching alignments...  [0.451s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.235s]
Building query seed array...  [0.173s]
Computing hash join...  [0.132s]
Building seed filter...  [0.019s]
Searching alignments...  [0.365s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.235s]
Building query seed array...  [0.21s]
Computing hash join...  [0.089s]
Building seed filter...  [0.014s]
Searching alignments...  [0.325s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.27s]
Building query seed array...  [0.283s]
Computing hash join...  [0.07s]
Building seed filter...  [0.012s]
Searching alignments...  [0.344s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.103s]
Building query seed array...  [0.218s]
Computing hash join...  [0.064s]
Building seed filter...  [0.018s]
Searching alignments...  [0.417s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.187s]
Building query seed array...  [0.232s]
Computing hash join...  [0.075s]
Building seed filter...  [0.012s]
Searching alignments...  [0.375s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.207s]
Building query seed array...  [0.174s]
Computing hash join...  [0.064s]
Building seed filter...  [0.02s]
Searching alignments...  [0.336s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.196s]
Building query seed array...  [0.167s]
Computing hash join...  [0.062s]
Building seed filter...  [0.015s]
Searching alignments...  [0.338s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.227s]
Computing hash join...  [0.077s]
Building seed filter...  [0.018s]
Searching alignments...  [0.316s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.169s]
Building query seed array...  [0.206s]
Computing hash join...  [0.082s]
Building seed filter...  [0.02s]
Searching alignments...  [0.404s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.216s]
Building query seed array...  [0.257s]
Computing hash join...  [0.089s]
Building seed filter...  [0.011s]
Searching alignments...  [0.299s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.199s]
Building query seed array...  [0.172s]
Computing hash join...  [0.075s]
Building seed filter...  [0.014s]
Searching alignments...  [0.326s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.23s]
Building query seed array...  [0.135s]
Computing hash join...  [0.081s]
Building seed filter...  [0.025s]
Searching alignments...  [0.334s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.16s]
Building query seed array...  [0.199s]
Computing hash join...  [0.072s]
Building seed filter...  [0.02s]
Searching alignments...  [0.368s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.271s]
Building query seed array...  [0.223s]
Computing hash join...  [0.059s]
Building seed filter...  [0.015s]
Searching alignments...  [0.345s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.162s]
Computing hash join...  [0.1s]
Building seed filter...  [0.011s]
Searching alignments...  [0.314s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.167s]
Building query seed array...  [0.181s]
Computing hash join...  [0.061s]
Building seed filter...  [0.016s]
Searching alignments...  [0.334s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.173s]
Building query seed array...  [0.174s]
Computing hash join...  [0.102s]
Building seed filter...  [0.018s]
Searching alignments...  [0.36s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.203s]
Computing hash join...  [0.079s]
Building seed filter...  [0.012s]
Searching alignments...  [0.3s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.225s]
Building query seed array...  [0.243s]
Computing hash join...  [0.087s]
Building seed filter...  [0.016s]
Searching alignments...  [0.31s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.19s]
Computing hash join...  [0.063s]
Building seed filter...  [0.02s]
Searching alignments...  [0.295s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.178s]
Building query seed array...  [0.172s]
Computing hash join...  [0.082s]
Building seed filter...  [0.016s]
Searching alignments...  [0.375s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.24s]
Building query seed array...  [0.233s]
Computing hash join...  [0.057s]
Building seed filter...  [0.013s]
Searching alignments...  [0.375s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.212s]
Building query seed array...  [0.22s]
Computing hash join...  [0.068s]
Building seed filter...  [0.01s]
Searching alignments...  [0.307s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.236s]
Building query seed array...  [0.284s]
Computing hash join...  [0.057s]
Building seed filter...  [0.011s]
Searching alignments...  [0.378s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.158s]
Building query seed array...  [0.143s]
Computing hash join...  [0.076s]
Building seed filter...  [0.015s]
Searching alignments...  [0.274s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.209s]
Building query seed array...  [0.232s]
Computing hash join...  [0.054s]
Building seed filter...  [0.011s]
Searching alignments...  [0.302s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.267s]
Building query seed array...  [0.184s]
Computing hash join...  [0.146s]
Building seed filter...  [0.023s]
Searching alignments...  [0.355s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.262s]
Building query seed array...  [0.171s]
Computing hash join...  [0.073s]
Building seed filter...  [0.025s]
Searching alignments...  [0.39s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.244s]
Building query seed array...  [0.291s]
Computing hash join...  [0.092s]
Building seed filter...  [0.019s]
Searching alignments...  [0.367s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.207s]
Building query seed array...  [0.247s]
Computing hash join...  [0.133s]
Building seed filter...  [0.022s]
Searching alignments...  [0.482s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.28s]
Building query seed array...  [0.189s]
Computing hash join...  [0.064s]
Building seed filter...  [0.028s]
Searching alignments...  [0.316s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.112s]
Building query seed array...  [0.193s]
Computing hash join...  [0.082s]
Building seed filter...  [0.016s]
Searching alignments...  [0.403s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.126s]
Building query seed array...  [0.124s]
Computing hash join...  [0.086s]
Building seed filter...  [0.017s]
Searching alignments...  [0.384s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.308s]
Building query seed array...  [0.146s]
Computing hash join...  [0.091s]
Building seed filter...  [0.019s]
Searching alignments...  [0.317s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.244s]
Building query seed array...  [0.196s]
Computing hash join...  [0.1s]
Building seed filter...  [0.024s]
Searching alignments...  [0.334s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.118s]
Building query seed array...  [0.239s]
Computing hash join...  [0.066s]
Building seed filter...  [0.024s]
Searching alignments...  [0.356s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.118s]
Building query seed array...  [0.161s]
Computing hash join...  [0.065s]
Building seed filter...  [0.016s]
Searching alignments...  [0.423s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.189s]
Building query seed array...  [0.236s]
Computing hash join...  [0.103s]
Building seed filter...  [0.024s]
Searching alignments...  [0.326s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.279s]
Building query seed array...  [0.256s]
Computing hash join...  [0.081s]
Building seed filter...  [0.024s]
Searching alignments...  [0.321s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.168s]
Building query seed array...  [0.367s]
Computing hash join...  [0.072s]
Building seed filter...  [0.025s]
Searching alignments...  [0.347s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.136s]
Building query seed array...  [0.236s]
Computing hash join...  [0.077s]
Building seed filter...  [0.024s]
Searching alignments...  [0.354s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.238s]
Computing hash join...  [0.119s]
Building seed filter...  [0.025s]
Searching alignments...  [0.333s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.275s]
Building query seed array...  [0.276s]
Computing hash join...  [0.073s]
Building seed filter...  [0.024s]
Searching alignments...  [0.275s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.273s]
Building query seed array...  [0.173s]
Computing hash join...  [0.075s]
Building seed filter...  [0.025s]
Searching alignments...  [0.324s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.203s]
Building query seed array...  [0.202s]
Computing hash join...  [0.11s]
Building seed filter...  [0.012s]
Searching alignments...  [0.351s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.227s]
Building query seed array...  [0.323s]
Computing hash join...  [0.092s]
Building seed filter...  [0.026s]
Searching alignments...  [0.398s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.237s]
Building query seed array...  [0.266s]
Computing hash join...  [0.067s]
Building seed filter...  [0.016s]
Searching alignments...  [0.344s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.175s]
Computing hash join...  [0.064s]
Building seed filter...  [0.021s]
Searching alignments...  [0.379s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.154s]
Building query seed array...  [0.228s]
Computing hash join...  [0.059s]
Building seed filter...  [0.02s]
Searching alignments...  [0.285s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.203s]
Building query seed array...  [0.228s]
Computing hash join...  [0.056s]
Building seed filter...  [0.025s]
Searching alignments...  [0.335s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.289s]
Building query seed array...  [0.323s]
Computing hash join...  [0.116s]
Building seed filter...  [0.014s]
Searching alignments...  [0.372s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.165s]
Building query seed array...  [0.188s]
Computing hash join...  [0.071s]
Building seed filter...  [0.015s]
Searching alignments...  [0.323s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.065s]
Computing alignments...  [11.408s]
Deallocating reference...  [0.001s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0.001s]
Deallocating queries...  [0.002s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.017s]
Closing the database file...  [0s]
Deallocating taxonomy...  [0s]
Total time = 73.122s
Reported 422327 pairwise alignments, 422327 HSPs.
29884 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
.............................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 291219x291219 matrix with 6025496 entries to stream <out/merged.mci>
[mclIO] wrote 291219 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 291219 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 291219x291219 matrix with 6025496 entries
[mcl] pid 9678
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  52.94  3.33 0.99/0.03/4.82 2.85 2.63 2.63   0
  2  ...................  67.55 16.12 0.86/0.08/4.58 4.06 0.84 2.21   5
  3  ...................  51.83  9.43 0.83/0.07/6.07 2.54 0.68 1.51   2
  4  ...................  25.16  3.45 0.82/0.13/11.16 1.58 0.71 1.07   0
  5  ...................  17.06  1.59 0.82/0.10/7.50 1.23 0.70 0.75   0
  6  ...................  10.17  0.85 0.82/0.11/3.94 1.09 0.73 0.54   0
  7  ...................   9.83  0.54 0.82/0.14/3.09 1.04 0.78 0.42   0
  8  ...................   5.58  0.40 0.83/0.20/3.22 1.01 0.80 0.34   0
  9  ...................   4.70  0.32 0.86/0.20/1.52 1.00 0.81 0.28   0
 10  ...................   5.09  0.28 0.89/0.25/1.07 1.00 0.81 0.22   0
 11  ...................   4.32  0.22 0.93/0.22/1.17 1.00 0.81 0.18   0
 12  ...................   4.81  0.19 0.95/0.22/1.22 1.00 0.83 0.15   0
 13  ...................   4.96  0.16 0.97/0.25/1.12 1.00 0.86 0.13   0
 14  ...................   4.16  0.16 0.98/0.19/1.28 1.00 0.90 0.12   0
 15  ...................   4.65  0.14 0.99/0.24/1.00 1.00 0.92 0.11   0
 16  ...................   3.57  0.15 0.99/0.27/1.00 1.00 0.96 0.10   0
 17  ...................   2.34  0.13 1.00/0.26/1.00 1.00 0.97 0.10   0
 18  ...................   4.74  0.13 1.00/0.29/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.14 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.03  0.14 1.00/0.46/1.00 1.00 1.00 0.10   0
 21  ...................   1.13  0.13 1.00/0.56/1.00 1.00 1.00 0.10   0
 22  ...................   0.43  0.12 1.00/0.69/1.00 1.00 1.00 0.10   0
 23  ...................   0.48  0.13 1.00/0.65/1.00 1.00 1.00 0.10   0
 24  ...................   0.17  0.13 1.00/0.84/1.00 1.00 1.00 0.10   0
 25  ...................   0.04  0.13 1.00/0.95/1.00 1.00 1.00 0.10   0
 26  ...................   0.01  0.13 1.00/0.99/1.00 1.00 1.00 0.10   0
 27  ...................   0.00  0.12 1.00/1.00/1.00 1.00 1.00 0.10   0
 28  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 34309 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3604 entries from out/pcs_contigs.csv
Read 282800 entries (dropped 2275 singletons) from out/Cyber_profiles.csv
.......... 0.79% 10000/1270937.0
.......... 1.57% 20000/1270937.0
.......... 2.36% 30000/1270937.0
.......... 3.15% 40000/1270937.0
.......... 3.93% 50000/1270937.0
.......... 4.72% 60000/1270937.0
.......... 5.51% 70000/1270937.0
.......... 6.29% 80000/1270937.0
.......... 7.08% 90000/1270937.0
.......... 7.87% 100000/1270937.0
.......... 8.66% 110000/1270937.0
.......... 9.44% 120000/1270937.0
..........10.23% 130000/1270937.0
..........11.02% 140000/1270937.0
..........11.80% 150000/1270937.0
..........12.59% 160000/1270937.0
..........13.38% 170000/1270937.0
..........14.16% 180000/1270937.0
..........14.95% 190000/1270937.0
..........15.74% 200000/1270937.0
..........16.52% 210000/1270937.0
..........17.31% 220000/1270937.0
..........18.10% 230000/1270937.0
..........18.88% 240000/1270937.0
..........19.67% 250000/1270937.0
..........20.46% 260000/1270937.0
..........21.24% 270000/1270937.0
..........22.03% 280000/1270937.0
..........22.82% 290000/1270937.0
..........23.60% 300000/1270937.0
..........24.39% 310000/1270937.0
..........25.18% 320000/1270937.0
..........25.97% 330000/1270937.0
..........26.75% 340000/1270937.0
..........27.54% 350000/1270937.0
..........28.33% 360000/1270937.0
..........29.11% 370000/1270937.0
..........29.90% 380000/1270937.0
..........30.69% 390000/1270937.0
..........31.47% 400000/1270937.0
..........32.26% 410000/1270937.0
..........33.05% 420000/1270937.0
..........33.83% 430000/1270937.0
..........34.62% 440000/1270937.0
..........35.41% 450000/1270937.0
..........36.19% 460000/1270937.0
..........36.98% 470000/1270937.0
..........37.77% 480000/1270937.0
..........38.55% 490000/1270937.0
..........39.34% 500000/1270937.0
..........40.13% 510000/1270937.0
..........40.91% 520000/1270937.0
..........41.70% 530000/1270937.0
..........42.49% 540000/1270937.0
..........43.28% 550000/1270937.0
..........44.06% 560000/1270937.0
..........44.85% 570000/1270937.0
..........45.64% 580000/1270937.0
..........46.42% 590000/1270937.0
..........47.21% 600000/1270937.0
..........48.00% 610000/1270937.0
..........48.78% 620000/1270937.0
..........49.57% 630000/1270937.0
..........50.36% 640000/1270937.0
..........51.14% 650000/1270937.0
..........51.93% 660000/1270937.0
..........52.72% 670000/1270937.0
..........53.50% 680000/1270937.0
..........54.29% 690000/1270937.0
..........55.08% 700000/1270937.0
..........55.86% 710000/1270937.0
..........56.65% 720000/1270937.0
..........57.44% 730000/1270937.0
..........58.22% 740000/1270937.0
..........59.01% 750000/1270937.0
..........59.80% 760000/1270937.0
..........60.59% 770000/1270937.0
..........61.37% 780000/1270937.0
..........62.16% 790000/1270937.0
..........62.95% 800000/1270937.0
..........63.73% 810000/1270937.0
..........64.52% 820000/1270937.0
..........65.31% 830000/1270937.0
..........66.09% 840000/1270937.0
..........66.88% 850000/1270937.0
..........67.67% 860000/1270937.0
..........68.45% 870000/1270937.0
..........69.24% 880000/1270937.0
..........70.03% 890000/1270937.0
..........70.81% 900000/1270937.0
..........71.60% 910000/1270937.0
..........72.39% 920000/1270937.0
..........73.17% 930000/1270937.0
..........73.96% 940000/1270937.0
..........74.75% 950000/1270937.0
..........75.53% 960000/1270937.0
..........76.32% 970000/1270937.0
..........77.11% 980000/1270937.0
..........77.90% 990000/1270937.0
..........78.68% 1000000/1270937.0
..........79.47% 1010000/1270937.0
..........80.26% 1020000/1270937.0
..........81.04% 1030000/1270937.0
..........81.83% 1040000/1270937.0
..........82.62% 1050000/1270937.0
..........83.40% 1060000/1270937.0
..........84.19% 1070000/1270937.0
..........84.98% 1080000/1270937.0
..........85.76% 1090000/1270937.0
..........86.55% 1100000/1270937.0
..........87.34% 1110000/1270937.0
..........88.12% 1120000/1270937.0
..........88.91% 1130000/1270937.0
..........89.70% 1140000/1270937.0
..........90.48% 1150000/1270937.0
..........91.27% 1160000/1270937.0
..........92.06% 1170000/1270937.0
..........92.84% 1180000/1270937.0
..........93.63% 1190000/1270937.0
..........94.42% 1200000/1270937.0
..........95.21% 1210000/1270937.0
..........95.99% 1220000/1270937.0
..........96.78% 1230000/1270937.0
..........97.57% 1240000/1270937.0
..........98.35% 1250000/1270937.0
..........99.14% 1260000/1270937.0
.......Hypergeometric contig-similarity network:
       3604 contigs,
     158926 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (158926 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3554, 3554)
features: (3554, 512)
y: (3554,) (3554,)
mask: (3554,) (3554,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3553, 3553, 3553],
                       [ 463,  450,  447,  ...,   32,    9,    7]]),
       values=tensor([0.0653, 0.0097, 0.0050,  ..., 0.0883, 0.0005, 0.0008]),
       device='cuda:0', size=(3554, 512), nnz=86453, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    2,  ..., 3004, 3006, 3553],
                       [   0,    0,    0,  ..., 3553, 3553, 3553]]),
       values=tensor([0.0082, 0.0084, 0.0087,  ..., 0.0523, 0.0531, 0.2000]),
       device='cuda:0', size=(3554, 3554), nnz=162562, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 86453
0 20.014345169067383 0.13526570048309178
10 16.24370002746582 0.5867707172054998
20 13.336004257202148 0.8298030471943515
30 11.095527648925781 0.8903753251579338
40 9.356117248535156 0.904496469713861
50 8.030808448791504 0.9234485321441843
60 6.874165058135986 0.9487179487179487
70 5.962104320526123 0.947603121516165
80 5.158328056335449 0.933853586027499
90 4.493803977966309 0.9502043849869937
100 3.8565781116485596 0.9576365663322185
110 3.3910744190216064 0.9594946116685247
120 2.978320837020874 0.958379784466741
130 2.6034302711486816 0.9565217391304348
140 2.286559820175171 0.9606094388703085
150 2.046288013458252 0.9635823114083983
160 1.7826825380325317 0.9609810479375697
170 1.606895923614502 0.9620958751393534
180 1.4118094444274902 0.9646971386101821
190 1.307444453239441 0.9602378298030472
200 1.17322838306427 0.9646971386101821
210 1.0470449924468994 0.9654403567447045
220 0.9341562986373901 0.9628390932738758
230 0.9103323221206665 0.9669267930137495
240 0.7939639687538147 0.9654403567447045
250 0.7450089454650879 0.9628390932738758
260 0.6953147649765015 0.9658119658119658
270 0.6382046937942505 0.9632107023411371
280 0.6088165640830994 0.9624674842066147
290 0.597224235534668 0.9632107023411371
300 0.5379258394241333 0.9672984020810108
310 0.5122345685958862 0.9665551839464883
320 0.5300409197807312 0.9687848383500557
330 0.4360818564891815 0.969156447417317
340 0.4518616795539856 0.9646971386101821
350 0.44130778312683105 0.9698996655518395
360 0.3979055881500244 0.9698996655518395
370 0.39304041862487793 0.9665551839464883
380 0.38355356454849243 0.967670011148272
390 0.3897849917411804 0.9646971386101821
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.045s]
Loading sequences...  [0.395s]
Masking sequences...  [0.448s]
Writing sequences...  [0.063s]
Hashing sequences...  [0.023s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.006s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 0.993s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.055s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.028s]
Opening the output file...  [0s]
Loading query sequences...  [0.722s]
Masking queries...  [0.935s]
Building query seed set...  [0.004s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.709s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.132s]
Masking reference...  [0.518s]
Initializing temporary storage...  [0.024s]
Building reference histograms...  [1.611s]
Allocating buffers...  [0s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.127s]
Building query seed array...  [0.149s]
Computing hash join...  [0.068s]
Building seed filter...  [0.02s]
Searching alignments...  [0.354s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.187s]
Building query seed array...  [0.2s]
Computing hash join...  [0.078s]
Building seed filter...  [0.026s]
Searching alignments...  [0.378s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.157s]
Building query seed array...  [0.299s]
Computing hash join...  [0.115s]
Building seed filter...  [0.026s]
Searching alignments...  [0.426s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.202s]
Building query seed array...  [0.209s]
Computing hash join...  [0.078s]
Building seed filter...  [0.026s]
Searching alignments...  [0.321s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.176s]
Building query seed array...  [0.226s]
Computing hash join...  [0.075s]
Building seed filter...  [0.009s]
Searching alignments...  [0.297s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.255s]
Building query seed array...  [0.225s]
Computing hash join...  [0.055s]
Building seed filter...  [0.017s]
Searching alignments...  [0.266s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.227s]
Building query seed array...  [0.262s]
Computing hash join...  [0.055s]
Building seed filter...  [0.009s]
Searching alignments...  [0.33s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.184s]
Building query seed array...  [0.203s]
Computing hash join...  [0.064s]
Building seed filter...  [0.011s]
Searching alignments...  [0.345s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.186s]
Building query seed array...  [0.212s]
Computing hash join...  [0.064s]
Building seed filter...  [0.021s]
Searching alignments...  [0.36s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.176s]
Building query seed array...  [0.253s]
Computing hash join...  [0.058s]
Building seed filter...  [0.013s]
Searching alignments...  [0.387s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.183s]
Building query seed array...  [0.216s]
Computing hash join...  [0.097s]
Building seed filter...  [0.009s]
Searching alignments...  [0.301s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.167s]
Building query seed array...  [0.194s]
Computing hash join...  [0.074s]
Building seed filter...  [0.013s]
Searching alignments...  [0.293s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.189s]
Computing hash join...  [0.073s]
Building seed filter...  [0.019s]
Searching alignments...  [0.373s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.217s]
Building query seed array...  [0.268s]
Computing hash join...  [0.064s]
Building seed filter...  [0.013s]
Searching alignments...  [0.316s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.18s]
Building query seed array...  [0.212s]
Computing hash join...  [0.087s]
Building seed filter...  [0.025s]
Searching alignments...  [0.356s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.167s]
Building query seed array...  [0.245s]
Computing hash join...  [0.084s]
Building seed filter...  [0.026s]
Searching alignments...  [0.425s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.188s]
Building query seed array...  [0.241s]
Computing hash join...  [0.073s]
Building seed filter...  [0.014s]
Searching alignments...  [0.286s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.223s]
Building query seed array...  [0.166s]
Computing hash join...  [0.091s]
Building seed filter...  [0.02s]
Searching alignments...  [0.275s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.218s]
Building query seed array...  [0.321s]
Computing hash join...  [0.089s]
Building seed filter...  [0.024s]
Searching alignments...  [0.344s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.179s]
Building query seed array...  [0.131s]
Computing hash join...  [0.126s]
Building seed filter...  [0.017s]
Searching alignments...  [0.378s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.143s]
Building query seed array...  [0.163s]
Computing hash join...  [0.12s]
Building seed filter...  [0.014s]
Searching alignments...  [0.32s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.188s]
Computing hash join...  [0.118s]
Building seed filter...  [0.013s]
Searching alignments...  [0.338s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.18s]
Building query seed array...  [0.193s]
Computing hash join...  [0.062s]
Building seed filter...  [0.015s]
Searching alignments...  [0.304s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.185s]
Building query seed array...  [0.301s]
Computing hash join...  [0.08s]
Building seed filter...  [0.021s]
Searching alignments...  [0.305s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.155s]
Building query seed array...  [0.119s]
Computing hash join...  [0.063s]
Building seed filter...  [0.015s]
Searching alignments...  [0.267s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.174s]
Building query seed array...  [0.222s]
Computing hash join...  [0.073s]
Building seed filter...  [0.009s]
Searching alignments...  [0.279s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.255s]
Building query seed array...  [0.245s]
Computing hash join...  [0.056s]
Building seed filter...  [0.013s]
Searching alignments...  [0.325s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.196s]
Building query seed array...  [0.19s]
Computing hash join...  [0.09s]
Building seed filter...  [0.015s]
Searching alignments...  [0.337s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.22s]
Building query seed array...  [0.148s]
Computing hash join...  [0.088s]
Building seed filter...  [0.014s]
Searching alignments...  [0.364s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.211s]
Building query seed array...  [0.23s]
Computing hash join...  [0.06s]
Building seed filter...  [0.017s]
Searching alignments...  [0.311s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.297s]
Building query seed array...  [0.166s]
Computing hash join...  [0.093s]
Building seed filter...  [0.016s]
Searching alignments...  [0.31s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.173s]
Building query seed array...  [0.127s]
Computing hash join...  [0.072s]
Building seed filter...  [0.009s]
Searching alignments...  [0.299s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.186s]
Building query seed array...  [0.16s]
Computing hash join...  [0.084s]
Building seed filter...  [0.018s]
Searching alignments...  [0.337s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.215s]
Building query seed array...  [0.191s]
Computing hash join...  [0.061s]
Building seed filter...  [0.012s]
Searching alignments...  [0.341s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.206s]
Building query seed array...  [0.275s]
Computing hash join...  [0.077s]
Building seed filter...  [0.017s]
Searching alignments...  [0.342s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.187s]
Building query seed array...  [0.131s]
Computing hash join...  [0.11s]
Building seed filter...  [0.017s]
Searching alignments...  [0.348s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.162s]
Computing hash join...  [0.085s]
Building seed filter...  [0.017s]
Searching alignments...  [0.336s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.218s]
Building query seed array...  [0.182s]
Computing hash join...  [0.104s]
Building seed filter...  [0.012s]
Searching alignments...  [0.272s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.229s]
Building query seed array...  [0.202s]
Computing hash join...  [0.063s]
Building seed filter...  [0.023s]
Searching alignments...  [0.336s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.173s]
Building query seed array...  [0.124s]
Computing hash join...  [0.062s]
Building seed filter...  [0.023s]
Searching alignments...  [0.318s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.22s]
Building query seed array...  [0.187s]
Computing hash join...  [0.085s]
Building seed filter...  [0.022s]
Searching alignments...  [0.297s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.237s]
Building query seed array...  [0.199s]
Computing hash join...  [0.104s]
Building seed filter...  [0.011s]
Searching alignments...  [0.314s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.222s]
Building query seed array...  [0.287s]
Computing hash join...  [0.135s]
Building seed filter...  [0.02s]
Searching alignments...  [0.277s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.182s]
Building query seed array...  [0.223s]
Computing hash join...  [0.086s]
Building seed filter...  [0.011s]
Searching alignments...  [0.305s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.208s]
Building query seed array...  [0.116s]
Computing hash join...  [0.069s]
Building seed filter...  [0.011s]
Searching alignments...  [0.393s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.145s]
Building query seed array...  [0.253s]
Computing hash join...  [0.108s]
Building seed filter...  [0.024s]
Searching alignments...  [0.294s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.168s]
Building query seed array...  [0.261s]
Computing hash join...  [0.09s]
Building seed filter...  [0.015s]
Searching alignments...  [0.29s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.21s]
Building query seed array...  [0.171s]
Computing hash join...  [0.064s]
Building seed filter...  [0.015s]
Searching alignments...  [0.329s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.172s]
Building query seed array...  [0.124s]
Computing hash join...  [0.102s]
Building seed filter...  [0.024s]
Searching alignments...  [0.313s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.202s]
Building query seed array...  [0.304s]
Computing hash join...  [0.098s]
Building seed filter...  [0.023s]
Searching alignments...  [0.423s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.362s]
Building query seed array...  [0.25s]
Computing hash join...  [0.09s]
Building seed filter...  [0.019s]
Searching alignments...  [0.334s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.197s]
Building query seed array...  [0.255s]
Computing hash join...  [0.102s]
Building seed filter...  [0.02s]
Searching alignments...  [0.314s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.307s]
Building query seed array...  [0.23s]
Computing hash join...  [0.105s]
Building seed filter...  [0.024s]
Searching alignments...  [0.379s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.14s]
Building query seed array...  [0.205s]
Computing hash join...  [0.142s]
Building seed filter...  [0.01s]
Searching alignments...  [0.267s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.164s]
Building query seed array...  [0.177s]
Computing hash join...  [0.068s]
Building seed filter...  [0.013s]
Searching alignments...  [0.273s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.179s]
Building query seed array...  [0.165s]
Computing hash join...  [0.055s]
Building seed filter...  [0.015s]
Searching alignments...  [0.376s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.197s]
Building query seed array...  [0.131s]
Computing hash join...  [0.095s]
Building seed filter...  [0.015s]
Searching alignments...  [0.297s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.204s]
Building query seed array...  [0.227s]
Computing hash join...  [0.073s]
Building seed filter...  [0.024s]
Searching alignments...  [0.336s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.263s]
Computing hash join...  [0.068s]
Building seed filter...  [0.022s]
Searching alignments...  [0.314s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.135s]
Computing hash join...  [0.089s]
Building seed filter...  [0.013s]
Searching alignments...  [0.269s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.196s]
Building query seed array...  [0.204s]
Computing hash join...  [0.065s]
Building seed filter...  [0.013s]
Searching alignments...  [0.259s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.244s]
Building query seed array...  [0.16s]
Computing hash join...  [0.072s]
Building seed filter...  [0.011s]
Searching alignments...  [0.308s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.231s]
Building query seed array...  [0.154s]
Computing hash join...  [0.062s]
Building seed filter...  [0.016s]
Searching alignments...  [0.315s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.221s]
Building query seed array...  [0.187s]
Computing hash join...  [0.055s]
Building seed filter...  [0.012s]
Searching alignments...  [0.265s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.062s]
Computing alignments...  [10.484s]
Deallocating reference...  [0s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0.001s]
Deallocating queries...  [0.001s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.015s]
Closing the database file...  [0s]
Deallocating taxonomy...  [0s]
Total time = 69.528s
Reported 376918 pairwise alignments, 376918 HSPs.
28149 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
...........................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 289279x289279 matrix with 5934678 entries to stream <out/merged.mci>
[mclIO] wrote 289279 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 289279 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 289279x289279 matrix with 5934678 entries
[mcl] pid 20276
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  57.69  3.34 1.00/0.02/4.44 2.91 2.68 2.68   0
  2  ...................  69.89 15.94 0.86/0.08/4.58 4.08 0.84 2.25   5
  3  ...................  45.35  9.40 0.83/0.08/6.96 2.58 0.67 1.51   2
  4  ...................  25.66  3.46 0.82/0.09/10.22 1.58 0.71 1.07   0
  5  ...................  19.47  1.57 0.82/0.10/5.23 1.23 0.70 0.75   0
  6  ...................  10.53  0.84 0.82/0.10/3.59 1.09 0.72 0.55   0
  7  ...................   9.04  0.53 0.82/0.14/2.46 1.03 0.78 0.42   0
  8  ...................   5.97  0.40 0.84/0.20/2.58 1.01 0.81 0.34   0
  9  ...................   5.08  0.33 0.86/0.20/1.49 1.01 0.81 0.28   0
 10  ...................   5.11  0.26 0.89/0.25/1.25 1.00 0.81 0.23   0
 11  ...................   4.31  0.22 0.93/0.23/1.17 1.00 0.82 0.18   0
 12  ...................   4.95  0.19 0.95/0.21/1.18 1.00 0.83 0.15   0
 13  ...................   4.96  0.17 0.97/0.27/1.01 1.00 0.86 0.13   0
 14  ...................   4.07  0.16 0.98/0.19/1.00 1.00 0.90 0.12   0
 15  ...................   4.66  0.15 0.99/0.24/1.00 1.00 0.93 0.11   0
 16  ...................   3.57  0.14 0.99/0.28/1.00 1.00 0.96 0.10   0
 17  ...................   2.11  0.13 1.00/0.35/1.00 1.00 0.97 0.10   0
 18  ...................   3.87  0.13 1.00/0.26/1.00 1.00 0.98 0.10   0
 19  ...................   1.49  0.13 1.00/0.45/1.00 1.00 0.99 0.10   0
 20  ...................   1.07  0.13 1.00/0.45/1.00 1.00 1.00 0.10   0
 21  ...................   1.09  0.12 1.00/0.58/1.00 1.00 1.00 0.10   0
 22  ...................   0.75  0.13 1.00/0.54/1.00 1.00 1.00 0.10   0
 23  ...................   0.43  0.13 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.15  0.13 1.00/0.85/1.00 1.00 1.00 0.10   0
 25  ...................   0.03  0.14 1.00/0.96/1.00 1.00 1.00 0.10   0
 26  ...................   0.00  0.13 1.00/1.00/1.00 1.00 1.00 0.10   0
 27  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 33991 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3560 entries from out/pcs_contigs.csv
Read 280581 entries (dropped 2271 singletons) from out/Cyber_profiles.csv
.......... 0.75% 10000/1338225.0
.......... 1.49% 20000/1338225.0
.......... 2.24% 30000/1338225.0
.......... 2.99% 40000/1338225.0
.......... 3.74% 50000/1338225.0
.......... 4.48% 60000/1338225.0
.......... 5.23% 70000/1338225.0
.......... 5.98% 80000/1338225.0
.......... 6.73% 90000/1338225.0
.......... 7.47% 100000/1338225.0
.......... 8.22% 110000/1338225.0
.......... 8.97% 120000/1338225.0
.......... 9.71% 130000/1338225.0
..........10.46% 140000/1338225.0
..........11.21% 150000/1338225.0
..........11.96% 160000/1338225.0
..........12.70% 170000/1338225.0
..........13.45% 180000/1338225.0
..........14.20% 190000/1338225.0
..........14.95% 200000/1338225.0
..........15.69% 210000/1338225.0
..........16.44% 220000/1338225.0
..........17.19% 230000/1338225.0
..........17.93% 240000/1338225.0
..........18.68% 250000/1338225.0
..........19.43% 260000/1338225.0
..........20.18% 270000/1338225.0
..........20.92% 280000/1338225.0
..........21.67% 290000/1338225.0
..........22.42% 300000/1338225.0
..........23.17% 310000/1338225.0
..........23.91% 320000/1338225.0
..........24.66% 330000/1338225.0
..........25.41% 340000/1338225.0
..........26.15% 350000/1338225.0
..........26.90% 360000/1338225.0
..........27.65% 370000/1338225.0
..........28.40% 380000/1338225.0
..........29.14% 390000/1338225.0
..........29.89% 400000/1338225.0
..........30.64% 410000/1338225.0
..........31.38% 420000/1338225.0
..........32.13% 430000/1338225.0
..........32.88% 440000/1338225.0
..........33.63% 450000/1338225.0
..........34.37% 460000/1338225.0
..........35.12% 470000/1338225.0
..........35.87% 480000/1338225.0
..........36.62% 490000/1338225.0
..........37.36% 500000/1338225.0
..........38.11% 510000/1338225.0
..........38.86% 520000/1338225.0
..........39.60% 530000/1338225.0
..........40.35% 540000/1338225.0
..........41.10% 550000/1338225.0
..........41.85% 560000/1338225.0
..........42.59% 570000/1338225.0
..........43.34% 580000/1338225.0
..........44.09% 590000/1338225.0
..........44.84% 600000/1338225.0
..........45.58% 610000/1338225.0
..........46.33% 620000/1338225.0
..........47.08% 630000/1338225.0
..........47.82% 640000/1338225.0
..........48.57% 650000/1338225.0
..........49.32% 660000/1338225.0
..........50.07% 670000/1338225.0
..........50.81% 680000/1338225.0
..........51.56% 690000/1338225.0
..........52.31% 700000/1338225.0
..........53.06% 710000/1338225.0
..........53.80% 720000/1338225.0
..........54.55% 730000/1338225.0
..........55.30% 740000/1338225.0
..........56.04% 750000/1338225.0
..........56.79% 760000/1338225.0
..........57.54% 770000/1338225.0
..........58.29% 780000/1338225.0
..........59.03% 790000/1338225.0
..........59.78% 800000/1338225.0
..........60.53% 810000/1338225.0
..........61.28% 820000/1338225.0
..........62.02% 830000/1338225.0
..........62.77% 840000/1338225.0
..........63.52% 850000/1338225.0
..........64.26% 860000/1338225.0
..........65.01% 870000/1338225.0
..........65.76% 880000/1338225.0
..........66.51% 890000/1338225.0
..........67.25% 900000/1338225.0
..........68.00% 910000/1338225.0
..........68.75% 920000/1338225.0
..........69.50% 930000/1338225.0
..........70.24% 940000/1338225.0
..........70.99% 950000/1338225.0
..........71.74% 960000/1338225.0
..........72.48% 970000/1338225.0
..........73.23% 980000/1338225.0
..........73.98% 990000/1338225.0
..........74.73% 1000000/1338225.0
..........75.47% 1010000/1338225.0
..........76.22% 1020000/1338225.0
..........76.97% 1030000/1338225.0
..........77.71% 1040000/1338225.0
..........78.46% 1050000/1338225.0
..........79.21% 1060000/1338225.0
..........79.96% 1070000/1338225.0
..........80.70% 1080000/1338225.0
..........81.45% 1090000/1338225.0
..........82.20% 1100000/1338225.0
..........82.95% 1110000/1338225.0
..........83.69% 1120000/1338225.0
..........84.44% 1130000/1338225.0
..........85.19% 1140000/1338225.0
..........85.93% 1150000/1338225.0
..........86.68% 1160000/1338225.0
..........87.43% 1170000/1338225.0
..........88.18% 1180000/1338225.0
..........88.92% 1190000/1338225.0
..........89.67% 1200000/1338225.0
..........90.42% 1210000/1338225.0
..........91.17% 1220000/1338225.0
..........91.91% 1230000/1338225.0
..........92.66% 1240000/1338225.0
..........93.41% 1250000/1338225.0
..........94.15% 1260000/1338225.0
..........94.90% 1270000/1338225.0
..........95.65% 1280000/1338225.0
..........96.40% 1290000/1338225.0
..........97.14% 1300000/1338225.0
..........97.89% 1310000/1338225.0
..........98.64% 1320000/1338225.0
..........99.39% 1330000/1338225.0
....Hypergeometric contig-similarity network:
       3560 contigs,
     162314 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (162314 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3522, 3522)
features: (3522, 512)
y: (3522,) (3522,)
mask: (3522,) (3522,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3521, 3521, 3521],
                       [ 466,  463,  450,  ...,   52,   34,   32]]),
       values=tensor([0.0016, 0.1601, 0.0182,  ..., 0.1164, 0.0573, 0.0948]),
       device='cuda:0', size=(3522, 512), nnz=86086, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    4,  ..., 2818, 2820, 3521],
                       [   0,    0,    0,  ..., 3521, 3521, 3521]]),
       values=tensor([0.0167, 0.0204, 0.0159,  ..., 0.2041, 0.2041, 0.2500]),
       device='cuda:0', size=(3522, 3522), nnz=165934, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 86086
0 20.075977325439453 0.10516536603493125
10 16.23861312866211 0.5860274990709773
20 13.461138725280762 0.8324043106651803
30 11.143722534179688 0.8892604979561501
40 9.317190170288086 0.9115570419918246
50 7.981244087219238 0.9349684132292828
60 6.902570724487305 0.947603121516165
70 5.939167022705078 0.9576365663322185
80 5.114516735076904 0.9606094388703085
90 4.473162651062012 0.9587513935340022
100 3.8775267601013184 0.9639539204756596
110 3.4166929721832275 0.9669267930137495
120 2.9910449981689453 0.9658119658119658
130 2.6437597274780273 0.9702712746191007
140 2.3345768451690674 0.9687848383500557
150 2.030514717102051 0.9646971386101821
160 1.7900410890579224 0.9635823114083983
170 1.6104812622070312 0.969156447417317
180 1.408889651298523 0.9698996655518395
190 1.310595154762268 0.9646971386101821
200 1.1587557792663574 0.969156447417317
210 1.0335588455200195 0.9717577108881457
220 0.9307536482810974 0.9702712746191007
230 0.874152660369873 0.9698996655518395
240 0.8396239876747131 0.9695280564845782
250 0.7256316542625427 0.966183574879227
260 0.6664689779281616 0.9695280564845782
270 0.6303421258926392 0.9635823114083983
280 0.5694597959518433 0.9665551839464883
290 0.5661153793334961 0.9695280564845782
300 0.5224008560180664 0.9680416202155333
310 0.4898955225944519 0.9684132292827945
320 0.4933531880378723 0.9658119658119658
330 0.4432305097579956 0.9721293199554069
340 0.4033758044242859 0.9665551839464883
350 0.40121394395828247 0.9695280564845782
360 0.43465861678123474 0.9728725380899294
370 0.39710569381713867 0.9684132292827945
380 0.38302820920944214 0.9717577108881457
390 0.39601200819015503 0.9736157562244518
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.05s]
Loading sequences...  [0.39s]
Masking sequences...  [0.447s]
Writing sequences...  [0.063s]
Hashing sequences...  [0.023s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 0.994s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.054s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.028s]
Opening the output file...  [0.001s]
Loading query sequences...  [0.533s]
Masking queries...  [0.722s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.197s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.133s]
Masking reference...  [0.615s]
Initializing temporary storage...  [0.034s]
Building reference histograms...  [1.588s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.25s]
Building query seed array...  [0.166s]
Computing hash join...  [0.08s]
Building seed filter...  [0.017s]
Searching alignments...  [0.415s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.231s]
Building query seed array...  [0.252s]
Computing hash join...  [0.089s]
Building seed filter...  [0.009s]
Searching alignments...  [0.257s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.268s]
Building query seed array...  [0.26s]
Computing hash join...  [0.118s]
Building seed filter...  [0.021s]
Searching alignments...  [0.39s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.172s]
Computing hash join...  [0.056s]
Building seed filter...  [0.01s]
Searching alignments...  [0.397s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.164s]
Building query seed array...  [0.127s]
Computing hash join...  [0.109s]
Building seed filter...  [0.019s]
Searching alignments...  [0.326s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.224s]
Building query seed array...  [0.224s]
Computing hash join...  [0.062s]
Building seed filter...  [0.018s]
Searching alignments...  [0.265s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.347s]
Building query seed array...  [0.34s]
Computing hash join...  [0.093s]
Building seed filter...  [0.014s]
Searching alignments...  [0.304s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.165s]
Building query seed array...  [0.143s]
Computing hash join...  [0.062s]
Building seed filter...  [0.018s]
Searching alignments...  [0.312s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.181s]
Building query seed array...  [0.22s]
Computing hash join...  [0.11s]
Building seed filter...  [0.027s]
Searching alignments...  [0.335s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.277s]
Building query seed array...  [0.234s]
Computing hash join...  [0.064s]
Building seed filter...  [0.021s]
Searching alignments...  [0.391s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.196s]
Building query seed array...  [0.3s]
Computing hash join...  [0.1s]
Building seed filter...  [0.015s]
Searching alignments...  [0.247s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.187s]
Building query seed array...  [0.168s]
Computing hash join...  [0.068s]
Building seed filter...  [0.014s]
Searching alignments...  [0.249s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.156s]
Building query seed array...  [0.205s]
Computing hash join...  [0.069s]
Building seed filter...  [0.014s]
Searching alignments...  [0.271s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.22s]
Building query seed array...  [0.134s]
Computing hash join...  [0.086s]
Building seed filter...  [0.012s]
Searching alignments...  [0.271s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.223s]
Building query seed array...  [0.14s]
Computing hash join...  [0.07s]
Building seed filter...  [0.013s]
Searching alignments...  [0.305s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.162s]
Computing hash join...  [0.054s]
Building seed filter...  [0.013s]
Searching alignments...  [0.24s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.183s]
Building query seed array...  [0.139s]
Computing hash join...  [0.127s]
Building seed filter...  [0.008s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.231s]
Building query seed array...  [0.187s]
Computing hash join...  [0.095s]
Building seed filter...  [0.027s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.239s]
Building query seed array...  [0.172s]
Computing hash join...  [0.057s]
Building seed filter...  [0.013s]
Searching alignments...  [0.294s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.172s]
Building query seed array...  [0.158s]
Computing hash join...  [0.058s]
Building seed filter...  [0.014s]
Searching alignments...  [0.326s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.252s]
Building query seed array...  [0.202s]
Computing hash join...  [0.126s]
Building seed filter...  [0.012s]
Searching alignments...  [0.287s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.238s]
Building query seed array...  [0.126s]
Computing hash join...  [0.103s]
Building seed filter...  [0.015s]
Searching alignments...  [0.256s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.234s]
Building query seed array...  [0.139s]
Computing hash join...  [0.092s]
Building seed filter...  [0.008s]
Searching alignments...  [0.243s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.21s]
Building query seed array...  [0.187s]
Computing hash join...  [0.082s]
Building seed filter...  [0.017s]
Searching alignments...  [0.274s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.171s]
Building query seed array...  [0.143s]
Computing hash join...  [0.053s]
Building seed filter...  [0.017s]
Searching alignments...  [0.372s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.21s]
Building query seed array...  [0.119s]
Computing hash join...  [0.116s]
Building seed filter...  [0.012s]
Searching alignments...  [0.257s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.233s]
Building query seed array...  [0.218s]
Computing hash join...  [0.053s]
Building seed filter...  [0.01s]
Searching alignments...  [0.269s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.162s]
Building query seed array...  [0.111s]
Computing hash join...  [0.108s]
Building seed filter...  [0.01s]
Searching alignments...  [0.285s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.188s]
Building query seed array...  [0.223s]
Computing hash join...  [0.183s]
Building seed filter...  [0.009s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.263s]
Building query seed array...  [0.192s]
Computing hash join...  [0.105s]
Building seed filter...  [0.01s]
Searching alignments...  [0.262s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.174s]
Building query seed array...  [0.238s]
Computing hash join...  [0.07s]
Building seed filter...  [0.019s]
Searching alignments...  [0.282s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.202s]
Computing hash join...  [0.054s]
Building seed filter...  [0.013s]
Searching alignments...  [0.244s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.125s]
Building query seed array...  [0.13s]
Computing hash join...  [0.074s]
Building seed filter...  [0.01s]
Searching alignments...  [0.254s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.162s]
Building query seed array...  [0.223s]
Computing hash join...  [0.053s]
Building seed filter...  [0.014s]
Searching alignments...  [0.323s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.228s]
Building query seed array...  [0.213s]
Computing hash join...  [0.065s]
Building seed filter...  [0.017s]
Searching alignments...  [0.293s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.206s]
Building query seed array...  [0.198s]
Computing hash join...  [0.062s]
Building seed filter...  [0.012s]
Searching alignments...  [0.236s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.178s]
Building query seed array...  [0.122s]
Computing hash join...  [0.09s]
Building seed filter...  [0.013s]
Searching alignments...  [0.331s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.267s]
Building query seed array...  [0.115s]
Computing hash join...  [0.118s]
Building seed filter...  [0.011s]
Searching alignments...  [0.306s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.175s]
Building query seed array...  [0.135s]
Computing hash join...  [0.055s]
Building seed filter...  [0.018s]
Searching alignments...  [0.27s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.179s]
Computing hash join...  [0.084s]
Building seed filter...  [0.019s]
Searching alignments...  [0.211s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.23s]
Building query seed array...  [0.199s]
Computing hash join...  [0.092s]
Building seed filter...  [0.021s]
Searching alignments...  [0.345s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.219s]
Computing hash join...  [0.111s]
Building seed filter...  [0.019s]
Searching alignments...  [0.296s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.293s]
Building query seed array...  [0.15s]
Computing hash join...  [0.086s]
Building seed filter...  [0.02s]
Searching alignments...  [0.261s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.214s]
Building query seed array...  [0.304s]
Computing hash join...  [0.053s]
Building seed filter...  [0.013s]
Searching alignments...  [0.232s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.178s]
Building query seed array...  [0.178s]
Computing hash join...  [0.053s]
Building seed filter...  [0.012s]
Searching alignments...  [0.23s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.193s]
Building query seed array...  [0.164s]
Computing hash join...  [0.056s]
Building seed filter...  [0.012s]
Searching alignments...  [0.253s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.287s]
Building query seed array...  [0.235s]
Computing hash join...  [0.086s]
Building seed filter...  [0.012s]
Searching alignments...  [0.301s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.177s]
Building query seed array...  [0.141s]
Computing hash join...  [0.065s]
Building seed filter...  [0.009s]
Searching alignments...  [0.308s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.163s]
Building query seed array...  [0.092s]
Computing hash join...  [0.114s]
Building seed filter...  [0.011s]
Searching alignments...  [0.246s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.236s]
Computing hash join...  [0.06s]
Building seed filter...  [0.016s]
Searching alignments...  [0.329s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.271s]
Building query seed array...  [0.262s]
Computing hash join...  [0.053s]
Building seed filter...  [0.011s]
Searching alignments...  [0.213s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.177s]
Building query seed array...  [0.091s]
Computing hash join...  [0.07s]
Building seed filter...  [0.011s]
Searching alignments...  [0.287s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.164s]
Building query seed array...  [0.195s]
Computing hash join...  [0.099s]
Building seed filter...  [0.01s]
Searching alignments...  [0.341s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.285s]
Building query seed array...  [0.249s]
Computing hash join...  [0.068s]
Building seed filter...  [0.01s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.288s]
Building query seed array...  [0.206s]
Computing hash join...  [0.053s]
Building seed filter...  [0.011s]
Searching alignments...  [0.249s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.166s]
Building query seed array...  [0.106s]
Computing hash join...  [0.136s]
Building seed filter...  [0.011s]
Searching alignments...  [0.309s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.162s]
Building query seed array...  [0.187s]
Computing hash join...  [0.086s]
Building seed filter...  [0.014s]
Searching alignments...  [0.294s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.184s]
Building query seed array...  [0.202s]
Computing hash join...  [0.105s]
Building seed filter...  [0.016s]
Searching alignments...  [0.299s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.234s]
Building query seed array...  [0.233s]
Computing hash join...  [0.069s]
Building seed filter...  [0.013s]
Searching alignments...  [0.225s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.164s]
Building query seed array...  [0.19s]
Computing hash join...  [0.061s]
Building seed filter...  [0.013s]
Searching alignments...  [0.321s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.163s]
Computing hash join...  [0.06s]
Building seed filter...  [0.014s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.142s]
Building query seed array...  [0.132s]
Computing hash join...  [0.06s]
Building seed filter...  [0.017s]
Searching alignments...  [0.258s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.216s]
Building query seed array...  [0.181s]
Computing hash join...  [0.054s]
Building seed filter...  [0.016s]
Searching alignments...  [0.214s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.159s]
Computing hash join...  [0.068s]
Building seed filter...  [0.016s]
Searching alignments...  [0.215s]
Deallocating buffers...  [0s]
Clearing query masking...  [0.049s]
Computing alignments...  [8.41s]
Deallocating reference...  [0s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0s]
Deallocating queries...  [0.001s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.01s]
Closing the database file...  [0s]
Deallocating taxonomy...  [0s]
Total time = 63.036s
Reported 320428 pairwise alignments, 320428 HSPs.
24034 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
........................
[mclIO] writing <out/merged.mci>
........................................
[mclIO] wrote native interchange 285120x285120 matrix with 5821698 entries to stream <out/merged.mci>
[mclIO] wrote 285120 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 285120 entries
[mclIO] reading <out/merged.mci>
........................................
[mclIO] read native interchange 285120x285120 matrix with 5821698 entries
[mcl] pid 7594
 ite --------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ....................  56.91  3.17 0.99/0.02/4.53 2.80 2.60 2.60   0
  2  ....................  79.59 14.56 0.86/0.08/4.58 3.95 0.84 2.19   5
  3  ....................  44.68  8.83 0.83/0.08/5.66 2.53 0.68 1.50   2
  4  ....................  24.78  3.37 0.82/0.11/9.55 1.56 0.72 1.07   0
  5  ....................  20.08  1.49 0.82/0.09/5.26 1.22 0.71 0.76   0
  6  ....................  12.74  0.82 0.82/0.12/3.59 1.09 0.73 0.55   0
  7  ....................   8.67  0.53 0.82/0.14/2.69 1.03 0.78 0.43   0
  8  ....................   6.87  0.39 0.83/0.20/2.44 1.01 0.81 0.35   0
  9  ....................   5.86  0.32 0.86/0.18/2.36 1.00 0.81 0.28   0
 10  ....................   4.60  0.27 0.89/0.27/1.49 1.00 0.81 0.23   0
 11  ....................   4.31  0.22 0.93/0.23/1.11 1.00 0.81 0.19   0
 12  ....................   4.81  0.18 0.95/0.20/1.18 1.00 0.83 0.15   0
 13  ....................   4.96  0.16 0.97/0.25/1.01 1.00 0.86 0.13   0
 14  ....................   4.62  0.16 0.98/0.24/1.00 1.00 0.90 0.12   0
 15  ....................   4.65  0.14 0.99/0.24/1.00 1.00 0.93 0.11   0
 16  ....................   3.80  0.13 0.99/0.25/1.00 1.00 0.95 0.11   0
 17  ....................   2.84  0.14 1.00/0.21/1.00 1.00 0.97 0.10   0
 18  ....................   4.59  0.13 1.00/0.33/1.00 1.00 0.98 0.10   0
 19  ....................   2.41  0.14 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ....................   1.05  0.12 1.00/0.46/1.00 1.00 1.00 0.10   0
 21  ....................   1.11  0.12 1.00/0.57/1.00 1.00 1.00 0.10   0
 22  ....................   0.43  0.13 1.00/0.69/1.00 1.00 1.00 0.10   0
 23  ....................   0.25  0.12 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ....................   0.18  0.13 1.00/0.85/1.00 1.00 1.00 0.10   0
 25  ....................   0.25  0.12 1.00/0.77/1.00 1.00 1.00 0.10   0
 26  ....................   0.18  0.13 1.00/0.82/1.00 1.00 1.00 0.10   0
 27  ....................   0.03  0.13 1.00/0.97/1.00 1.00 1.00 0.10   0
 28  ....................   0.00  0.12 1.00/1.00/1.00 1.00 1.00 0.10   0
 29  ....................   0.00  0.12 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 33932 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3458 entries from out/pcs_contigs.csv
Read 276800 entries (dropped 2275 singletons) from out/Cyber_profiles.csv
.......... 0.84% 10000/1189819.0
.......... 1.68% 20000/1189819.0
.......... 2.52% 30000/1189819.0
.......... 3.36% 40000/1189819.0
.......... 4.20% 50000/1189819.0
.......... 5.04% 60000/1189819.0
.......... 5.88% 70000/1189819.0
.......... 6.72% 80000/1189819.0
.......... 7.56% 90000/1189819.0
.......... 8.40% 100000/1189819.0
.......... 9.25% 110000/1189819.0
..........10.09% 120000/1189819.0
..........10.93% 130000/1189819.0
..........11.77% 140000/1189819.0
..........12.61% 150000/1189819.0
..........13.45% 160000/1189819.0
..........14.29% 170000/1189819.0
..........15.13% 180000/1189819.0
..........15.97% 190000/1189819.0
..........16.81% 200000/1189819.0
..........17.65% 210000/1189819.0
..........18.49% 220000/1189819.0
..........19.33% 230000/1189819.0
..........20.17% 240000/1189819.0
..........21.01% 250000/1189819.0
..........21.85% 260000/1189819.0
..........22.69% 270000/1189819.0
..........23.53% 280000/1189819.0
..........24.37% 290000/1189819.0
..........25.21% 300000/1189819.0
..........26.05% 310000/1189819.0
..........26.89% 320000/1189819.0
..........27.74% 330000/1189819.0
..........28.58% 340000/1189819.0
..........29.42% 350000/1189819.0
..........30.26% 360000/1189819.0
..........31.10% 370000/1189819.0
..........31.94% 380000/1189819.0
..........32.78% 390000/1189819.0
..........33.62% 400000/1189819.0
..........34.46% 410000/1189819.0
..........35.30% 420000/1189819.0
..........36.14% 430000/1189819.0
..........36.98% 440000/1189819.0
..........37.82% 450000/1189819.0
..........38.66% 460000/1189819.0
..........39.50% 470000/1189819.0
..........40.34% 480000/1189819.0
..........41.18% 490000/1189819.0
..........42.02% 500000/1189819.0
..........42.86% 510000/1189819.0
..........43.70% 520000/1189819.0
..........44.54% 530000/1189819.0
..........45.39% 540000/1189819.0
..........46.23% 550000/1189819.0
..........47.07% 560000/1189819.0
..........47.91% 570000/1189819.0
..........48.75% 580000/1189819.0
..........49.59% 590000/1189819.0
..........50.43% 600000/1189819.0
..........51.27% 610000/1189819.0
..........52.11% 620000/1189819.0
..........52.95% 630000/1189819.0
..........53.79% 640000/1189819.0
..........54.63% 650000/1189819.0
..........55.47% 660000/1189819.0
..........56.31% 670000/1189819.0
..........57.15% 680000/1189819.0
..........57.99% 690000/1189819.0
..........58.83% 700000/1189819.0
..........59.67% 710000/1189819.0
..........60.51% 720000/1189819.0
..........61.35% 730000/1189819.0
..........62.19% 740000/1189819.0
..........63.03% 750000/1189819.0
..........63.88% 760000/1189819.0
..........64.72% 770000/1189819.0
..........65.56% 780000/1189819.0
..........66.40% 790000/1189819.0
..........67.24% 800000/1189819.0
..........68.08% 810000/1189819.0
..........68.92% 820000/1189819.0
..........69.76% 830000/1189819.0
..........70.60% 840000/1189819.0
..........71.44% 850000/1189819.0
..........72.28% 860000/1189819.0
..........73.12% 870000/1189819.0
..........73.96% 880000/1189819.0
..........74.80% 890000/1189819.0
..........75.64% 900000/1189819.0
..........76.48% 910000/1189819.0
..........77.32% 920000/1189819.0
..........78.16% 930000/1189819.0
..........79.00% 940000/1189819.0
..........79.84% 950000/1189819.0
..........80.68% 960000/1189819.0
..........81.53% 970000/1189819.0
..........82.37% 980000/1189819.0
..........83.21% 990000/1189819.0
..........84.05% 1000000/1189819.0
..........84.89% 1010000/1189819.0
..........85.73% 1020000/1189819.0
..........86.57% 1030000/1189819.0
..........87.41% 1040000/1189819.0
..........88.25% 1050000/1189819.0
..........89.09% 1060000/1189819.0
..........89.93% 1070000/1189819.0
..........90.77% 1080000/1189819.0
..........91.61% 1090000/1189819.0
..........92.45% 1100000/1189819.0
..........93.29% 1110000/1189819.0
..........94.13% 1120000/1189819.0
..........94.97% 1130000/1189819.0
..........95.81% 1140000/1189819.0
..........96.65% 1150000/1189819.0
..........97.49% 1160000/1189819.0
..........98.33% 1170000/1189819.0
..........99.17% 1180000/1189819.0
......Hypergeometric contig-similarity network:
       3458 contigs,
     155156 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (155156 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3400, 3400)
features: (3400, 512)
y: (3400,) (3400,)
mask: (3400,) (3400,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3399, 3399, 3399],
                       [ 466,  463,  447,  ...,   52,   34,   32]]),
       values=tensor([0.0002, 0.0689, 0.0942,  ..., 0.0831, 0.1464, 0.0149]),
       device='cuda:0', size=(3400, 512), nnz=82573, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    2,  ..., 2146, 2147, 3399],
                       [   0,    0,    0,  ..., 3399, 3399, 3399]]),
       values=tensor([0.0103, 0.0102, 0.0338,  ..., 0.1118, 0.1118, 0.2000]),
       device='cuda:0', size=(3400, 3400), nnz=158574, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 82573
0 20.03146743774414 0.11519881085098477
10 16.203845977783203 0.6131549609810479
20 13.332326889038086 0.8234856930509105
30 11.111892700195312 0.8807134894091416
40 9.358821868896484 0.9115570419918246
50 8.025867462158203 0.9416573764399851
60 6.924361228942871 0.9394277220364177
70 5.937891483306885 0.9479747305834262
80 5.170968055725098 0.9468599033816425
90 4.477915287017822 0.9542920847268673
100 3.8597447872161865 0.961352657004831
110 3.41725492477417 0.9654403567447045
120 2.956946849822998 0.9680416202155333
130 2.6227545738220215 0.9628390932738758
140 2.309314250946045 0.9658119658119658
150 2.059826612472534 0.969156447417317
160 1.8223975896835327 0.9650687476774433
170 1.6212975978851318 0.9646971386101821
180 1.4489829540252686 0.9665551839464883
190 1.3258705139160156 0.9646971386101821
200 1.1650011539459229 0.9658119658119658
210 1.0596336126327515 0.967670011148272
220 0.9511301517486572 0.9695280564845782
230 0.8919469118118286 0.9680416202155333
240 0.7983980178833008 0.9650687476774433
250 0.7435263395309448 0.9654403567447045
260 0.679499626159668 0.9669267930137495
270 0.6377561092376709 0.966183574879227
280 0.5742487907409668 0.9672984020810108
290 0.5568532347679138 0.9684132292827945
300 0.5134397149085999 0.9658119658119658
310 0.47529566287994385 0.969156447417317
320 0.47376975417137146 0.9695280564845782
330 0.45523667335510254 0.9665551839464883
340 0.4645851254463196 0.9669267930137495
350 0.39142224192619324 0.9687848383500557
360 0.4179391860961914 0.970642883686362
370 0.35468053817749023 0.9702712746191007
380 0.39298194646835327 0.9721293199554069
390 0.35231831669807434 0.9680416202155333
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.055s]
Loading sequences...  [0.409s]
Masking sequences...  [0.446s]
Writing sequences...  [0.065s]
Hashing sequences...  [0.022s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0.001s]
Closing the database file...  [0.008s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.019s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.053s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.028s]
Opening the output file...  [0s]
Loading query sequences...  [0.705s]
Masking queries...  [1.024s]
Building query seed set...  [0.004s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.819s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.141s]
Masking reference...  [0.481s]
Initializing temporary storage...  [0.03s]
Building reference histograms...  [1.303s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.193s]
Building query seed array...  [0.187s]
Computing hash join...  [0.104s]
Building seed filter...  [0.015s]
Searching alignments...  [0.297s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.211s]
Building query seed array...  [0.163s]
Computing hash join...  [0.109s]
Building seed filter...  [0.019s]
Searching alignments...  [0.322s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.208s]
Building query seed array...  [0.215s]
Computing hash join...  [0.072s]
Building seed filter...  [0.01s]
Searching alignments...  [0.311s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.119s]
Building query seed array...  [0.353s]
Computing hash join...  [0.1s]
Building seed filter...  [0.024s]
Searching alignments...  [0.352s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.173s]
Building query seed array...  [0.228s]
Computing hash join...  [0.092s]
Building seed filter...  [0.012s]
Searching alignments...  [0.28s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.268s]
Building query seed array...  [0.242s]
Computing hash join...  [0.076s]
Building seed filter...  [0.019s]
Searching alignments...  [0.295s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.303s]
Building query seed array...  [0.148s]
Computing hash join...  [0.067s]
Building seed filter...  [0.01s]
Searching alignments...  [0.306s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.196s]
Building query seed array...  [0.187s]
Computing hash join...  [0.129s]
Building seed filter...  [0.014s]
Searching alignments...  [0.258s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.168s]
Building query seed array...  [0.126s]
Computing hash join...  [0.107s]
Building seed filter...  [0.019s]
Searching alignments...  [0.327s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.271s]
Building query seed array...  [0.236s]
Computing hash join...  [0.056s]
Building seed filter...  [0.017s]
Searching alignments...  [0.314s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.216s]
Building query seed array...  [0.272s]
Computing hash join...  [0.078s]
Building seed filter...  [0.013s]
Searching alignments...  [0.372s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.112s]
Building query seed array...  [0.204s]
Computing hash join...  [0.076s]
Building seed filter...  [0.022s]
Searching alignments...  [0.364s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.283s]
Building query seed array...  [0.128s]
Computing hash join...  [0.11s]
Building seed filter...  [0.016s]
Searching alignments...  [0.323s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.273s]
Building query seed array...  [0.152s]
Computing hash join...  [0.07s]
Building seed filter...  [0.012s]
Searching alignments...  [0.37s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.222s]
Building query seed array...  [0.289s]
Computing hash join...  [0.115s]
Building seed filter...  [0.014s]
Searching alignments...  [0.266s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.221s]
Building query seed array...  [0.182s]
Computing hash join...  [0.087s]
Building seed filter...  [0.018s]
Searching alignments...  [0.283s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.144s]
Building query seed array...  [0.177s]
Computing hash join...  [0.073s]
Building seed filter...  [0.015s]
Searching alignments...  [0.306s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.23s]
Building query seed array...  [0.233s]
Computing hash join...  [0.055s]
Building seed filter...  [0.028s]
Searching alignments...  [0.258s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.292s]
Building query seed array...  [0.196s]
Computing hash join...  [0.104s]
Building seed filter...  [0.015s]
Searching alignments...  [0.281s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.145s]
Building query seed array...  [0.268s]
Computing hash join...  [0.125s]
Building seed filter...  [0.019s]
Searching alignments...  [0.358s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.117s]
Building query seed array...  [0.201s]
Computing hash join...  [0.088s]
Building seed filter...  [0.022s]
Searching alignments...  [0.347s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.215s]
Building query seed array...  [0.288s]
Computing hash join...  [0.082s]
Building seed filter...  [0.022s]
Searching alignments...  [0.475s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.221s]
Building query seed array...  [0.243s]
Computing hash join...  [0.149s]
Building seed filter...  [0.018s]
Searching alignments...  [0.305s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.209s]
Building query seed array...  [0.12s]
Computing hash join...  [0.116s]
Building seed filter...  [0.014s]
Searching alignments...  [0.457s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.271s]
Building query seed array...  [0.234s]
Computing hash join...  [0.062s]
Building seed filter...  [0.021s]
Searching alignments...  [0.3s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.211s]
Building query seed array...  [0.205s]
Computing hash join...  [0.11s]
Building seed filter...  [0.012s]
Searching alignments...  [0.296s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.157s]
Building query seed array...  [0.217s]
Computing hash join...  [0.091s]
Building seed filter...  [0.01s]
Searching alignments...  [0.237s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.199s]
Building query seed array...  [0.152s]
Computing hash join...  [0.072s]
Building seed filter...  [0.013s]
Searching alignments...  [0.304s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.215s]
Building query seed array...  [0.205s]
Computing hash join...  [0.085s]
Building seed filter...  [0.011s]
Searching alignments...  [0.284s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.216s]
Building query seed array...  [0.239s]
Computing hash join...  [0.088s]
Building seed filter...  [0.013s]
Searching alignments...  [0.271s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.239s]
Building query seed array...  [0.152s]
Computing hash join...  [0.075s]
Building seed filter...  [0.018s]
Searching alignments...  [0.288s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.258s]
Building query seed array...  [0.167s]
Computing hash join...  [0.061s]
Building seed filter...  [0.013s]
Searching alignments...  [0.302s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.152s]
Building query seed array...  [0.144s]
Computing hash join...  [0.107s]
Building seed filter...  [0.011s]
Searching alignments...  [0.292s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.25s]
Computing hash join...  [0.066s]
Building seed filter...  [0.015s]
Searching alignments...  [0.427s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.317s]
Building query seed array...  [0.25s]
Computing hash join...  [0.069s]
Building seed filter...  [0.02s]
Searching alignments...  [0.275s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.185s]
Building query seed array...  [0.197s]
Computing hash join...  [0.108s]
Building seed filter...  [0.015s]
Searching alignments...  [0.392s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.227s]
Building query seed array...  [0.206s]
Computing hash join...  [0.081s]
Building seed filter...  [0.015s]
Searching alignments...  [0.264s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.192s]
Building query seed array...  [0.144s]
Computing hash join...  [0.059s]
Building seed filter...  [0.014s]
Searching alignments...  [0.357s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.254s]
Building query seed array...  [0.26s]
Computing hash join...  [0.07s]
Building seed filter...  [0.02s]
Searching alignments...  [0.271s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.176s]
Building query seed array...  [0.195s]
Computing hash join...  [0.088s]
Building seed filter...  [0.014s]
Searching alignments...  [0.325s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.195s]
Building query seed array...  [0.169s]
Computing hash join...  [0.073s]
Building seed filter...  [0.018s]
Searching alignments...  [0.29s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.234s]
Building query seed array...  [0.258s]
Computing hash join...  [0.065s]
Building seed filter...  [0.011s]
Searching alignments...  [0.308s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.156s]
Building query seed array...  [0.262s]
Computing hash join...  [0.096s]
Building seed filter...  [0.017s]
Searching alignments...  [0.261s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.175s]
Building query seed array...  [0.187s]
Computing hash join...  [0.058s]
Building seed filter...  [0.013s]
Searching alignments...  [0.34s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.184s]
Building query seed array...  [0.218s]
Computing hash join...  [0.056s]
Building seed filter...  [0.012s]
Searching alignments...  [0.307s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.211s]
Building query seed array...  [0.266s]
Computing hash join...  [0.083s]
Building seed filter...  [0.01s]
Searching alignments...  [0.278s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.307s]
Building query seed array...  [0.23s]
Computing hash join...  [0.061s]
Building seed filter...  [0.011s]
Searching alignments...  [0.305s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.209s]
Building query seed array...  [0.14s]
Computing hash join...  [0.089s]
Building seed filter...  [0.015s]
Searching alignments...  [0.303s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.155s]
Building query seed array...  [0.167s]
Computing hash join...  [0.068s]
Building seed filter...  [0.018s]
Searching alignments...  [0.35s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.327s]
Building query seed array...  [0.211s]
Computing hash join...  [0.06s]
Building seed filter...  [0.011s]
Searching alignments...  [0.266s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.208s]
Building query seed array...  [0.267s]
Computing hash join...  [0.104s]
Building seed filter...  [0.015s]
Searching alignments...  [0.269s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.131s]
Building query seed array...  [0.203s]
Computing hash join...  [0.078s]
Building seed filter...  [0.015s]
Searching alignments...  [0.28s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.183s]
Building query seed array...  [0.24s]
Computing hash join...  [0.147s]
Building seed filter...  [0.021s]
Searching alignments...  [0.289s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.163s]
Building query seed array...  [0.307s]
Computing hash join...  [0.107s]
Building seed filter...  [0.022s]
Searching alignments...  [0.323s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.209s]
Building query seed array...  [0.16s]
Computing hash join...  [0.162s]
Building seed filter...  [0.014s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.157s]
Building query seed array...  [0.112s]
Computing hash join...  [0.097s]
Building seed filter...  [0.011s]
Searching alignments...  [0.322s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.213s]
Building query seed array...  [0.186s]
Computing hash join...  [0.079s]
Building seed filter...  [0.017s]
Searching alignments...  [0.325s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.248s]
Building query seed array...  [0.155s]
Computing hash join...  [0.075s]
Building seed filter...  [0.018s]
Searching alignments...  [0.283s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.166s]
Computing hash join...  [0.17s]
Building seed filter...  [0.015s]
Searching alignments...  [0.332s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.175s]
Building query seed array...  [0.159s]
Computing hash join...  [0.063s]
Building seed filter...  [0.021s]
Searching alignments...  [0.294s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.206s]
Building query seed array...  [0.238s]
Computing hash join...  [0.115s]
Building seed filter...  [0.02s]
Searching alignments...  [0.415s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.243s]
Building query seed array...  [0.202s]
Computing hash join...  [0.087s]
Building seed filter...  [0.015s]
Searching alignments...  [0.29s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.224s]
Building query seed array...  [0.235s]
Computing hash join...  [0.07s]
Building seed filter...  [0.017s]
Searching alignments...  [0.248s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.215s]
Building query seed array...  [0.171s]
Computing hash join...  [0.062s]
Building seed filter...  [0.024s]
Searching alignments...  [0.336s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.058s]
Computing alignments...  [9.904s]
Deallocating reference...  [0.001s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0s]
Deallocating queries...  [0.003s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.011s]
Closing the database file...  [0s]
Deallocating taxonomy...  [0s]
Total time = 69.321s
Reported 357374 pairwise alignments, 357374 HSPs.
26645 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
..........................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 287642x287642 matrix with 5895590 entries to stream <out/merged.mci>
[mclIO] wrote 287642 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 287642 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 287642x287642 matrix with 5895590 entries
[mcl] pid 26113
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  61.85  3.54 1.00/0.02/4.70 2.95 2.72 2.72   0
  2  ...................  79.37 17.37 0.86/0.08/4.58 4.06 0.83 2.27   5
  3  ...................  47.16 10.11 0.83/0.08/6.01 2.60 0.66 1.50   2
  4  ...................  23.72  3.51 0.83/0.14/10.47 1.58 0.71 1.07   0
  5  ...................  17.97  1.64 0.82/0.11/5.38 1.23 0.71 0.75   0
  6  ...................  10.06  0.87 0.82/0.12/3.59 1.09 0.73 0.55   0
  7  ...................   8.67  0.57 0.82/0.14/2.45 1.03 0.78 0.42   0
  8  ...................   6.19  0.43 0.84/0.17/2.11 1.01 0.81 0.34   0
  9  ...................   4.79  0.34 0.86/0.18/2.44 1.01 0.81 0.28   0
 10  ...................   5.11  0.28 0.89/0.23/2.44 1.00 0.81 0.23   0
 11  ...................   4.31  0.23 0.93/0.23/1.18 1.00 0.81 0.18   0
 12  ...................   4.47  0.21 0.95/0.19/1.18 1.00 0.83 0.15   0
 13  ...................   4.96  0.22 0.97/0.18/1.40 1.00 0.86 0.13   0
 14  ...................   4.26  0.19 0.98/0.21/1.00 1.00 0.90 0.12   0
 15  ...................   4.65  0.15 0.99/0.24/1.00 1.00 0.93 0.11   0
 16  ...................   3.57  0.14 0.99/0.28/1.00 1.00 0.95 0.10   0
 17  ...................   3.60  0.14 1.00/0.34/1.00 1.00 0.97 0.10   0
 18  ...................   1.73  0.14 1.00/0.33/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.13 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.05  0.14 1.00/0.46/1.00 1.00 1.00 0.10   0
 21  ...................   1.11  0.14 1.00/0.57/1.00 1.00 1.00 0.10   0
 22  ...................   0.42  0.14 1.00/0.70/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.13 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.15  0.14 1.00/0.85/1.00 1.00 1.00 0.10   0
 25  ...................   0.03  0.13 1.00/0.96/1.00 1.00 1.00 0.10   0
 26  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
 27  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 33846 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3549 entries from out/pcs_contigs.csv
Read 279181 entries (dropped 2293 singletons) from out/Cyber_profiles.csv
.......... 0.74% 10000/1353222.0
.......... 1.48% 20000/1353222.0
.......... 2.22% 30000/1353222.0
.......... 2.96% 40000/1353222.0
.......... 3.69% 50000/1353222.0
.......... 4.43% 60000/1353222.0
.......... 5.17% 70000/1353222.0
.......... 5.91% 80000/1353222.0
.......... 6.65% 90000/1353222.0
.......... 7.39% 100000/1353222.0
.......... 8.13% 110000/1353222.0
.......... 8.87% 120000/1353222.0
.......... 9.61% 130000/1353222.0
..........10.35% 140000/1353222.0
..........11.08% 150000/1353222.0
..........11.82% 160000/1353222.0
..........12.56% 170000/1353222.0
..........13.30% 180000/1353222.0
..........14.04% 190000/1353222.0
..........14.78% 200000/1353222.0
..........15.52% 210000/1353222.0
..........16.26% 220000/1353222.0
..........17.00% 230000/1353222.0
..........17.74% 240000/1353222.0
..........18.47% 250000/1353222.0
..........19.21% 260000/1353222.0
..........19.95% 270000/1353222.0
..........20.69% 280000/1353222.0
..........21.43% 290000/1353222.0
..........22.17% 300000/1353222.0
..........22.91% 310000/1353222.0
..........23.65% 320000/1353222.0
..........24.39% 330000/1353222.0
..........25.13% 340000/1353222.0
..........25.86% 350000/1353222.0
..........26.60% 360000/1353222.0
..........27.34% 370000/1353222.0
..........28.08% 380000/1353222.0
..........28.82% 390000/1353222.0
..........29.56% 400000/1353222.0
..........30.30% 410000/1353222.0
..........31.04% 420000/1353222.0
..........31.78% 430000/1353222.0
..........32.51% 440000/1353222.0
..........33.25% 450000/1353222.0
..........33.99% 460000/1353222.0
..........34.73% 470000/1353222.0
..........35.47% 480000/1353222.0
..........36.21% 490000/1353222.0
..........36.95% 500000/1353222.0
..........37.69% 510000/1353222.0
..........38.43% 520000/1353222.0
..........39.17% 530000/1353222.0
..........39.90% 540000/1353222.0
..........40.64% 550000/1353222.0
..........41.38% 560000/1353222.0
..........42.12% 570000/1353222.0
..........42.86% 580000/1353222.0
..........43.60% 590000/1353222.0
..........44.34% 600000/1353222.0
..........45.08% 610000/1353222.0
..........45.82% 620000/1353222.0
..........46.56% 630000/1353222.0
..........47.29% 640000/1353222.0
..........48.03% 650000/1353222.0
..........48.77% 660000/1353222.0
..........49.51% 670000/1353222.0
..........50.25% 680000/1353222.0
..........50.99% 690000/1353222.0
..........51.73% 700000/1353222.0
..........52.47% 710000/1353222.0
..........53.21% 720000/1353222.0
..........53.95% 730000/1353222.0
..........54.68% 740000/1353222.0
..........55.42% 750000/1353222.0
..........56.16% 760000/1353222.0
..........56.90% 770000/1353222.0
..........57.64% 780000/1353222.0
..........58.38% 790000/1353222.0
..........59.12% 800000/1353222.0
..........59.86% 810000/1353222.0
..........60.60% 820000/1353222.0
..........61.34% 830000/1353222.0
..........62.07% 840000/1353222.0
..........62.81% 850000/1353222.0
..........63.55% 860000/1353222.0
..........64.29% 870000/1353222.0
..........65.03% 880000/1353222.0
..........65.77% 890000/1353222.0
..........66.51% 900000/1353222.0
..........67.25% 910000/1353222.0
..........67.99% 920000/1353222.0
..........68.72% 930000/1353222.0
..........69.46% 940000/1353222.0
..........70.20% 950000/1353222.0
..........70.94% 960000/1353222.0
..........71.68% 970000/1353222.0
..........72.42% 980000/1353222.0
..........73.16% 990000/1353222.0
..........73.90% 1000000/1353222.0
..........74.64% 1010000/1353222.0
..........75.38% 1020000/1353222.0
..........76.11% 1030000/1353222.0
..........76.85% 1040000/1353222.0
..........77.59% 1050000/1353222.0
..........78.33% 1060000/1353222.0
..........79.07% 1070000/1353222.0
..........79.81% 1080000/1353222.0
..........80.55% 1090000/1353222.0
..........81.29% 1100000/1353222.0
..........82.03% 1110000/1353222.0
..........82.77% 1120000/1353222.0
..........83.50% 1130000/1353222.0
..........84.24% 1140000/1353222.0
..........84.98% 1150000/1353222.0
..........85.72% 1160000/1353222.0
..........86.46% 1170000/1353222.0
..........87.20% 1180000/1353222.0
..........87.94% 1190000/1353222.0
..........88.68% 1200000/1353222.0
..........89.42% 1210000/1353222.0
..........90.16% 1220000/1353222.0
..........90.89% 1230000/1353222.0
..........91.63% 1240000/1353222.0
..........92.37% 1250000/1353222.0
..........93.11% 1260000/1353222.0
..........93.85% 1270000/1353222.0
..........94.59% 1280000/1353222.0
..........95.33% 1290000/1353222.0
..........96.07% 1300000/1353222.0
..........96.81% 1310000/1353222.0
..........97.54% 1320000/1353222.0
..........98.28% 1330000/1353222.0
..........99.02% 1340000/1353222.0
.........Hypergeometric contig-similarity network:
       3549 contigs,
     172718 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (172718 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3502, 3502)
features: (3502, 512)
y: (3502,) (3502,)
mask: (3502,) (3502,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3501, 3501, 3501],
                       [ 463,  450,  447,  ...,   52,   34,   32]]),
       values=tensor([0.1055, 0.0118, 0.0034,  ..., 0.1471, 0.1338, 0.0197]),
       device='cuda:0', size=(3502, 512), nnz=85510, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    3,  ..., 1146, 1147, 3501],
                       [   0,    0,    0,  ..., 3501, 3501, 3501]]),
       values=tensor([0.0067, 0.0076, 0.0064,  ..., 0.0913, 0.0894, 0.2000]),
       device='cuda:0', size=(3502, 3502), nnz=176286, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 85510
0 20.027755737304688 0.10999628390932739
10 16.248558044433594 0.5667038275733928
20 13.399608612060547 0.8591601635079896
30 11.055274963378906 0.8903753251579338
40 9.32986831665039 0.9063545150501672
50 7.99285888671875 0.9230769230769231
60 6.8954267501831055 0.9345968041620215
70 5.951699733734131 0.9438870308435526
80 5.128574848175049 0.9483463396506875
90 4.46765661239624 0.9591230026012635
100 3.872500419616699 0.9602378298030472
110 3.3607168197631836 0.9646971386101821
120 2.932910919189453 0.9684132292827945
130 2.5885837078094482 0.9654403567447045
140 2.3134543895721436 0.9687848383500557
150 2.044044017791748 0.9665551839464883
160 1.7962632179260254 0.9658119658119658
170 1.6180360317230225 0.9680416202155333
180 1.4634146690368652 0.9672984020810108
190 1.307855486869812 0.9617242660720922
200 1.22332763671875 0.9654403567447045
210 1.0550453662872314 0.9669267930137495
220 0.9457964897155762 0.9617242660720922
230 0.8749778270721436 0.9650687476774433
240 0.7914098501205444 0.9687848383500557
250 0.7298821210861206 0.966183574879227
260 0.6910983324050903 0.9669267930137495
270 0.6441600918769836 0.9654403567447045
280 0.5857314467430115 0.9654403567447045
290 0.5532029271125793 0.967670011148272
300 0.5339552164077759 0.9658119658119658
310 0.487476110458374 0.9632107023411371
320 0.4920799136161804 0.9684132292827945
330 0.4385554790496826 0.9717577108881457
340 0.4127255082130432 0.9684132292827945
350 0.4217786192893982 0.9650687476774433
360 0.4106570780277252 0.967670011148272
370 0.3666672110557556 0.9635823114083983
380 0.3921215534210205 0.9702712746191007
390 0.33190593123435974 0.9702712746191007
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.064s]
Loading sequences...  [0.435s]
Masking sequences...  [0.447s]
Writing sequences...  [0.064s]
Hashing sequences...  [0.024s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.056s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.053s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.027s]
Opening the output file...  [0.001s]
Loading query sequences...  [0.598s]
Masking queries...  [0.777s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.38s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.113s]
Masking reference...  [0.466s]
Initializing temporary storage...  [0.032s]
Building reference histograms...  [1.418s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.145s]
Building query seed array...  [0.188s]
Computing hash join...  [0.075s]
Building seed filter...  [0.008s]
Searching alignments...  [0.296s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.224s]
Computing hash join...  [0.074s]
Building seed filter...  [0.027s]
Searching alignments...  [0.297s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.159s]
Building query seed array...  [0.248s]
Computing hash join...  [0.072s]
Building seed filter...  [0.011s]
Searching alignments...  [0.276s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.16s]
Building query seed array...  [0.159s]
Computing hash join...  [0.064s]
Building seed filter...  [0.036s]
Searching alignments...  [0.356s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.203s]
Building query seed array...  [0.204s]
Computing hash join...  [0.14s]
Building seed filter...  [0.025s]
Searching alignments...  [0.382s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.22s]
Building query seed array...  [0.379s]
Computing hash join...  [0.065s]
Building seed filter...  [0.024s]
Searching alignments...  [0.319s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.258s]
Building query seed array...  [0.245s]
Computing hash join...  [0.159s]
Building seed filter...  [0.024s]
Searching alignments...  [0.315s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.217s]
Building query seed array...  [0.169s]
Computing hash join...  [0.059s]
Building seed filter...  [0.022s]
Searching alignments...  [0.252s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.127s]
Building query seed array...  [0.151s]
Computing hash join...  [0.083s]
Building seed filter...  [0.013s]
Searching alignments...  [0.263s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.163s]
Building query seed array...  [0.172s]
Computing hash join...  [0.067s]
Building seed filter...  [0.011s]
Searching alignments...  [0.303s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.17s]
Building query seed array...  [0.203s]
Computing hash join...  [0.065s]
Building seed filter...  [0.01s]
Searching alignments...  [0.277s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.173s]
Building query seed array...  [0.227s]
Computing hash join...  [0.058s]
Building seed filter...  [0.009s]
Searching alignments...  [0.26s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.112s]
Computing hash join...  [0.061s]
Building seed filter...  [0.009s]
Searching alignments...  [0.319s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.192s]
Building query seed array...  [0.205s]
Computing hash join...  [0.063s]
Building seed filter...  [0.009s]
Searching alignments...  [0.325s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.207s]
Building query seed array...  [0.227s]
Computing hash join...  [0.073s]
Building seed filter...  [0.009s]
Searching alignments...  [0.322s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.157s]
Building query seed array...  [0.168s]
Computing hash join...  [0.073s]
Building seed filter...  [0.009s]
Searching alignments...  [0.323s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.159s]
Building query seed array...  [0.171s]
Computing hash join...  [0.063s]
Building seed filter...  [0.008s]
Searching alignments...  [0.316s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.189s]
Building query seed array...  [0.191s]
Computing hash join...  [0.07s]
Building seed filter...  [0.009s]
Searching alignments...  [0.312s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.172s]
Building query seed array...  [0.141s]
Computing hash join...  [0.106s]
Building seed filter...  [0.024s]
Searching alignments...  [0.264s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.124s]
Computing hash join...  [0.07s]
Building seed filter...  [0.009s]
Searching alignments...  [0.25s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.157s]
Building query seed array...  [0.159s]
Computing hash join...  [0.054s]
Building seed filter...  [0.009s]
Searching alignments...  [0.285s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.147s]
Building query seed array...  [0.158s]
Computing hash join...  [0.054s]
Building seed filter...  [0.009s]
Searching alignments...  [0.265s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.147s]
Building query seed array...  [0.155s]
Computing hash join...  [0.08s]
Building seed filter...  [0.027s]
Searching alignments...  [0.326s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.163s]
Building query seed array...  [0.137s]
Computing hash join...  [0.072s]
Building seed filter...  [0.017s]
Searching alignments...  [0.251s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.154s]
Building query seed array...  [0.151s]
Computing hash join...  [0.071s]
Building seed filter...  [0.008s]
Searching alignments...  [0.27s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.164s]
Computing hash join...  [0.068s]
Building seed filter...  [0.012s]
Searching alignments...  [0.233s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.314s]
Computing hash join...  [0.102s]
Building seed filter...  [0.019s]
Searching alignments...  [0.394s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.176s]
Building query seed array...  [0.261s]
Computing hash join...  [0.11s]
Building seed filter...  [0.017s]
Searching alignments...  [0.268s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.137s]
Building query seed array...  [0.106s]
Computing hash join...  [0.055s]
Building seed filter...  [0.009s]
Searching alignments...  [0.241s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.129s]
Building query seed array...  [0.137s]
Computing hash join...  [0.063s]
Building seed filter...  [0.011s]
Searching alignments...  [0.24s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.137s]
Building query seed array...  [0.161s]
Computing hash join...  [0.058s]
Building seed filter...  [0.012s]
Searching alignments...  [0.239s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.115s]
Building query seed array...  [0.145s]
Computing hash join...  [0.065s]
Building seed filter...  [0.008s]
Searching alignments...  [0.237s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.127s]
Computing hash join...  [0.055s]
Building seed filter...  [0.02s]
Searching alignments...  [0.27s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.152s]
Building query seed array...  [0.163s]
Computing hash join...  [0.068s]
Building seed filter...  [0.014s]
Searching alignments...  [0.256s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.176s]
Building query seed array...  [0.194s]
Computing hash join...  [0.061s]
Building seed filter...  [0.01s]
Searching alignments...  [0.251s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.126s]
Building query seed array...  [0.121s]
Computing hash join...  [0.056s]
Building seed filter...  [0.01s]
Searching alignments...  [0.255s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.113s]
Building query seed array...  [0.121s]
Computing hash join...  [0.058s]
Building seed filter...  [0.011s]
Searching alignments...  [0.227s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.189s]
Building query seed array...  [0.157s]
Computing hash join...  [0.058s]
Building seed filter...  [0.009s]
Searching alignments...  [0.231s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.17s]
Building query seed array...  [0.13s]
Computing hash join...  [0.057s]
Building seed filter...  [0.009s]
Searching alignments...  [0.238s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.104s]
Building query seed array...  [0.119s]
Computing hash join...  [0.067s]
Building seed filter...  [0.012s]
Searching alignments...  [0.225s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.156s]
Computing hash join...  [0.06s]
Building seed filter...  [0.011s]
Searching alignments...  [0.283s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.146s]
Building query seed array...  [0.206s]
Computing hash join...  [0.059s]
Building seed filter...  [0.011s]
Searching alignments...  [0.245s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.16s]
Building query seed array...  [0.154s]
Computing hash join...  [0.058s]
Building seed filter...  [0.009s]
Searching alignments...  [0.257s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.106s]
Computing hash join...  [0.058s]
Building seed filter...  [0.014s]
Searching alignments...  [0.257s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.121s]
Building query seed array...  [0.132s]
Computing hash join...  [0.06s]
Building seed filter...  [0.016s]
Searching alignments...  [0.243s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.185s]
Building query seed array...  [0.122s]
Computing hash join...  [0.06s]
Building seed filter...  [0.01s]
Searching alignments...  [0.249s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.13s]
Computing hash join...  [0.059s]
Building seed filter...  [0.01s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.113s]
Computing hash join...  [0.054s]
Building seed filter...  [0.011s]
Searching alignments...  [0.254s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.126s]
Building query seed array...  [0.13s]
Computing hash join...  [0.068s]
Building seed filter...  [0.01s]
Searching alignments...  [0.23s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.143s]
Building query seed array...  [0.169s]
Computing hash join...  [0.06s]
Building seed filter...  [0.01s]
Searching alignments...  [0.231s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.206s]
Building query seed array...  [0.154s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.269s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.131s]
Building query seed array...  [0.127s]
Computing hash join...  [0.073s]
Building seed filter...  [0.009s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.108s]
Computing hash join...  [0.054s]
Building seed filter...  [0.01s]
Searching alignments...  [0.236s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.125s]
Computing hash join...  [0.1s]
Building seed filter...  [0.01s]
Searching alignments...  [0.27s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.199s]
Building query seed array...  [0.17s]
Computing hash join...  [0.072s]
Building seed filter...  [0.011s]
Searching alignments...  [0.239s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.124s]
Building query seed array...  [0.151s]
Computing hash join...  [0.071s]
Building seed filter...  [0.009s]
Searching alignments...  [0.243s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.156s]
Computing hash join...  [0.053s]
Building seed filter...  [0.011s]
Searching alignments...  [0.245s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.177s]
Building query seed array...  [0.168s]
Computing hash join...  [0.06s]
Building seed filter...  [0.01s]
Searching alignments...  [0.245s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.15s]
Computing hash join...  [0.061s]
Building seed filter...  [0.012s]
Searching alignments...  [0.272s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.141s]
Building query seed array...  [0.136s]
Computing hash join...  [0.057s]
Building seed filter...  [0.013s]
Searching alignments...  [0.251s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.104s]
Computing hash join...  [0.071s]
Building seed filter...  [0.01s]
Searching alignments...  [0.244s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.143s]
Building query seed array...  [0.194s]
Computing hash join...  [0.06s]
Building seed filter...  [0.009s]
Searching alignments...  [0.296s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.159s]
Computing hash join...  [0.07s]
Building seed filter...  [0.01s]
Searching alignments...  [0.228s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.141s]
Building query seed array...  [0.146s]
Computing hash join...  [0.054s]
Building seed filter...  [0.011s]
Searching alignments...  [0.238s]
Deallocating buffers...  [0s]
Clearing query masking...  [0.05s]
Computing alignments...  [9.187s]
Deallocating reference...  [0.001s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0s]
Deallocating queries...  [0.002s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.013s]
Closing the database file...  [0.001s]
Deallocating taxonomy...  [0s]
Total time = 57.999s
Reported 347137 pairwise alignments, 347137 HSPs.
25672 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
.........................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 286998x286998 matrix with 5875116 entries to stream <out/merged.mci>
[mclIO] wrote 286998 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 286998 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 286998x286998 matrix with 5875116 entries
[mcl] pid 5698
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  55.19  3.30 0.99/0.03/4.27 2.80 2.60 2.60   0
  2  ...................  70.74 15.78 0.86/0.09/4.58 3.98 0.84 2.19   5
  3  ...................  43.34  9.15 0.83/0.10/6.97 2.52 0.69 1.50   1
  4  ...................  24.19  3.39 0.82/0.13/10.78 1.56 0.72 1.07   0
  5  ...................  18.12  1.58 0.82/0.13/6.18 1.22 0.71 0.76   0
  6  ...................  10.41  0.83 0.82/0.12/4.96 1.09 0.72 0.55   0
  7  ...................   9.48  0.54 0.82/0.14/3.34 1.04 0.78 0.43   0
  8  ...................   6.33  0.41 0.83/0.18/1.97 1.01 0.81 0.35   0
  9  ...................   4.69  0.34 0.86/0.18/1.60 1.01 0.81 0.28   0
 10  ...................   4.61  0.28 0.89/0.19/2.27 1.00 0.81 0.23   0
 11  ...................   4.31  0.23 0.93/0.20/1.34 1.00 0.81 0.19   0
 12  ...................   4.62  0.20 0.95/0.22/1.18 1.00 0.83 0.15   0
 13  ...................   4.96  0.18 0.97/0.22/1.01 1.00 0.85 0.13   0
 14  ...................   4.24  0.17 0.98/0.22/1.00 1.00 0.90 0.12   0
 15  ...................   4.65  0.15 0.99/0.19/1.00 1.00 0.93 0.11   0
 16  ...................   3.57  0.14 0.99/0.28/1.00 1.00 0.95 0.10   0
 17  ...................   2.11  0.14 1.00/0.45/1.00 1.00 0.97 0.10   0
 18  ...................   1.72  0.13 1.00/0.33/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.13 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   2.20  0.13 1.00/0.28/1.00 1.00 1.00 0.10   0
 21  ...................   4.00  0.14 1.00/0.33/1.00 1.00 1.00 0.10   0
 22  ...................   0.70  0.13 1.00/0.70/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.14 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.15  0.13 1.00/0.85/1.00 1.00 1.00 0.10   0
 25  ...................   0.03  0.14 1.00/0.96/1.00 1.00 1.00 0.10   0
 26  ...................   0.00  0.13 1.00/1.00/1.00 1.00 1.00 0.10   0
 27  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 34237 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3513 entries from out/pcs_contigs.csv
Read 278796 entries (dropped 2277 singletons) from out/Cyber_profiles.csv
.......... 0.84% 10000/1196928.0
.......... 1.67% 20000/1196928.0
.......... 2.51% 30000/1196928.0
.......... 3.34% 40000/1196928.0
.......... 4.18% 50000/1196928.0
.......... 5.01% 60000/1196928.0
.......... 5.85% 70000/1196928.0
.......... 6.68% 80000/1196928.0
.......... 7.52% 90000/1196928.0
.......... 8.35% 100000/1196928.0
.......... 9.19% 110000/1196928.0
..........10.03% 120000/1196928.0
..........10.86% 130000/1196928.0
..........11.70% 140000/1196928.0
..........12.53% 150000/1196928.0
..........13.37% 160000/1196928.0
..........14.20% 170000/1196928.0
..........15.04% 180000/1196928.0
..........15.87% 190000/1196928.0
..........16.71% 200000/1196928.0
..........17.54% 210000/1196928.0
..........18.38% 220000/1196928.0
..........19.22% 230000/1196928.0
..........20.05% 240000/1196928.0
..........20.89% 250000/1196928.0
..........21.72% 260000/1196928.0
..........22.56% 270000/1196928.0
..........23.39% 280000/1196928.0
..........24.23% 290000/1196928.0
..........25.06% 300000/1196928.0
..........25.90% 310000/1196928.0
..........26.74% 320000/1196928.0
..........27.57% 330000/1196928.0
..........28.41% 340000/1196928.0
..........29.24% 350000/1196928.0
..........30.08% 360000/1196928.0
..........30.91% 370000/1196928.0
..........31.75% 380000/1196928.0
..........32.58% 390000/1196928.0
..........33.42% 400000/1196928.0
..........34.25% 410000/1196928.0
..........35.09% 420000/1196928.0
..........35.93% 430000/1196928.0
..........36.76% 440000/1196928.0
..........37.60% 450000/1196928.0
..........38.43% 460000/1196928.0
..........39.27% 470000/1196928.0
..........40.10% 480000/1196928.0
..........40.94% 490000/1196928.0
..........41.77% 500000/1196928.0
..........42.61% 510000/1196928.0
..........43.44% 520000/1196928.0
..........44.28% 530000/1196928.0
..........45.12% 540000/1196928.0
..........45.95% 550000/1196928.0
..........46.79% 560000/1196928.0
..........47.62% 570000/1196928.0
..........48.46% 580000/1196928.0
..........49.29% 590000/1196928.0
..........50.13% 600000/1196928.0
..........50.96% 610000/1196928.0
..........51.80% 620000/1196928.0
..........52.63% 630000/1196928.0
..........53.47% 640000/1196928.0
..........54.31% 650000/1196928.0
..........55.14% 660000/1196928.0
..........55.98% 670000/1196928.0
..........56.81% 680000/1196928.0
..........57.65% 690000/1196928.0
..........58.48% 700000/1196928.0
..........59.32% 710000/1196928.0
..........60.15% 720000/1196928.0
..........60.99% 730000/1196928.0
..........61.82% 740000/1196928.0
..........62.66% 750000/1196928.0
..........63.50% 760000/1196928.0
..........64.33% 770000/1196928.0
..........65.17% 780000/1196928.0
..........66.00% 790000/1196928.0
..........66.84% 800000/1196928.0
..........67.67% 810000/1196928.0
..........68.51% 820000/1196928.0
..........69.34% 830000/1196928.0
..........70.18% 840000/1196928.0
..........71.02% 850000/1196928.0
..........71.85% 860000/1196928.0
..........72.69% 870000/1196928.0
..........73.52% 880000/1196928.0
..........74.36% 890000/1196928.0
..........75.19% 900000/1196928.0
..........76.03% 910000/1196928.0
..........76.86% 920000/1196928.0
..........77.70% 930000/1196928.0
..........78.53% 940000/1196928.0
..........79.37% 950000/1196928.0
..........80.21% 960000/1196928.0
..........81.04% 970000/1196928.0
..........81.88% 980000/1196928.0
..........82.71% 990000/1196928.0
..........83.55% 1000000/1196928.0
..........84.38% 1010000/1196928.0
..........85.22% 1020000/1196928.0
..........86.05% 1030000/1196928.0
..........86.89% 1040000/1196928.0
..........87.72% 1050000/1196928.0
..........88.56% 1060000/1196928.0
..........89.40% 1070000/1196928.0
..........90.23% 1080000/1196928.0
..........91.07% 1090000/1196928.0
..........91.90% 1100000/1196928.0
..........92.74% 1110000/1196928.0
..........93.57% 1120000/1196928.0
..........94.41% 1130000/1196928.0
..........95.24% 1140000/1196928.0
..........96.08% 1150000/1196928.0
..........96.91% 1160000/1196928.0
..........97.75% 1170000/1196928.0
..........98.59% 1180000/1196928.0
..........99.42% 1190000/1196928.0
...Hypergeometric contig-similarity network:
       3513 contigs,
     157580 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (157580 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3441, 3441)
features: (3441, 512)
y: (3441,) (3441,)
mask: (3441,) (3441,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3440, 3440, 3440],
                       [ 466,  463,  450,  ...,   34,   32,    7]]),
       values=tensor([0.0006, 0.1008, 0.0066,  ..., 0.0972, 0.0526, 0.0009]),
       device='cuda:0', size=(3441, 512), nnz=83819, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    3,  ..., 1178, 1585, 3440],
                       [   0,    0,    0,  ..., 3440, 3440, 3440]]),
       values=tensor([0.0196, 0.0196, 0.0206,  ..., 0.1155, 0.1155, 0.2000]),
       device='cuda:0', size=(3441, 3441), nnz=161189, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 83819
0 19.989526748657227 0.1111111111111111
10 16.19649887084961 0.5711631363805277
20 13.421407699584961 0.8201412114455593
30 11.108468055725098 0.8695652173913043
40 9.412642478942871 0.9108138238573021
50 7.983043670654297 0.929022668153103
60 6.869946479797363 0.947603121516165
70 5.951255798339844 0.9572649572649573
80 5.113101482391357 0.9539204756596061
90 4.467889785766602 0.961352657004831
100 3.8526954650878906 0.9665551839464883
110 3.366483449935913 0.9591230026012635
120 2.9844305515289307 0.9717577108881457
130 2.6373701095581055 0.9702712746191007
140 2.2715399265289307 0.9672984020810108
150 2.046035051345825 0.9680416202155333
160 1.8397586345672607 0.9684132292827945
170 1.5796338319778442 0.9665551839464883
180 1.4621645212173462 0.9698996655518395
190 1.2771930694580078 0.969156447417317
200 1.1574983596801758 0.9654403567447045
210 1.0215271711349487 0.9609810479375697
220 0.949803352355957 0.9684132292827945
230 0.8653721213340759 0.9650687476774433
240 0.7895878553390503 0.9665551839464883
250 0.7487274408340454 0.9717577108881457
260 0.7208219766616821 0.9680416202155333
270 0.6173511743545532 0.9632107023411371
280 0.5743391513824463 0.9702712746191007
290 0.542778730392456 0.9654403567447045
300 0.5300793647766113 0.9620958751393534
310 0.47785764932632446 0.9695280564845782
320 0.47263580560684204 0.9717577108881457
330 0.4382919371128082 0.9721293199554069
340 0.41015130281448364 0.9646971386101821
350 0.3852710425853729 0.9680416202155333
360 0.37494128942489624 0.9695280564845782
370 0.34955334663391113 0.9620958751393534
380 0.39116090536117554 0.9713861018208845
390 0.37265998125076294 0.9702712746191007
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
error length < 2000bp
DTR_607443
error length < 2000bp
DTR_606728
error length < 2000bp
DTR_606722
error length < 2000bp
DTR_605559
error length < 2000bp
DTR_607456
error length < 2000bp
DTR_608697
error length < 2000bp
DTR_605565
error length < 2000bp
DTR_605571
error length < 2000bp
DTR_610561
error length < 2000bp
DTR_610553
error length < 2000bp
DTR_606733
error length < 2000bp
DTR_609536
error length < 2000bp
DTR_605573
error length < 2000bp
DTR_608693
error length < 2000bp
DTR_610602
error length < 2000bp
DTR_610556
error length < 2000bp
DTR_605572
error length < 2000bp
DTR_606755
error length < 2000bp
DTR_609562
error length < 2000bp
DTR_610609
error length < 2000bp
DTR_610555
error length < 2000bp
DTR_609568
error length < 2000bp
DTR_609511
error length < 2000bp
DTR_605606
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_644.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_645.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_646.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_647.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_648.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_649.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_697.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_698.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_699.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_700.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_726.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_727.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_774.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_775.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_800.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_801.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_802.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_803.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_847.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_848.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_849.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_850.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_851.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_852.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_644.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.046s]
Loading sequences...  [0.403s]
Masking sequences...  [0.478s]
Writing sequences...  [0.064s]
Hashing sequences...  [0.023s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.006s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.034s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.056s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.028s]
Opening the output file...  [0.001s]
Loading query sequences...  [0.606s]
Masking queries...  [0.786s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.325s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.117s]
Masking reference...  [0.454s]
Initializing temporary storage...  [0.026s]
Building reference histograms...  [1.285s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.16s]
Building query seed array...  [0.135s]
Computing hash join...  [0.069s]
Building seed filter...  [0.008s]
Searching alignments...  [0.26s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.204s]
Computing hash join...  [0.069s]
Building seed filter...  [0.009s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.161s]
Building query seed array...  [0.183s]
Computing hash join...  [0.063s]
Building seed filter...  [0.008s]
Searching alignments...  [0.264s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.108s]
Building query seed array...  [0.146s]
Computing hash join...  [0.073s]
Building seed filter...  [0.009s]
Searching alignments...  [0.266s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.121s]
Building query seed array...  [0.148s]
Computing hash join...  [0.066s]
Building seed filter...  [0.007s]
Searching alignments...  [0.207s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.128s]
Building query seed array...  [0.134s]
Computing hash join...  [0.068s]
Building seed filter...  [0.008s]
Searching alignments...  [0.227s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.176s]
Building query seed array...  [0.134s]
Computing hash join...  [0.072s]
Building seed filter...  [0.008s]
Searching alignments...  [0.206s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.168s]
Building query seed array...  [0.143s]
Computing hash join...  [0.065s]
Building seed filter...  [0.009s]
Searching alignments...  [0.219s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.143s]
Building query seed array...  [0.123s]
Computing hash join...  [0.056s]
Building seed filter...  [0.008s]
Searching alignments...  [0.25s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.139s]
Building query seed array...  [0.174s]
Computing hash join...  [0.063s]
Building seed filter...  [0.009s]
Searching alignments...  [0.247s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.174s]
Building query seed array...  [0.179s]
Computing hash join...  [0.06s]
Building seed filter...  [0.008s]
Searching alignments...  [0.26s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.107s]
Building query seed array...  [0.169s]
Computing hash join...  [0.067s]
Building seed filter...  [0.009s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.114s]
Computing hash join...  [0.079s]
Building seed filter...  [0.011s]
Searching alignments...  [0.253s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.136s]
Building query seed array...  [0.191s]
Computing hash join...  [0.067s]
Building seed filter...  [0.013s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.18s]
Computing hash join...  [0.06s]
Building seed filter...  [0.007s]
Searching alignments...  [0.24s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.126s]
Building query seed array...  [0.148s]
Computing hash join...  [0.073s]
Building seed filter...  [0.009s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.136s]
Computing hash join...  [0.056s]
Building seed filter...  [0.008s]
Searching alignments...  [0.239s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.137s]
Building query seed array...  [0.164s]
Computing hash join...  [0.06s]
Building seed filter...  [0.011s]
Searching alignments...  [0.243s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.183s]
Building query seed array...  [0.144s]
Computing hash join...  [0.071s]
Building seed filter...  [0.007s]
Searching alignments...  [0.219s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.106s]
Computing hash join...  [0.07s]
Building seed filter...  [0.008s]
Searching alignments...  [0.213s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.143s]
Building query seed array...  [0.121s]
Computing hash join...  [0.063s]
Building seed filter...  [0.011s]
Searching alignments...  [0.238s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.142s]
Computing hash join...  [0.072s]
Building seed filter...  [0.008s]
Searching alignments...  [0.234s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.171s]
Building query seed array...  [0.172s]
Computing hash join...  [0.071s]
Building seed filter...  [0.011s]
Searching alignments...  [0.231s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.158s]
Computing hash join...  [0.062s]
Building seed filter...  [0.01s]
Searching alignments...  [0.222s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.115s]
Building query seed array...  [0.118s]
Computing hash join...  [0.071s]
Building seed filter...  [0.008s]
Searching alignments...  [0.209s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.137s]
Building query seed array...  [0.135s]
Computing hash join...  [0.066s]
Building seed filter...  [0.008s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.158s]
Building query seed array...  [0.154s]
Computing hash join...  [0.065s]
Building seed filter...  [0.009s]
Searching alignments...  [0.203s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.145s]
Building query seed array...  [0.148s]
Computing hash join...  [0.053s]
Building seed filter...  [0.008s]
Searching alignments...  [0.218s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.126s]
Computing hash join...  [0.07s]
Building seed filter...  [0.008s]
Searching alignments...  [0.213s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.155s]
Building query seed array...  [0.157s]
Computing hash join...  [0.054s]
Building seed filter...  [0.011s]
Searching alignments...  [0.216s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.174s]
Building query seed array...  [0.166s]
Computing hash join...  [0.065s]
Building seed filter...  [0.008s]
Searching alignments...  [0.209s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.103s]
Building query seed array...  [0.107s]
Computing hash join...  [0.06s]
Building seed filter...  [0.008s]
Searching alignments...  [0.21s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.115s]
Building query seed array...  [0.162s]
Computing hash join...  [0.065s]
Building seed filter...  [0.009s]
Searching alignments...  [0.264s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.129s]
Building query seed array...  [0.16s]
Computing hash join...  [0.067s]
Building seed filter...  [0.011s]
Searching alignments...  [0.252s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.154s]
Building query seed array...  [0.157s]
Computing hash join...  [0.057s]
Building seed filter...  [0.008s]
Searching alignments...  [0.239s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.137s]
Computing hash join...  [0.068s]
Building seed filter...  [0.009s]
Searching alignments...  [0.266s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.116s]
Computing hash join...  [0.069s]
Building seed filter...  [0.01s]
Searching alignments...  [0.199s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.172s]
Building query seed array...  [0.172s]
Computing hash join...  [0.061s]
Building seed filter...  [0.009s]
Searching alignments...  [0.204s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.164s]
Computing hash join...  [0.068s]
Building seed filter...  [0.008s]
Searching alignments...  [0.199s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.097s]
Computing hash join...  [0.056s]
Building seed filter...  [0.011s]
Searching alignments...  [0.203s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.108s]
Computing hash join...  [0.054s]
Building seed filter...  [0.01s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.132s]
Building query seed array...  [0.207s]
Computing hash join...  [0.054s]
Building seed filter...  [0.01s]
Searching alignments...  [0.262s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.212s]
Computing hash join...  [0.071s]
Building seed filter...  [0.009s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.155s]
Computing hash join...  [0.06s]
Building seed filter...  [0.01s]
Searching alignments...  [0.224s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.124s]
Building query seed array...  [0.114s]
Computing hash join...  [0.057s]
Building seed filter...  [0.01s]
Searching alignments...  [0.214s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.189s]
Building query seed array...  [0.145s]
Computing hash join...  [0.054s]
Building seed filter...  [0.009s]
Searching alignments...  [0.218s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.144s]
Building query seed array...  [0.134s]
Computing hash join...  [0.072s]
Building seed filter...  [0.009s]
Searching alignments...  [0.198s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.125s]
Building query seed array...  [0.118s]
Computing hash join...  [0.056s]
Building seed filter...  [0.008s]
Searching alignments...  [0.2s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.11s]
Building query seed array...  [0.112s]
Computing hash join...  [0.061s]
Building seed filter...  [0.01s]
Searching alignments...  [0.206s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.132s]
Building query seed array...  [0.171s]
Computing hash join...  [0.054s]
Building seed filter...  [0.009s]
Searching alignments...  [0.205s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.169s]
Building query seed array...  [0.13s]
Computing hash join...  [0.056s]
Building seed filter...  [0.01s]
Searching alignments...  [0.22s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.114s]
Building query seed array...  [0.125s]
Computing hash join...  [0.071s]
Building seed filter...  [0.012s]
Searching alignments...  [0.2s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.111s]
Building query seed array...  [0.129s]
Computing hash join...  [0.071s]
Building seed filter...  [0.009s]
Searching alignments...  [0.21s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.185s]
Building query seed array...  [0.184s]
Computing hash join...  [0.063s]
Building seed filter...  [0.009s]
Searching alignments...  [0.205s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.183s]
Building query seed array...  [0.164s]
Computing hash join...  [0.07s]
Building seed filter...  [0.009s]
Searching alignments...  [0.207s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.147s]
Computing hash join...  [0.062s]
Building seed filter...  [0.009s]
Searching alignments...  [0.215s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.105s]
Building query seed array...  [0.151s]
Computing hash join...  [0.067s]
Building seed filter...  [0.012s]
Searching alignments...  [0.212s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.219s]
Building query seed array...  [0.19s]
Computing hash join...  [0.062s]
Building seed filter...  [0.011s]
Searching alignments...  [0.213s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.145s]
Building query seed array...  [0.157s]
Computing hash join...  [0.066s]
Building seed filter...  [0.01s]
Searching alignments...  [0.219s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.155s]
Building query seed array...  [0.108s]
Computing hash join...  [0.057s]
Building seed filter...  [0.009s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.111s]
Computing hash join...  [0.054s]
Building seed filter...  [0.01s]
Searching alignments...  [0.208s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.161s]
Building query seed array...  [0.136s]
Computing hash join...  [0.071s]
Building seed filter...  [0.01s]
Searching alignments...  [0.201s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.147s]
Building query seed array...  [0.13s]
Computing hash join...  [0.056s]
Building seed filter...  [0.009s]
Searching alignments...  [0.211s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.135s]
Building query seed array...  [0.139s]
Computing hash join...  [0.071s]
Building seed filter...  [0.01s]
Searching alignments...  [0.207s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.052s]
Computing alignments...  [9.108s]
Deallocating reference...  [0.001s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0.001s]
Deallocating queries...  [0.002s]
Loading query sequences...  [0s]
Closing the input file...  [0s]
Closing the output file...  [0.009s]
Closing the database file...  [0s]
Deallocating taxonomy...  [0s]
Total time = 52.313s
Reported 321238 pairwise alignments, 321238 HSPs.
23709 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
........................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 284892x284892 matrix with 5823318 entries to stream <out/merged.mci>
[mclIO] wrote 284892 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 284892 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 284892x284892 matrix with 5823318 entries
[mcl] pid 23443
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  49.44  3.20 0.99/0.03/4.49 2.72 2.52 2.52   0
  2  ...................  68.48 14.89 0.86/0.08/4.58 3.96 0.85 2.15   4
  3  ...................  46.20  8.82 0.83/0.09/8.95 2.50 0.70 1.51   1
  4  ...................  24.43  3.41 0.82/0.14/10.77 1.56 0.71 1.08   0
  5  ...................  17.39  1.61 0.82/0.09/7.00 1.22 0.70 0.76   0
  6  ...................  12.48  0.89 0.82/0.11/3.47 1.09 0.73 0.55   0
  7  ...................  12.70  0.56 0.82/0.14/2.43 1.03 0.78 0.43   0
  8  ...................   6.52  0.41 0.83/0.15/3.12 1.01 0.81 0.35   0
  9  ...................   4.69  0.33 0.86/0.18/1.26 1.01 0.81 0.28   0
 10  ...................   4.69  0.27 0.89/0.24/1.29 1.00 0.81 0.23   0
 11  ...................   4.31  0.23 0.93/0.23/1.11 1.00 0.81 0.19   0
 12  ...................   4.81  0.20 0.95/0.22/1.18 1.00 0.83 0.15   0
 13  ...................   4.96  0.17 0.97/0.27/1.01 1.00 0.86 0.13   0
 14  ...................   3.72  0.15 0.98/0.21/1.00 1.00 0.90 0.12   0
 15  ...................   4.65  0.14 0.99/0.31/1.00 1.00 0.93 0.11   0
 16  ...................   3.05  0.15 0.99/0.31/1.00 1.00 0.96 0.11   0
 17  ...................   3.61  0.14 1.00/0.36/1.00 1.00 0.97 0.10   0
 18  ...................   1.73  0.13 1.00/0.33/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.14 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.23  0.14 1.00/0.44/1.00 1.00 1.00 0.10   0
 21  ...................   2.00  0.14 1.00/0.38/1.00 1.00 1.00 0.10   0
 22  ...................   0.66  0.14 1.00/0.69/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.13 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.15  0.14 1.00/0.85/1.00 1.00 1.00 0.10   0
 25  ...................   0.03  0.15 1.00/0.96/1.00 1.00 1.00 0.10   0
 26  ...................   0.00  0.13 1.00/1.00/1.00 1.00 1.00 0.10   0
 27  ...................   0.00  0.13 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 34088 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3594 entries from out/pcs_contigs.csv
Read 277106 entries (dropped 2282 singletons) from out/Cyber_profiles.csv
.......... 0.86% 10000/1161353.0
.......... 1.72% 20000/1161353.0
.......... 2.58% 30000/1161353.0
.......... 3.44% 40000/1161353.0
.......... 4.31% 50000/1161353.0
.......... 5.17% 60000/1161353.0
.......... 6.03% 70000/1161353.0
.......... 6.89% 80000/1161353.0
.......... 7.75% 90000/1161353.0
.......... 8.61% 100000/1161353.0
.......... 9.47% 110000/1161353.0
..........10.33% 120000/1161353.0
..........11.19% 130000/1161353.0
..........12.05% 140000/1161353.0
..........12.92% 150000/1161353.0
..........13.78% 160000/1161353.0
..........14.64% 170000/1161353.0
..........15.50% 180000/1161353.0
..........16.36% 190000/1161353.0
..........17.22% 200000/1161353.0
..........18.08% 210000/1161353.0
..........18.94% 220000/1161353.0
..........19.80% 230000/1161353.0
..........20.67% 240000/1161353.0
..........21.53% 250000/1161353.0
..........22.39% 260000/1161353.0
..........23.25% 270000/1161353.0
..........24.11% 280000/1161353.0
..........24.97% 290000/1161353.0
..........25.83% 300000/1161353.0
..........26.69% 310000/1161353.0
..........27.55% 320000/1161353.0
..........28.42% 330000/1161353.0
..........29.28% 340000/1161353.0
..........30.14% 350000/1161353.0
..........31.00% 360000/1161353.0
..........31.86% 370000/1161353.0
..........32.72% 380000/1161353.0
..........33.58% 390000/1161353.0
..........34.44% 400000/1161353.0
..........35.30% 410000/1161353.0
..........36.16% 420000/1161353.0
..........37.03% 430000/1161353.0
..........37.89% 440000/1161353.0
..........38.75% 450000/1161353.0
..........39.61% 460000/1161353.0
..........40.47% 470000/1161353.0
..........41.33% 480000/1161353.0
..........42.19% 490000/1161353.0
..........43.05% 500000/1161353.0
..........43.91% 510000/1161353.0
..........44.78% 520000/1161353.0
..........45.64% 530000/1161353.0
..........46.50% 540000/1161353.0
..........47.36% 550000/1161353.0
..........48.22% 560000/1161353.0
..........49.08% 570000/1161353.0
..........49.94% 580000/1161353.0
..........50.80% 590000/1161353.0
..........51.66% 600000/1161353.0
..........52.52% 610000/1161353.0
..........53.39% 620000/1161353.0
..........54.25% 630000/1161353.0
..........55.11% 640000/1161353.0
..........55.97% 650000/1161353.0
..........56.83% 660000/1161353.0
..........57.69% 670000/1161353.0
..........58.55% 680000/1161353.0
..........59.41% 690000/1161353.0
..........60.27% 700000/1161353.0
..........61.14% 710000/1161353.0
..........62.00% 720000/1161353.0
..........62.86% 730000/1161353.0
..........63.72% 740000/1161353.0
..........64.58% 750000/1161353.0
..........65.44% 760000/1161353.0
..........66.30% 770000/1161353.0
..........67.16% 780000/1161353.0
..........68.02% 790000/1161353.0
..........68.89% 800000/1161353.0
..........69.75% 810000/1161353.0
..........70.61% 820000/1161353.0
..........71.47% 830000/1161353.0
..........72.33% 840000/1161353.0
..........73.19% 850000/1161353.0
..........74.05% 860000/1161353.0
..........74.91% 870000/1161353.0
..........75.77% 880000/1161353.0
..........76.63% 890000/1161353.0
..........77.50% 900000/1161353.0
..........78.36% 910000/1161353.0
..........79.22% 920000/1161353.0
..........80.08% 930000/1161353.0
..........80.94% 940000/1161353.0
..........81.80% 950000/1161353.0
..........82.66% 960000/1161353.0
..........83.52% 970000/1161353.0
..........84.38% 980000/1161353.0
..........85.25% 990000/1161353.0
..........86.11% 1000000/1161353.0
..........86.97% 1010000/1161353.0
..........87.83% 1020000/1161353.0
..........88.69% 1030000/1161353.0
..........89.55% 1040000/1161353.0
..........90.41% 1050000/1161353.0
..........91.27% 1060000/1161353.0
..........92.13% 1070000/1161353.0
..........92.99% 1080000/1161353.0
..........93.86% 1090000/1161353.0
..........94.72% 1100000/1161353.0
..........95.58% 1110000/1161353.0
..........96.44% 1120000/1161353.0
..........97.30% 1130000/1161353.0
..........98.16% 1140000/1161353.0
..........99.02% 1150000/1161353.0
.......Hypergeometric contig-similarity network:
       3594 contigs,
     146012 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (146012 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3532, 3532)
features: (3532, 512)
y: (3532,) (3532,)
mask: (3532,) (3532,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3531, 3531, 3531],
                       [ 466,  463,  447,  ...,   52,   34,   32]]),
       values=tensor([0.0126, 0.0527, 0.0976,  ..., 0.0526, 0.1533, 0.0272]),
       device='cuda:0', size=(3532, 512), nnz=85593, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,   13,  ..., 3530, 2569, 3531],
                       [   0,    0,    0,  ..., 3530, 3531, 3531]]),
       values=tensor([0.0476, 0.0445, 0.0374,  ..., 0.2000, 0.1104, 0.5000]),
       device='cuda:0', size=(3532, 3532), nnz=149614, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 85593
0 20.024593353271484 0.1408398364920104
10 16.245712280273438 0.5730211817168339
20 13.411008834838867 0.850613154960981
30 11.110040664672852 0.8710516536603493
40 9.381549835205078 0.9037532515793385
50 7.977716445922852 0.9457450761798588
60 6.898007392883301 0.939799331103679
70 5.945474624633789 0.9431438127090301
80 5.165398597717285 0.9528056484578223
90 4.459655284881592 0.958379784466741
100 3.8767244815826416 0.961352657004831
110 3.3795673847198486 0.9620958751393534
120 3.0021045207977295 0.961352657004831
130 2.57045841217041 0.9680416202155333
140 2.3188812732696533 0.9680416202155333
150 2.0351014137268066 0.9639539204756596
160 1.8339449167251587 0.9650687476774433
170 1.6411409378051758 0.9665551839464883
180 1.4097857475280762 0.9643255295429208
190 1.3018710613250732 0.9736157562244518
200 1.1769907474517822 0.9710144927536232
210 1.052152156829834 0.9687848383500557
220 0.9469045400619507 0.9680416202155333
230 0.8654398918151855 0.9713861018208845
240 0.8027268648147583 0.9665551839464883
250 0.7416765689849854 0.967670011148272
260 0.6885644197463989 0.9672984020810108
270 0.6451520919799805 0.969156447417317
280 0.609234631061554 0.9628390932738758
290 0.5691642165184021 0.970642883686362
300 0.5517289638519287 0.9654403567447045
310 0.5084733963012695 0.9721293199554069
320 0.4796072840690613 0.9725009290226682
330 0.4417552351951599 0.967670011148272
340 0.4328392744064331 0.9710144927536232
350 0.4312938451766968 0.9728725380899294
360 0.43622735142707825 0.9717577108881457
370 0.376529723405838 0.9702712746191007
380 0.3897053599357605 0.9721293199554069
390 0.3678584694862366 0.9669267930137495
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
error length < 2000bp
DTR_639325
error length < 2000bp
DTR_639504
error length < 2000bp
DTR_639832
error length < 2000bp
DTR_640304
error length < 2000bp
DTR_639501
error length < 2000bp
DTR_639505
error length < 2000bp
DTR_639457
error length < 2000bp
DTR_639327
error length < 2000bp
DTR_639502
error length < 2000bp
DTR_639450
error length < 2000bp
DTR_639833
error length < 2000bp
DTR_639620
error length < 2000bp
DTR_639627
error length < 2000bp
DTR_639823
error length < 2000bp
DTR_639503
error length < 2000bp
DTR_639829
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_198.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_199.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_217.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_219.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_228.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_229.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_230.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_231.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_232.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_269.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_273.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_351.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_352.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_353.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_354.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_542.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_198.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.051s]
Loading sequences...  [0.417s]
Masking sequences...  [0.45s]
Writing sequences...  [0.064s]
Hashing sequences...  [0.022s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.024s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.055s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.027s]
Opening the output file...  [0.001s]
Loading query sequences...  [0.528s]
Masking queries...  [0.675s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.253s]
Allocating buffers...  [0s]
Loading reference sequences...  [0.127s]
Masking reference...  [0.468s]
Initializing temporary storage...  [0.027s]
Building reference histograms...  [1.232s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.14s]
Building query seed array...  [0.166s]
Computing hash join...  [0.057s]
Building seed filter...  [0.008s]
Searching alignments...  [0.252s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.196s]
Computing hash join...  [0.063s]
Building seed filter...  [0.009s]
Searching alignments...  [0.238s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.179s]
Computing hash join...  [0.065s]
Building seed filter...  [0.008s]
Searching alignments...  [0.236s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.111s]
Building query seed array...  [0.143s]
Computing hash join...  [0.065s]
Building seed filter...  [0.009s]
Searching alignments...  [0.24s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.129s]
Building query seed array...  [0.127s]
Computing hash join...  [0.072s]
Building seed filter...  [0.008s]
Searching alignments...  [0.214s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.133s]
Building query seed array...  [0.129s]
Computing hash join...  [0.069s]
Building seed filter...  [0.008s]
Searching alignments...  [0.208s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.145s]
Building query seed array...  [0.185s]
Computing hash join...  [0.07s]
Building seed filter...  [0.008s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.123s]
Building query seed array...  [0.103s]
Computing hash join...  [0.059s]
Building seed filter...  [0.009s]
Searching alignments...  [0.222s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.124s]
Building query seed array...  [0.138s]
Computing hash join...  [0.071s]
Building seed filter...  [0.009s]
Searching alignments...  [0.234s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.178s]
Building query seed array...  [0.174s]
Computing hash join...  [0.068s]
Building seed filter...  [0.011s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.178s]
Building query seed array...  [0.154s]
Computing hash join...  [0.054s]
Building seed filter...  [0.01s]
Searching alignments...  [0.231s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.113s]
Building query seed array...  [0.137s]
Computing hash join...  [0.069s]
Building seed filter...  [0.013s]
Searching alignments...  [0.248s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.106s]
Building query seed array...  [0.124s]
Computing hash join...  [0.072s]
Building seed filter...  [0.009s]
Searching alignments...  [0.233s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.153s]
Building query seed array...  [0.148s]
Computing hash join...  [0.072s]
Building seed filter...  [0.009s]
Searching alignments...  [0.222s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.182s]
Building query seed array...  [0.158s]
Computing hash join...  [0.068s]
Building seed filter...  [0.008s]
Searching alignments...  [0.252s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.114s]
Building query seed array...  [0.114s]
Computing hash join...  [0.066s]
Building seed filter...  [0.01s]
Searching alignments...  [0.22s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.119s]
Computing hash join...  [0.062s]
Building seed filter...  [0.008s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.122s]
Computing hash join...  [0.07s]
Building seed filter...  [0.01s]
Searching alignments...  [0.215s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.139s]
Building query seed array...  [0.131s]
Computing hash join...  [0.061s]
Building seed filter...  [0.008s]
Searching alignments...  [0.221s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.102s]
Building query seed array...  [0.142s]
Computing hash join...  [0.057s]
Building seed filter...  [0.01s]
Searching alignments...  [0.215s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.147s]
Computing hash join...  [0.055s]
Building seed filter...  [0.009s]
Searching alignments...  [0.225s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.17s]
Building query seed array...  [0.141s]
Computing hash join...  [0.056s]
Building seed filter...  [0.009s]
Searching alignments...  [0.227s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.159s]
Building query seed array...  [0.138s]
Computing hash join...  [0.061s]
Building seed filter...  [0.009s]
Searching alignments...  [0.238s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.119s]
Building query seed array...  [0.143s]
Computing hash join...  [0.058s]
Building seed filter...  [0.009s]
Searching alignments...  [0.224s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.102s]
Building query seed array...  [0.102s]
Computing hash join...  [0.069s]
Building seed filter...  [0.008s]
Searching alignments...  [0.243s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.165s]
Building query seed array...  [0.148s]
Computing hash join...  [0.07s]
Building seed filter...  [0.009s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.175s]
Building query seed array...  [0.169s]
Computing hash join...  [0.07s]
Building seed filter...  [0.01s]
Searching alignments...  [0.233s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.103s]
Computing hash join...  [0.061s]
Building seed filter...  [0.009s]
Searching alignments...  [0.213s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.111s]
Building query seed array...  [0.124s]
Computing hash join...  [0.072s]
Building seed filter...  [0.012s]
Searching alignments...  [0.215s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.184s]
Building query seed array...  [0.174s]
Computing hash join...  [0.068s]
Building seed filter...  [0.012s]
Searching alignments...  [0.21s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.172s]
Building query seed array...  [0.153s]
Computing hash join...  [0.072s]
Building seed filter...  [0.012s]
Searching alignments...  [0.212s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.16s]
Building query seed array...  [0.142s]
Computing hash join...  [0.063s]
Building seed filter...  [0.012s]
Searching alignments...  [0.212s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.146s]
Building query seed array...  [0.135s]
Computing hash join...  [0.063s]
Building seed filter...  [0.009s]
Searching alignments...  [0.228s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.161s]
Computing hash join...  [0.065s]
Building seed filter...  [0.01s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.172s]
Building query seed array...  [0.139s]
Computing hash join...  [0.058s]
Building seed filter...  [0.011s]
Searching alignments...  [0.246s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.13s]
Building query seed array...  [0.126s]
Computing hash join...  [0.067s]
Building seed filter...  [0.009s]
Searching alignments...  [0.227s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.171s]
Building query seed array...  [0.129s]
Computing hash join...  [0.074s]
Building seed filter...  [0.009s]
Searching alignments...  [0.208s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.142s]
Building query seed array...  [0.134s]
Computing hash join...  [0.053s]
Building seed filter...  [0.009s]
Searching alignments...  [0.205s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.219s]
Building query seed array...  [0.144s]
Computing hash join...  [0.059s]
Building seed filter...  [0.011s]
Searching alignments...  [0.211s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.118s]
Computing hash join...  [0.054s]
Building seed filter...  [0.011s]
Searching alignments...  [0.213s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.161s]
Building query seed array...  [0.102s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.166s]
Building query seed array...  [0.132s]
Computing hash join...  [0.067s]
Building seed filter...  [0.008s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.144s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.244s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.102s]
Building query seed array...  [0.121s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.22s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.134s]
Computing hash join...  [0.064s]
Building seed filter...  [0.01s]
Searching alignments...  [0.212s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.136s]
Building query seed array...  [0.158s]
Computing hash join...  [0.146s]
Building seed filter...  [0.016s]
Searching alignments...  [0.225s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.254s]
Building query seed array...  [0.177s]
Computing hash join...  [0.115s]
Building seed filter...  [0.013s]
Searching alignments...  [0.203s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.154s]
Building query seed array...  [0.182s]
Computing hash join...  [0.114s]
Building seed filter...  [0.009s]
Searching alignments...  [0.199s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.152s]
Building query seed array...  [0.129s]
Computing hash join...  [0.056s]
Building seed filter...  [0.011s]
Searching alignments...  [0.207s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.188s]
Building query seed array...  [0.116s]
Computing hash join...  [0.053s]
Building seed filter...  [0.011s]
Searching alignments...  [0.2s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.156s]
Building query seed array...  [0.179s]
Computing hash join...  [0.058s]
Building seed filter...  [0.009s]
Searching alignments...  [0.199s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.108s]
Building query seed array...  [0.093s]
Computing hash join...  [0.095s]
Building seed filter...  [0.009s]
Searching alignments...  [0.202s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.105s]
Building query seed array...  [0.103s]
Computing hash join...  [0.06s]
Building seed filter...  [0.011s]
Searching alignments...  [0.207s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.165s]
Computing hash join...  [0.071s]
Building seed filter...  [0.012s]
Searching alignments...  [0.224s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.183s]
Building query seed array...  [0.182s]
Computing hash join...  [0.057s]
Building seed filter...  [0.015s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.123s]
Building query seed array...  [0.096s]
Computing hash join...  [0.058s]
Building seed filter...  [0.01s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.097s]
Computing hash join...  [0.072s]
Building seed filter...  [0.01s]
Searching alignments...  [0.216s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.123s]
Computing hash join...  [0.054s]
Building seed filter...  [0.01s]
Searching alignments...  [0.211s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.146s]
Building query seed array...  [0.14s]
Computing hash join...  [0.06s]
Building seed filter...  [0.011s]
Searching alignments...  [0.208s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.097s]
Computing hash join...  [0.063s]
Building seed filter...  [0.01s]
Searching alignments...  [0.221s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.108s]
Building query seed array...  [0.1s]
Computing hash join...  [0.056s]
Building seed filter...  [0.011s]
Searching alignments...  [0.221s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.139s]
Building query seed array...  [0.154s]
Computing hash join...  [0.061s]
Building seed filter...  [0.019s]
Searching alignments...  [0.202s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.173s]
Building query seed array...  [0.141s]
Computing hash join...  [0.07s]
Building seed filter...  [0.013s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.101s]
Building query seed array...  [0.092s]
Computing hash join...  [0.069s]
Building seed filter...  [0.015s]
Searching alignments...  [0.204s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.043s]
Computing alignments...  [7.842s]
Deallocating reference...  [0s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0s]
Deallocating queries...  [0.003s]
Loading query sequences...  [0s]
Closing the input file...  [0s]
Closing the output file...  [0.014s]
Closing the database file...  [0s]
Deallocating taxonomy...  [0s]
Total time = 50.218s
Reported 303853 pairwise alignments, 303853 HSPs.
22517 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
.......................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 283912x283912 matrix with 5788548 entries to stream <out/merged.mci>
[mclIO] wrote 283912 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 283912 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 283912x283912 matrix with 5788548 entries
[mcl] pid 21626
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  54.77  3.14 0.98/0.04/3.68 2.70 2.51 2.51   0
  2  ...................  91.41 14.52 0.87/0.10/4.57 3.98 0.85 2.14   4
  3  ...................  50.25  8.59 0.83/0.11/5.70 2.50 0.70 1.50   1
  4  ...................  28.88  3.39 0.82/0.12/8.90 1.56 0.72 1.08   0
  5  ...................  17.66  1.56 0.82/0.10/5.87 1.22 0.71 0.76   0
  6  ...................  12.04  0.87 0.82/0.12/7.41 1.09 0.73 0.55   0
  7  ...................   9.84  0.59 0.82/0.14/2.45 1.03 0.78 0.43   0
  8  ...................   6.45  0.41 0.83/0.19/1.81 1.01 0.81 0.35   0
  9  ...................   5.04  0.35 0.86/0.20/1.59 1.01 0.81 0.28   0
 10  ...................   4.93  0.29 0.89/0.22/1.95 1.00 0.81 0.23   0
 11  ...................   4.31  0.24 0.93/0.21/1.13 1.00 0.82 0.19   0
 12  ...................   4.49  0.20 0.95/0.22/1.18 1.00 0.83 0.16   0
 13  ...................   4.92  0.19 0.97/0.23/1.01 1.00 0.86 0.13   0
 14  ...................   4.51  0.17 0.98/0.19/1.00 1.00 0.90 0.12   0
 15  ...................   4.66  0.15 0.99/0.24/1.00 1.00 0.92 0.11   0
 16  ...................   3.62  0.14 0.99/0.28/1.00 1.00 0.96 0.11   0
 17  ...................   2.92  0.15 1.00/0.27/1.00 1.00 0.97 0.10   0
 18  ...................   4.74  0.14 1.00/0.29/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.14 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.06  0.14 1.00/0.46/1.00 1.00 1.00 0.10   0
 21  ...................   1.10  0.13 1.00/0.58/1.00 1.00 1.00 0.10   0
 22  ...................   0.45  0.15 1.00/0.67/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.14 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.17  0.14 1.00/0.83/1.00 1.00 1.00 0.10   0
 25  ...................   0.03  0.13 1.00/0.96/1.00 1.00 1.00 0.10   0
 26  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
 27  ...................   0.00  0.13 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 34316 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3460 entries from out/pcs_contigs.csv
Read 276006 entries (dropped 2260 singletons) from out/Cyber_profiles.csv
.......... 0.92% 10000/1086089.0
.......... 1.84% 20000/1086089.0
.......... 2.76% 30000/1086089.0
.......... 3.68% 40000/1086089.0
.......... 4.60% 50000/1086089.0
.......... 5.52% 60000/1086089.0
.......... 6.45% 70000/1086089.0
.......... 7.37% 80000/1086089.0
.......... 8.29% 90000/1086089.0
.......... 9.21% 100000/1086089.0
..........10.13% 110000/1086089.0
..........11.05% 120000/1086089.0
..........11.97% 130000/1086089.0
..........12.89% 140000/1086089.0
..........13.81% 150000/1086089.0
..........14.73% 160000/1086089.0
..........15.65% 170000/1086089.0
..........16.57% 180000/1086089.0
..........17.49% 190000/1086089.0
..........18.41% 200000/1086089.0
..........19.34% 210000/1086089.0
..........20.26% 220000/1086089.0
..........21.18% 230000/1086089.0
..........22.10% 240000/1086089.0
..........23.02% 250000/1086089.0
..........23.94% 260000/1086089.0
..........24.86% 270000/1086089.0
..........25.78% 280000/1086089.0
..........26.70% 290000/1086089.0
..........27.62% 300000/1086089.0
..........28.54% 310000/1086089.0
..........29.46% 320000/1086089.0
..........30.38% 330000/1086089.0
..........31.30% 340000/1086089.0
..........32.23% 350000/1086089.0
..........33.15% 360000/1086089.0
..........34.07% 370000/1086089.0
..........34.99% 380000/1086089.0
..........35.91% 390000/1086089.0
..........36.83% 400000/1086089.0
..........37.75% 410000/1086089.0
..........38.67% 420000/1086089.0
..........39.59% 430000/1086089.0
..........40.51% 440000/1086089.0
..........41.43% 450000/1086089.0
..........42.35% 460000/1086089.0
..........43.27% 470000/1086089.0
..........44.20% 480000/1086089.0
..........45.12% 490000/1086089.0
..........46.04% 500000/1086089.0
..........46.96% 510000/1086089.0
..........47.88% 520000/1086089.0
..........48.80% 530000/1086089.0
..........49.72% 540000/1086089.0
..........50.64% 550000/1086089.0
..........51.56% 560000/1086089.0
..........52.48% 570000/1086089.0
..........53.40% 580000/1086089.0
..........54.32% 590000/1086089.0
..........55.24% 600000/1086089.0
..........56.16% 610000/1086089.0
..........57.09% 620000/1086089.0
..........58.01% 630000/1086089.0
..........58.93% 640000/1086089.0
..........59.85% 650000/1086089.0
..........60.77% 660000/1086089.0
..........61.69% 670000/1086089.0
..........62.61% 680000/1086089.0
..........63.53% 690000/1086089.0
..........64.45% 700000/1086089.0
..........65.37% 710000/1086089.0
..........66.29% 720000/1086089.0
..........67.21% 730000/1086089.0
..........68.13% 740000/1086089.0
..........69.06% 750000/1086089.0
..........69.98% 760000/1086089.0
..........70.90% 770000/1086089.0
..........71.82% 780000/1086089.0
..........72.74% 790000/1086089.0
..........73.66% 800000/1086089.0
..........74.58% 810000/1086089.0
..........75.50% 820000/1086089.0
..........76.42% 830000/1086089.0
..........77.34% 840000/1086089.0
..........78.26% 850000/1086089.0
..........79.18% 860000/1086089.0
..........80.10% 870000/1086089.0
..........81.02% 880000/1086089.0
..........81.95% 890000/1086089.0
..........82.87% 900000/1086089.0
..........83.79% 910000/1086089.0
..........84.71% 920000/1086089.0
..........85.63% 930000/1086089.0
..........86.55% 940000/1086089.0
..........87.47% 950000/1086089.0
..........88.39% 960000/1086089.0
..........89.31% 970000/1086089.0
..........90.23% 980000/1086089.0
..........91.15% 990000/1086089.0
..........92.07% 1000000/1086089.0
..........92.99% 1010000/1086089.0
..........93.91% 1020000/1086089.0
..........94.84% 1030000/1086089.0
..........95.76% 1040000/1086089.0
..........96.68% 1050000/1086089.0
..........97.60% 1060000/1086089.0
..........98.52% 1070000/1086089.0
..........99.44% 1080000/1086089.0
..Hypergeometric contig-similarity network:
       3460 contigs,
     140162 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (140162 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3360, 3360)
features: (3360, 512)
y: (3360,) (3360,)
mask: (3360,) (3360,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3359, 3359, 3359],
                       [ 466,  463,  447,  ...,   52,   34,   32]]),
       values=tensor([0.0037, 0.0696, 0.1019,  ..., 0.0359, 0.1489, 0.0361]),
       device='cuda:0', size=(3360, 512), nnz=81109, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    2,  ..., 1748, 1952, 3359],
                       [   0,    0,    0,  ..., 3359, 3359, 3359]]),
       values=tensor([0.0125, 0.0125, 0.0125,  ..., 0.0698, 0.0716, 0.2000]),
       device='cuda:0', size=(3360, 3360), nnz=143884, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 81109
0 20.01933479309082 0.1236998514115899
10 16.218503952026367 0.6326151560178306
20 13.375086784362793 0.8231797919762258
30 11.092503547668457 0.8844725111441307
40 9.349322319030762 0.8974739970282318
50 8.03571605682373 0.924219910846954
60 6.848156452178955 0.9394502228826151
70 5.933917045593262 0.9424219910846954
80 5.176692008972168 0.9450222882615156
90 4.524190902709961 0.9535661218424963
100 3.8957953453063965 0.9598811292719168
110 3.4183948040008545 0.9565378900445766
120 2.9432315826416016 0.9602526002971769
130 2.6182398796081543 0.9583952451708767
140 2.3080599308013916 0.9598811292719168
150 2.0569002628326416 0.9609955423476969
160 1.7673041820526123 0.9609955423476969
170 1.585047721862793 0.961738484398217
180 1.4452546834945679 0.9643387815750372
190 1.2939592599868774 0.9635958395245171
200 1.1452912092208862 0.9632243684992571
210 1.048507571220398 0.9647102526002972
220 1.0311157703399658 0.9661961367013373
230 0.8867237567901611 0.9673105497771174
240 0.8098211288452148 0.9658246656760773
250 0.7306455373764038 0.9676820208023774
260 0.7073036432266235 0.9650817236255572
270 0.6351442933082581 0.9684249628528975
280 0.5969016551971436 0.9654531946508172
290 0.554829478263855 0.9661961367013373
300 0.520649790763855 0.9661961367013373
310 0.5180992484092712 0.9602526002971769
320 0.4652102589607239 0.9632243684992571
330 0.4295252859592438 0.9647102526002972
340 0.46253639459609985 0.9658246656760773
350 0.39977917075157166 0.9713967310549777
360 0.4123210906982422 0.962852897473997
370 0.4036152958869934 0.9654531946508172
380 0.3552747666835785 0.9650817236255572
390 0.3678985834121704 0.9680534918276374
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.046s]
Loading sequences...  [0.444s]
Masking sequences...  [0.448s]
Writing sequences...  [0.065s]
Hashing sequences...  [0.024s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.048s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.055s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.028s]
Opening the output file...  [0.001s]
Loading query sequences...  [0.472s]
Masking queries...  [0.643s]
Building query seed set...  [0.002s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.157s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.116s]
Masking reference...  [0.471s]
Initializing temporary storage...  [0.03s]
Building reference histograms...  [1.155s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.132s]
Building query seed array...  [0.186s]
Computing hash join...  [0.062s]
Building seed filter...  [0.009s]
Searching alignments...  [0.215s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.207s]
Building query seed array...  [0.135s]
Computing hash join...  [0.061s]
Building seed filter...  [0.008s]
Searching alignments...  [0.235s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.209s]
Building query seed array...  [0.168s]
Computing hash join...  [0.063s]
Building seed filter...  [0.011s]
Searching alignments...  [0.255s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.107s]
Building query seed array...  [0.149s]
Computing hash join...  [0.063s]
Building seed filter...  [0.012s]
Searching alignments...  [0.236s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.183s]
Building query seed array...  [0.146s]
Computing hash join...  [0.102s]
Building seed filter...  [0.015s]
Searching alignments...  [0.247s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.204s]
Building query seed array...  [0.158s]
Computing hash join...  [0.065s]
Building seed filter...  [0.013s]
Searching alignments...  [0.185s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.204s]
Building query seed array...  [0.121s]
Computing hash join...  [0.065s]
Building seed filter...  [0.012s]
Searching alignments...  [0.189s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.182s]
Building query seed array...  [0.125s]
Computing hash join...  [0.074s]
Building seed filter...  [0.009s]
Searching alignments...  [0.188s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.109s]
Building query seed array...  [0.136s]
Computing hash join...  [0.073s]
Building seed filter...  [0.012s]
Searching alignments...  [0.206s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.173s]
Computing hash join...  [0.056s]
Building seed filter...  [0.008s]
Searching alignments...  [0.259s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.213s]
Building query seed array...  [0.152s]
Computing hash join...  [0.11s]
Building seed filter...  [0.008s]
Searching alignments...  [0.241s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.098s]
Computing hash join...  [0.064s]
Building seed filter...  [0.016s]
Searching alignments...  [0.2s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.104s]
Building query seed array...  [0.104s]
Computing hash join...  [0.067s]
Building seed filter...  [0.013s]
Searching alignments...  [0.234s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.139s]
Building query seed array...  [0.128s]
Computing hash join...  [0.078s]
Building seed filter...  [0.012s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.216s]
Computing hash join...  [0.081s]
Building seed filter...  [0.009s]
Searching alignments...  [0.198s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.161s]
Building query seed array...  [0.157s]
Computing hash join...  [0.065s]
Building seed filter...  [0.018s]
Searching alignments...  [0.209s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.167s]
Building query seed array...  [0.136s]
Computing hash join...  [0.068s]
Building seed filter...  [0.01s]
Searching alignments...  [0.235s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.199s]
Building query seed array...  [0.175s]
Computing hash join...  [0.058s]
Building seed filter...  [0.021s]
Searching alignments...  [0.247s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.2s]
Building query seed array...  [0.16s]
Computing hash join...  [0.091s]
Building seed filter...  [0.021s]
Searching alignments...  [0.212s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.157s]
Computing hash join...  [0.106s]
Building seed filter...  [0.015s]
Searching alignments...  [0.27s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.16s]
Building query seed array...  [0.108s]
Computing hash join...  [0.112s]
Building seed filter...  [0.022s]
Searching alignments...  [0.283s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.189s]
Building query seed array...  [0.222s]
Computing hash join...  [0.091s]
Building seed filter...  [0.019s]
Searching alignments...  [0.261s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.202s]
Building query seed array...  [0.238s]
Computing hash join...  [0.096s]
Building seed filter...  [0.014s]
Searching alignments...  [0.257s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.265s]
Building query seed array...  [0.142s]
Computing hash join...  [0.096s]
Building seed filter...  [0.009s]
Searching alignments...  [0.209s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.189s]
Building query seed array...  [0.159s]
Computing hash join...  [0.124s]
Building seed filter...  [0.021s]
Searching alignments...  [0.253s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.213s]
Building query seed array...  [0.167s]
Computing hash join...  [0.085s]
Building seed filter...  [0.014s]
Searching alignments...  [0.253s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.204s]
Building query seed array...  [0.219s]
Computing hash join...  [0.069s]
Building seed filter...  [0.02s]
Searching alignments...  [0.182s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.179s]
Building query seed array...  [0.204s]
Computing hash join...  [0.137s]
Building seed filter...  [0.019s]
Searching alignments...  [0.241s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.152s]
Building query seed array...  [0.092s]
Computing hash join...  [0.106s]
Building seed filter...  [0.012s]
Searching alignments...  [0.254s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.209s]
Building query seed array...  [0.197s]
Computing hash join...  [0.056s]
Building seed filter...  [0.01s]
Searching alignments...  [0.191s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.229s]
Building query seed array...  [0.189s]
Computing hash join...  [0.084s]
Building seed filter...  [0.013s]
Searching alignments...  [0.193s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.131s]
Building query seed array...  [0.114s]
Computing hash join...  [0.064s]
Building seed filter...  [0.011s]
Searching alignments...  [0.185s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.162s]
Computing hash join...  [0.064s]
Building seed filter...  [0.009s]
Searching alignments...  [0.214s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.136s]
Building query seed array...  [0.125s]
Computing hash join...  [0.074s]
Building seed filter...  [0.012s]
Searching alignments...  [0.219s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.213s]
Building query seed array...  [0.15s]
Computing hash join...  [0.075s]
Building seed filter...  [0.018s]
Searching alignments...  [0.203s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.154s]
Building query seed array...  [0.148s]
Computing hash join...  [0.093s]
Building seed filter...  [0.012s]
Searching alignments...  [0.199s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.177s]
Building query seed array...  [0.107s]
Computing hash join...  [0.089s]
Building seed filter...  [0.008s]
Searching alignments...  [0.23s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.216s]
Building query seed array...  [0.162s]
Computing hash join...  [0.058s]
Building seed filter...  [0.011s]
Searching alignments...  [0.195s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.197s]
Building query seed array...  [0.116s]
Computing hash join...  [0.063s]
Building seed filter...  [0.011s]
Searching alignments...  [0.192s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.12s]
Building query seed array...  [0.142s]
Computing hash join...  [0.09s]
Building seed filter...  [0.01s]
Searching alignments...  [0.193s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.103s]
Computing hash join...  [0.09s]
Building seed filter...  [0.017s]
Searching alignments...  [0.255s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.21s]
Building query seed array...  [0.117s]
Computing hash join...  [0.063s]
Building seed filter...  [0.017s]
Searching alignments...  [0.199s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.217s]
Building query seed array...  [0.126s]
Computing hash join...  [0.062s]
Building seed filter...  [0.014s]
Searching alignments...  [0.222s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.162s]
Building query seed array...  [0.149s]
Computing hash join...  [0.094s]
Building seed filter...  [0.011s]
Searching alignments...  [0.27s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.114s]
Building query seed array...  [0.134s]
Computing hash join...  [0.065s]
Building seed filter...  [0.009s]
Searching alignments...  [0.178s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.127s]
Building query seed array...  [0.17s]
Computing hash join...  [0.074s]
Building seed filter...  [0.01s]
Searching alignments...  [0.188s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.192s]
Building query seed array...  [0.187s]
Computing hash join...  [0.086s]
Building seed filter...  [0.011s]
Searching alignments...  [0.235s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.17s]
Building query seed array...  [0.086s]
Computing hash join...  [0.069s]
Building seed filter...  [0.009s]
Searching alignments...  [0.205s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.176s]
Building query seed array...  [0.112s]
Computing hash join...  [0.065s]
Building seed filter...  [0.013s]
Searching alignments...  [0.203s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.163s]
Building query seed array...  [0.149s]
Computing hash join...  [0.06s]
Building seed filter...  [0.012s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.17s]
Building query seed array...  [0.116s]
Computing hash join...  [0.071s]
Building seed filter...  [0.01s]
Searching alignments...  [0.245s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.157s]
Building query seed array...  [0.122s]
Computing hash join...  [0.056s]
Building seed filter...  [0.01s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.197s]
Building query seed array...  [0.127s]
Computing hash join...  [0.085s]
Building seed filter...  [0.011s]
Searching alignments...  [0.239s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.21s]
Building query seed array...  [0.112s]
Computing hash join...  [0.097s]
Building seed filter...  [0.012s]
Searching alignments...  [0.21s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.214s]
Building query seed array...  [0.119s]
Computing hash join...  [0.108s]
Building seed filter...  [0.014s]
Searching alignments...  [0.227s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.11s]
Building query seed array...  [0.101s]
Computing hash join...  [0.066s]
Building seed filter...  [0.028s]
Searching alignments...  [0.2s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.159s]
Building query seed array...  [0.111s]
Computing hash join...  [0.077s]
Building seed filter...  [0.016s]
Searching alignments...  [0.191s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.13s]
Building query seed array...  [0.196s]
Computing hash join...  [0.067s]
Building seed filter...  [0.009s]
Searching alignments...  [0.208s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.215s]
Building query seed array...  [0.122s]
Computing hash join...  [0.062s]
Building seed filter...  [0.013s]
Searching alignments...  [0.199s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.172s]
Building query seed array...  [0.148s]
Computing hash join...  [0.087s]
Building seed filter...  [0.012s]
Searching alignments...  [0.189s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.118s]
Building query seed array...  [0.087s]
Computing hash join...  [0.089s]
Building seed filter...  [0.01s]
Searching alignments...  [0.179s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.197s]
Building query seed array...  [0.229s]
Computing hash join...  [0.081s]
Building seed filter...  [0.014s]
Searching alignments...  [0.216s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.145s]
Building query seed array...  [0.169s]
Computing hash join...  [0.069s]
Building seed filter...  [0.013s]
Searching alignments...  [0.195s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.168s]
Building query seed array...  [0.144s]
Computing hash join...  [0.067s]
Building seed filter...  [0.01s]
Searching alignments...  [0.241s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.042s]
Computing alignments...  [7.106s]
Deallocating reference...  [0.001s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0.001s]
Deallocating queries...  [0.001s]
Loading query sequences...  [0.002s]
Closing the input file...  [0s]
Closing the output file...  [0.009s]
Closing the database file...  [0.001s]
Deallocating taxonomy...  [0s]
Total time = 52.464s
Reported 253776 pairwise alignments, 253776 HSPs.
19258 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
.....................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 280510x280510 matrix with 5688394 entries to stream <out/merged.mci>
[mclIO] wrote 280510 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 280510 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 280510x280510 matrix with 5688394 entries
[mcl] pid 14396
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  54.93  3.00 0.98/0.04/3.90 2.63 2.45 2.45   0
  2  ...................  56.70 13.96 0.87/0.10/4.58 3.95 0.86 2.11   4
  3  ...................  46.61  8.24 0.83/0.10/9.44 2.48 0.71 1.50   1
  4  ...................  24.54  3.26 0.83/0.15/13.34 1.55 0.72 1.09   0
  5  ...................  18.07  1.59 0.82/0.09/7.72 1.22 0.71 0.77   0
  6  ...................  11.81  0.84 0.82/0.10/4.19 1.09 0.73 0.56   0
  7  ...................  10.79  0.54 0.82/0.14/3.59 1.03 0.78 0.43   0
  8  ...................   6.29  0.41 0.83/0.14/1.81 1.01 0.81 0.35   0
  9  ...................   4.82  0.32 0.86/0.20/1.59 1.00 0.81 0.29   0
 10  ...................   5.51  0.27 0.89/0.23/1.16 1.00 0.81 0.23   0
 11  ...................   4.31  0.23 0.93/0.23/1.25 1.00 0.82 0.19   0
 12  ...................   4.54  0.19 0.95/0.19/1.18 1.00 0.83 0.16   0
 13  ...................   4.96  0.17 0.97/0.22/1.01 1.00 0.86 0.13   0
 14  ...................   4.06  0.15 0.98/0.19/1.00 1.00 0.90 0.12   0
 15  ...................   4.62  0.15 0.99/0.30/1.00 1.00 0.92 0.11   0
 16  ...................   4.41  0.14 0.99/0.28/1.00 1.00 0.96 0.11   0
 17  ...................   2.81  0.14 1.00/0.26/1.00 1.00 0.96 0.10   0
 18  ...................   4.74  0.14 1.00/0.29/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.15 1.00/0.30/1.00 1.00 0.99 0.10   0
 20  ...................   2.19  0.13 1.00/0.46/1.00 1.00 0.99 0.10   0
 21  ...................   1.11  0.14 1.00/0.57/1.00 1.00 1.00 0.10   0
 22  ...................   0.43  0.14 1.00/0.69/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.14 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.23  0.13 1.00/0.77/1.00 1.00 1.00 0.10   0
 25  ...................   0.09  0.14 1.00/0.91/1.00 1.00 1.00 0.10   0
 26  ...................   0.06  0.13 1.00/0.97/1.00 1.00 1.00 0.10   0
 27  ...................   0.10  0.14 1.00/0.92/1.00 1.00 1.00 0.10   0
 28  ...................   0.12  0.14 1.00/0.86/1.00 1.00 1.00 0.10   0
 29  ...................   0.09  0.14 1.00/0.89/1.00 1.00 1.00 0.10   0
 30  ...................   0.02  0.14 1.00/0.98/1.00 1.00 1.00 0.10   0
 31  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
 32  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 34124 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3515 entries from out/pcs_contigs.csv
Read 272811 entries (dropped 2279 singletons) from out/Cyber_profiles.csv
.......... 0.94% 10000/1066196.0
.......... 1.88% 20000/1066196.0
.......... 2.81% 30000/1066196.0
.......... 3.75% 40000/1066196.0
.......... 4.69% 50000/1066196.0
.......... 5.63% 60000/1066196.0
.......... 6.57% 70000/1066196.0
.......... 7.50% 80000/1066196.0
.......... 8.44% 90000/1066196.0
.......... 9.38% 100000/1066196.0
..........10.32% 110000/1066196.0
..........11.25% 120000/1066196.0
..........12.19% 130000/1066196.0
..........13.13% 140000/1066196.0
..........14.07% 150000/1066196.0
..........15.01% 160000/1066196.0
..........15.94% 170000/1066196.0
..........16.88% 180000/1066196.0
..........17.82% 190000/1066196.0
..........18.76% 200000/1066196.0
..........19.70% 210000/1066196.0
..........20.63% 220000/1066196.0
..........21.57% 230000/1066196.0
..........22.51% 240000/1066196.0
..........23.45% 250000/1066196.0
..........24.39% 260000/1066196.0
..........25.32% 270000/1066196.0
..........26.26% 280000/1066196.0
..........27.20% 290000/1066196.0
..........28.14% 300000/1066196.0
..........29.08% 310000/1066196.0
..........30.01% 320000/1066196.0
..........30.95% 330000/1066196.0
..........31.89% 340000/1066196.0
..........32.83% 350000/1066196.0
..........33.76% 360000/1066196.0
..........34.70% 370000/1066196.0
..........35.64% 380000/1066196.0
..........36.58% 390000/1066196.0
..........37.52% 400000/1066196.0
..........38.45% 410000/1066196.0
..........39.39% 420000/1066196.0
..........40.33% 430000/1066196.0
..........41.27% 440000/1066196.0
..........42.21% 450000/1066196.0
..........43.14% 460000/1066196.0
..........44.08% 470000/1066196.0
..........45.02% 480000/1066196.0
..........45.96% 490000/1066196.0
..........46.90% 500000/1066196.0
..........47.83% 510000/1066196.0
..........48.77% 520000/1066196.0
..........49.71% 530000/1066196.0
..........50.65% 540000/1066196.0
..........51.59% 550000/1066196.0
..........52.52% 560000/1066196.0
..........53.46% 570000/1066196.0
..........54.40% 580000/1066196.0
..........55.34% 590000/1066196.0
..........56.27% 600000/1066196.0
..........57.21% 610000/1066196.0
..........58.15% 620000/1066196.0
..........59.09% 630000/1066196.0
..........60.03% 640000/1066196.0
..........60.96% 650000/1066196.0
..........61.90% 660000/1066196.0
..........62.84% 670000/1066196.0
..........63.78% 680000/1066196.0
..........64.72% 690000/1066196.0
..........65.65% 700000/1066196.0
..........66.59% 710000/1066196.0
..........67.53% 720000/1066196.0
..........68.47% 730000/1066196.0
..........69.41% 740000/1066196.0
..........70.34% 750000/1066196.0
..........71.28% 760000/1066196.0
..........72.22% 770000/1066196.0
..........73.16% 780000/1066196.0
..........74.10% 790000/1066196.0
..........75.03% 800000/1066196.0
..........75.97% 810000/1066196.0
..........76.91% 820000/1066196.0
..........77.85% 830000/1066196.0
..........78.78% 840000/1066196.0
..........79.72% 850000/1066196.0
..........80.66% 860000/1066196.0
..........81.60% 870000/1066196.0
..........82.54% 880000/1066196.0
..........83.47% 890000/1066196.0
..........84.41% 900000/1066196.0
..........85.35% 910000/1066196.0
..........86.29% 920000/1066196.0
..........87.23% 930000/1066196.0
..........88.16% 940000/1066196.0
..........89.10% 950000/1066196.0
..........90.04% 960000/1066196.0
..........90.98% 970000/1066196.0
..........91.92% 980000/1066196.0
..........92.85% 990000/1066196.0
..........93.79% 1000000/1066196.0
..........94.73% 1010000/1066196.0
..........95.67% 1020000/1066196.0
..........96.61% 1030000/1066196.0
..........97.54% 1040000/1066196.0
..........98.48% 1050000/1066196.0
..........99.42% 1060000/1066196.0
..Hypergeometric contig-similarity network:
       3515 contigs,
     133854 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (133854 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3403, 3403)
features: (3403, 512)
y: (3403,) (3403,)
mask: (3403,) (3403,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3402, 3402, 3402],
                       [ 463,  450,  436,  ...,   52,   34,   32]]),
       values=tensor([0.0602, 0.0217, 0.0336,  ..., 0.0082, 0.1138, 0.0004]),
       device='cuda:0', size=(3403, 512), nnz=81859, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    3,  ...,  264, 2279, 3402],
                       [   0,    0,    0,  ..., 3402, 3402, 3402]]),
       values=tensor([0.0270, 0.0245, 0.0254,  ..., 0.1240, 0.1118, 0.2000]),
       device='cuda:0', size=(3403, 3403), nnz=137817, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 81859
0 20.00299644470215 0.11924219910846955
10 16.23699951171875 0.587295690936107
20 13.394574165344238 0.8469539375928677
30 11.122665405273438 0.8933878157503715
40 9.345544815063477 0.9086181277860327
50 7.98292875289917 0.9279346210995543
60 6.930983543395996 0.9431649331352154
70 5.956063747406006 0.9491084695393759
80 5.159208297729492 0.9535661218424963
90 4.439015865325928 0.9517087667161961
100 3.879314661026001 0.9572808320950966
110 3.421604633331299 0.962852897473997
120 2.9962806701660156 0.9650817236255572
130 2.6105008125305176 0.9639673105497771
140 2.3231041431427 0.962481426448737
150 2.051922082901001 0.9658246656760773
160 1.790544033050537 0.9658246656760773
170 1.611286997795105 0.9669390787518574
180 1.4571633338928223 0.9676820208023774
190 1.3270398378372192 0.9673105497771174
200 1.201411247253418 0.9669390787518574
210 1.048994541168213 0.9654531946508172
220 0.942581057548523 0.9647102526002972
230 0.8621206283569336 0.9717682020802377
240 0.7949720621109009 0.9717682020802377
250 0.7242426872253418 0.9732540861812778
260 0.7107162475585938 0.9713967310549777
270 0.6108760237693787 0.9684249628528975
280 0.5948160290718079 0.961738484398217
290 0.5506966710090637 0.9602526002971769
300 0.5856093168258667 0.9602526002971769
310 0.4931214451789856 0.961738484398217
320 0.45766979455947876 0.9669390787518574
330 0.4186539053916931 0.9706537890044576
340 0.4282029867172241 0.9732540861812778
350 0.35518205165863037 0.9684249628528975
360 0.3611125349998474 0.9739970282317979
370 0.3716263771057129 0.9717682020802377
380 0.36710309982299805 0.9728826151560178
390 0.33732759952545166 0.9695393759286776
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.056s]
Loading sequences...  [0.412s]
Masking sequences...  [0.45s]
Writing sequences...  [0.063s]
Hashing sequences...  [0.024s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.024s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.054s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.021s]
Opening the output file...  [0.001s]
Loading query sequences...  [0.292s]
Masking queries...  [0.392s]
Building query seed set...  [0.002s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [0.701s]
Allocating buffers...  [0s]
Loading reference sequences...  [0.118s]
Masking reference...  [0.446s]
Initializing temporary storage...  [0.033s]
Building reference histograms...  [1.461s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.189s]
Building query seed array...  [0.195s]
Computing hash join...  [0.08s]
Building seed filter...  [0.009s]
Searching alignments...  [0.158s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.083s]
Computing hash join...  [0.145s]
Building seed filter...  [0.016s]
Searching alignments...  [0.167s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.248s]
Building query seed array...  [0.125s]
Computing hash join...  [0.076s]
Building seed filter...  [0.009s]
Searching alignments...  [0.26s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.135s]
Building query seed array...  [0.111s]
Computing hash join...  [0.113s]
Building seed filter...  [0.011s]
Searching alignments...  [0.209s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.172s]
Building query seed array...  [0.105s]
Computing hash join...  [0.078s]
Building seed filter...  [0.015s]
Searching alignments...  [0.203s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.331s]
Building query seed array...  [0.07s]
Computing hash join...  [0.134s]
Building seed filter...  [0.014s]
Searching alignments...  [0.155s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.218s]
Building query seed array...  [0.18s]
Computing hash join...  [0.168s]
Building seed filter...  [0.008s]
Searching alignments...  [0.156s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.256s]
Building query seed array...  [0.116s]
Computing hash join...  [0.123s]
Building seed filter...  [0.008s]
Searching alignments...  [0.152s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.152s]
Building query seed array...  [0.125s]
Computing hash join...  [0.06s]
Building seed filter...  [0.02s]
Searching alignments...  [0.232s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.286s]
Building query seed array...  [0.239s]
Computing hash join...  [0.05s]
Building seed filter...  [0.016s]
Searching alignments...  [0.211s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.219s]
Building query seed array...  [0.094s]
Computing hash join...  [0.077s]
Building seed filter...  [0.007s]
Searching alignments...  [0.151s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.181s]
Building query seed array...  [0.096s]
Computing hash join...  [0.098s]
Building seed filter...  [0.007s]
Searching alignments...  [0.216s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.097s]
Computing hash join...  [0.113s]
Building seed filter...  [0.011s]
Searching alignments...  [0.161s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.271s]
Building query seed array...  [0.089s]
Computing hash join...  [0.148s]
Building seed filter...  [0.016s]
Searching alignments...  [0.18s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.221s]
Building query seed array...  [0.195s]
Computing hash join...  [0.055s]
Building seed filter...  [0.011s]
Searching alignments...  [0.237s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.171s]
Building query seed array...  [0.061s]
Computing hash join...  [0.053s]
Building seed filter...  [0.008s]
Searching alignments...  [0.166s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.114s]
Building query seed array...  [0.059s]
Computing hash join...  [0.058s]
Building seed filter...  [0.012s]
Searching alignments...  [0.14s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.113s]
Computing hash join...  [0.084s]
Building seed filter...  [0.015s]
Searching alignments...  [0.2s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.216s]
Building query seed array...  [0.191s]
Computing hash join...  [0.103s]
Building seed filter...  [0.008s]
Searching alignments...  [0.233s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.228s]
Building query seed array...  [0.143s]
Computing hash join...  [0.067s]
Building seed filter...  [0.015s]
Searching alignments...  [0.235s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.18s]
Building query seed array...  [0.097s]
Computing hash join...  [0.122s]
Building seed filter...  [0.016s]
Searching alignments...  [0.205s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.202s]
Building query seed array...  [0.076s]
Computing hash join...  [0.178s]
Building seed filter...  [0.01s]
Searching alignments...  [0.344s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.245s]
Building query seed array...  [0.119s]
Computing hash join...  [0.051s]
Building seed filter...  [0.011s]
Searching alignments...  [0.188s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.29s]
Building query seed array...  [0.12s]
Computing hash join...  [0.085s]
Building seed filter...  [0.014s]
Searching alignments...  [0.23s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.155s]
Building query seed array...  [0.092s]
Computing hash join...  [0.133s]
Building seed filter...  [0.01s]
Searching alignments...  [0.302s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.209s]
Building query seed array...  [0.183s]
Computing hash join...  [0.105s]
Building seed filter...  [0.015s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.164s]
Building query seed array...  [0.098s]
Computing hash join...  [0.14s]
Building seed filter...  [0.011s]
Searching alignments...  [0.217s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.391s]
Building query seed array...  [0.134s]
Computing hash join...  [0.067s]
Building seed filter...  [0.011s]
Searching alignments...  [0.273s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.17s]
Building query seed array...  [0.17s]
Computing hash join...  [0.056s]
Building seed filter...  [0.016s]
Searching alignments...  [0.236s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.225s]
Building query seed array...  [0.14s]
Computing hash join...  [0.063s]
Building seed filter...  [0.015s]
Searching alignments...  [0.211s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.216s]
Building query seed array...  [0.27s]
Computing hash join...  [0.086s]
Building seed filter...  [0.015s]
Searching alignments...  [0.236s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.157s]
Building query seed array...  [0.144s]
Computing hash join...  [0.096s]
Building seed filter...  [0.012s]
Searching alignments...  [0.249s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.269s]
Building query seed array...  [0.214s]
Computing hash join...  [0.175s]
Building seed filter...  [0.015s]
Searching alignments...  [0.145s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.202s]
Building query seed array...  [0.159s]
Computing hash join...  [0.169s]
Building seed filter...  [0.013s]
Searching alignments...  [0.17s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.245s]
Building query seed array...  [0.134s]
Computing hash join...  [0.114s]
Building seed filter...  [0.012s]
Searching alignments...  [0.261s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.126s]
Building query seed array...  [0.228s]
Computing hash join...  [0.054s]
Building seed filter...  [0.011s]
Searching alignments...  [0.145s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.186s]
Building query seed array...  [0.115s]
Computing hash join...  [0.138s]
Building seed filter...  [0.008s]
Searching alignments...  [0.161s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.226s]
Building query seed array...  [0.232s]
Computing hash join...  [0.049s]
Building seed filter...  [0.015s]
Searching alignments...  [0.169s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.277s]
Building query seed array...  [0.26s]
Computing hash join...  [0.065s]
Building seed filter...  [0.011s]
Searching alignments...  [0.207s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.254s]
Building query seed array...  [0.2s]
Computing hash join...  [0.125s]
Building seed filter...  [0.012s]
Searching alignments...  [0.204s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.182s]
Building query seed array...  [0.112s]
Computing hash join...  [0.056s]
Building seed filter...  [0.017s]
Searching alignments...  [0.146s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.177s]
Building query seed array...  [0.101s]
Computing hash join...  [0.111s]
Building seed filter...  [0.011s]
Searching alignments...  [0.238s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.28s]
Building query seed array...  [0.273s]
Computing hash join...  [0.057s]
Building seed filter...  [0.013s]
Searching alignments...  [0.218s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.174s]
Building query seed array...  [0.086s]
Computing hash join...  [0.065s]
Building seed filter...  [0.01s]
Searching alignments...  [0.164s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.056s]
Computing hash join...  [0.046s]
Building seed filter...  [0.008s]
Searching alignments...  [0.181s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.214s]
Building query seed array...  [0.102s]
Computing hash join...  [0.087s]
Building seed filter...  [0.009s]
Searching alignments...  [0.13s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.239s]
Building query seed array...  [0.131s]
Computing hash join...  [0.075s]
Building seed filter...  [0.009s]
Searching alignments...  [0.159s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.106s]
Building query seed array...  [0.082s]
Computing hash join...  [0.054s]
Building seed filter...  [0.009s]
Searching alignments...  [0.172s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.177s]
Building query seed array...  [0.089s]
Computing hash join...  [0.055s]
Building seed filter...  [0.013s]
Searching alignments...  [0.153s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.27s]
Building query seed array...  [0.102s]
Computing hash join...  [0.072s]
Building seed filter...  [0.011s]
Searching alignments...  [0.18s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.222s]
Building query seed array...  [0.082s]
Computing hash join...  [0.075s]
Building seed filter...  [0.008s]
Searching alignments...  [0.135s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.1s]
Computing hash join...  [0.08s]
Building seed filter...  [0.015s]
Searching alignments...  [0.189s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.158s]
Building query seed array...  [0.093s]
Computing hash join...  [0.135s]
Building seed filter...  [0.012s]
Searching alignments...  [0.236s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.259s]
Building query seed array...  [0.092s]
Computing hash join...  [0.055s]
Building seed filter...  [0.016s]
Searching alignments...  [0.297s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.234s]
Building query seed array...  [0.158s]
Computing hash join...  [0.046s]
Building seed filter...  [0.009s]
Searching alignments...  [0.155s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.104s]
Building query seed array...  [0.185s]
Computing hash join...  [0.09s]
Building seed filter...  [0.017s]
Searching alignments...  [0.228s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.168s]
Building query seed array...  [0.137s]
Computing hash join...  [0.1s]
Building seed filter...  [0.015s]
Searching alignments...  [0.207s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.333s]
Building query seed array...  [0.155s]
Computing hash join...  [0.175s]
Building seed filter...  [0.017s]
Searching alignments...  [0.18s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.221s]
Building query seed array...  [0.086s]
Computing hash join...  [0.069s]
Building seed filter...  [0.018s]
Searching alignments...  [0.17s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.125s]
Computing hash join...  [0.093s]
Building seed filter...  [0.02s]
Searching alignments...  [0.14s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.162s]
Building query seed array...  [0.097s]
Computing hash join...  [0.069s]
Building seed filter...  [0.016s]
Searching alignments...  [0.167s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.07s]
Computing hash join...  [0.118s]
Building seed filter...  [0.016s]
Searching alignments...  [0.296s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.307s]
Building query seed array...  [0.278s]
Computing hash join...  [0.064s]
Building seed filter...  [0.008s]
Searching alignments...  [0.202s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.31s]
Building query seed array...  [0.179s]
Computing hash join...  [0.119s]
Building seed filter...  [0.012s]
Searching alignments...  [0.147s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.027s]
Computing alignments...  [4.449s]
Deallocating reference...  [0.001s]
Loading reference sequences...  [0.001s]
Deallocating buffers...  [0.001s]
Deallocating queries...  [0s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.007s]
Closing the database file...  [0.001s]
Deallocating taxonomy...  [0s]
Total time = 50.058s
Reported 162540 pairwise alignments, 162540 HSPs.
12232 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 273393x273393 matrix with 5505922 entries to stream <out/merged.mci>
[mclIO] wrote 273393 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 273393 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 273393x273393 matrix with 5505922 entries
[mcl] pid 18920
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  44.35  2.77 0.98/0.12/3.51 2.49 2.33 2.33   0
  2  ...................  50.37 11.95 0.87/0.10/4.57 3.64 0.87 2.03   3
  3  ...................  35.81  7.15 0.84/0.13/6.89 2.33 0.73 1.49   1
  4  ...................  22.13  3.10 0.82/0.15/14.55 1.52 0.73 1.09   0
  5  ...................  17.56  1.56 0.82/0.09/5.88 1.21 0.71 0.78   0
  6  ...................  10.22  0.83 0.82/0.10/3.59 1.09 0.73 0.57   0
  7  ...................  10.88  0.54 0.82/0.14/3.94 1.03 0.78 0.44   0
  8  ...................   6.19  0.39 0.83/0.20/1.85 1.01 0.81 0.36   0
  9  ...................   5.23  0.33 0.86/0.20/1.29 1.01 0.81 0.29   0
 10  ...................   5.10  0.26 0.89/0.25/1.26 1.00 0.81 0.24   0
 11  ...................   4.31  0.23 0.92/0.23/1.36 1.00 0.82 0.19   0
 12  ...................   4.99  0.19 0.95/0.23/1.18 1.00 0.83 0.16   0
 13  ...................   3.98  0.18 0.97/0.22/1.08 1.00 0.86 0.14   0
 14  ...................   4.07  0.15 0.98/0.19/1.17 1.00 0.90 0.12   0
 15  ...................   4.65  0.14 0.99/0.29/1.30 1.00 0.93 0.11   0
 16  ...................   3.35  0.15 0.99/0.28/1.00 1.00 0.96 0.11   0
 17  ...................   3.61  0.15 1.00/0.26/1.00 1.00 0.97 0.11   0
 18  ...................   4.74  0.13 1.00/0.29/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.14 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.05  0.13 1.00/0.46/1.00 1.00 0.99 0.10   0
 21  ...................   1.11  0.14 1.00/0.57/1.00 1.00 1.00 0.10   0
 22  ...................   0.49  0.13 1.00/0.64/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.14 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.15  0.12 1.00/0.85/1.00 1.00 1.00 0.10   0
 25  ...................   0.11  0.14 1.00/0.87/1.00 1.00 1.00 0.10   0
 26  ...................   0.03  0.13 1.00/0.96/1.00 1.00 1.00 0.10   0
 27  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
 28  ...................   0.00  0.13 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 34013 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3210 entries from out/pcs_contigs.csv
Read 265908 entries (dropped 2291 singletons) from out/Cyber_profiles.csv
.......... 1.21% 10000/825385.0
.......... 2.42% 20000/825385.0
.......... 3.63% 30000/825385.0
.......... 4.85% 40000/825385.0
.......... 6.06% 50000/825385.0
.......... 7.27% 60000/825385.0
.......... 8.48% 70000/825385.0
.......... 9.69% 80000/825385.0
..........10.90% 90000/825385.0
..........12.12% 100000/825385.0
..........13.33% 110000/825385.0
..........14.54% 120000/825385.0
..........15.75% 130000/825385.0
..........16.96% 140000/825385.0
..........18.17% 150000/825385.0
..........19.38% 160000/825385.0
..........20.60% 170000/825385.0
..........21.81% 180000/825385.0
..........23.02% 190000/825385.0
..........24.23% 200000/825385.0
..........25.44% 210000/825385.0
..........26.65% 220000/825385.0
..........27.87% 230000/825385.0
..........29.08% 240000/825385.0
..........30.29% 250000/825385.0
..........31.50% 260000/825385.0
..........32.71% 270000/825385.0
..........33.92% 280000/825385.0
..........35.14% 290000/825385.0
..........36.35% 300000/825385.0
..........37.56% 310000/825385.0
..........38.77% 320000/825385.0
..........39.98% 330000/825385.0
..........41.19% 340000/825385.0
..........42.40% 350000/825385.0
..........43.62% 360000/825385.0
..........44.83% 370000/825385.0
..........46.04% 380000/825385.0
..........47.25% 390000/825385.0
..........48.46% 400000/825385.0
..........49.67% 410000/825385.0
..........50.89% 420000/825385.0
..........52.10% 430000/825385.0
..........53.31% 440000/825385.0
..........54.52% 450000/825385.0
..........55.73% 460000/825385.0
..........56.94% 470000/825385.0
..........58.15% 480000/825385.0
..........59.37% 490000/825385.0
..........60.58% 500000/825385.0
..........61.79% 510000/825385.0
..........63.00% 520000/825385.0
..........64.21% 530000/825385.0
..........65.42% 540000/825385.0
..........66.64% 550000/825385.0
..........67.85% 560000/825385.0
..........69.06% 570000/825385.0
..........70.27% 580000/825385.0
..........71.48% 590000/825385.0
..........72.69% 600000/825385.0
..........73.90% 610000/825385.0
..........75.12% 620000/825385.0
..........76.33% 630000/825385.0
..........77.54% 640000/825385.0
..........78.75% 650000/825385.0
..........79.96% 660000/825385.0
..........81.17% 670000/825385.0
..........82.39% 680000/825385.0
..........83.60% 690000/825385.0
..........84.81% 700000/825385.0
..........86.02% 710000/825385.0
..........87.23% 720000/825385.0
..........88.44% 730000/825385.0
..........89.66% 740000/825385.0
..........90.87% 750000/825385.0
..........92.08% 760000/825385.0
..........93.29% 770000/825385.0
..........94.50% 780000/825385.0
..........95.71% 790000/825385.0
..........96.92% 800000/825385.0
..........98.14% 810000/825385.0
..........99.35% 820000/825385.0
..Hypergeometric contig-similarity network:
       3210 contigs,
     126156 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (126156 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3081, 3081)
features: (3081, 512)
y: (3081,) (3081,)
mask: (3081,) (3081,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3080, 3080, 3080],
                       [ 463,  450,  436,  ...,   52,   34,   32]]),
       values=tensor([0.0375, 0.0728, 0.0103,  ..., 0.0665, 0.1206, 0.0030]),
       device='cuda:0', size=(3081, 512), nnz=74017, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    2,  ..., 1410, 2313, 3080],
                       [   0,    0,    0,  ..., 3080, 3080, 3080]]),
       values=tensor([0.0333, 0.0339, 0.0358,  ..., 0.1118, 0.0830, 0.2000]),
       device='cuda:0', size=(3081, 3081), nnz=129559, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 74017
0 20.002994537353516 0.11817168338907469
10 16.203773498535156 0.6072092159048681
20 13.348554611206055 0.813452248234857
30 11.084108352661133 0.8896321070234113
40 9.35662841796875 0.9123002601263471
50 7.992761135101318 0.9342251950947603
60 6.867270469665527 0.9450018580453363
70 5.918436050415039 0.94908955778521
80 5.176182746887207 0.9591230026012635
90 4.483619213104248 0.9628390932738758
100 3.907169818878174 0.9632107023411371
110 3.3972461223602295 0.9669267930137495
120 2.9551146030426025 0.9632107023411371
130 2.5897774696350098 0.9669267930137495
140 2.2961957454681396 0.9680416202155333
150 2.004103183746338 0.967670011148272
160 1.8153462409973145 0.9687848383500557
170 1.634031891822815 0.9698996655518395
180 1.4471690654754639 0.970642883686362
190 1.2919893264770508 0.9702712746191007
200 1.1280704736709595 0.970642883686362
210 1.0289853811264038 0.9702712746191007
220 0.898413896560669 0.9687848383500557
230 0.8896607756614685 0.9654403567447045
240 0.8055357933044434 0.9717577108881457
250 0.6904697418212891 0.9698996655518395
260 0.680005669593811 0.9624674842066147
270 0.6275845170021057 0.9658119658119658
280 0.5771582126617432 0.9665551839464883
290 0.5263994932174683 0.9743589743589743
300 0.5205338001251221 0.9725009290226682
310 0.4951600432395935 0.9721293199554069
320 0.45531168580055237 0.9736157562244518
330 0.42441433668136597 0.9747305834262356
340 0.43487799167633057 0.9665551839464883
350 0.38589775562286377 0.9698996655518395
360 0.3866713047027588 0.9717577108881457
370 0.3469926118850708 0.9702712746191007
380 0.37079185247421265 0.9635823114083983
390 0.3640521466732025 0.9528056484578223
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
error length < 2000bp
DTR_734078
error length < 2000bp
DTR_733917
error length < 2000bp
DTR_733777
error length < 2000bp
DTR_734418
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_537.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_583.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_631.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_752.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_537.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.07s]
Loading sequences...  [0.43s]
Masking sequences...  [0.452s]
Writing sequences...  [0.064s]
Hashing sequences...  [0.023s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.06s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.055s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.028s]
Opening the output file...  [0s]
Loading query sequences...  [0.549s]
Masking queries...  [0.71s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.347s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.13s]
Masking reference...  [0.491s]
Initializing temporary storage...  [0.03s]
Building reference histograms...  [1.384s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.155s]
Computing hash join...  [0.075s]
Building seed filter...  [0.01s]
Searching alignments...  [0.241s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.199s]
Building query seed array...  [0.145s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.235s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.154s]
Building query seed array...  [0.248s]
Computing hash join...  [0.059s]
Building seed filter...  [0.017s]
Searching alignments...  [0.254s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.16s]
Building query seed array...  [0.159s]
Computing hash join...  [0.066s]
Building seed filter...  [0.009s]
Searching alignments...  [0.231s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.115s]
Building query seed array...  [0.112s]
Computing hash join...  [0.068s]
Building seed filter...  [0.009s]
Searching alignments...  [0.207s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.192s]
Building query seed array...  [0.19s]
Computing hash join...  [0.055s]
Building seed filter...  [0.008s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.134s]
Computing hash join...  [0.087s]
Building seed filter...  [0.011s]
Searching alignments...  [0.25s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.133s]
Building query seed array...  [0.137s]
Computing hash join...  [0.103s]
Building seed filter...  [0.014s]
Searching alignments...  [0.228s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.126s]
Computing hash join...  [0.061s]
Building seed filter...  [0.014s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.184s]
Building query seed array...  [0.181s]
Computing hash join...  [0.067s]
Building seed filter...  [0.009s]
Searching alignments...  [0.228s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.221s]
Building query seed array...  [0.175s]
Computing hash join...  [0.071s]
Building seed filter...  [0.012s]
Searching alignments...  [0.254s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.189s]
Building query seed array...  [0.105s]
Computing hash join...  [0.055s]
Building seed filter...  [0.008s]
Searching alignments...  [0.263s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.11s]
Building query seed array...  [0.161s]
Computing hash join...  [0.061s]
Building seed filter...  [0.013s]
Searching alignments...  [0.304s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.127s]
Computing hash join...  [0.087s]
Building seed filter...  [0.008s]
Searching alignments...  [0.227s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.203s]
Building query seed array...  [0.141s]
Computing hash join...  [0.06s]
Building seed filter...  [0.008s]
Searching alignments...  [0.216s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.163s]
Computing hash join...  [0.084s]
Building seed filter...  [0.008s]
Searching alignments...  [0.267s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.102s]
Building query seed array...  [0.176s]
Computing hash join...  [0.078s]
Building seed filter...  [0.011s]
Searching alignments...  [0.25s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.206s]
Building query seed array...  [0.13s]
Computing hash join...  [0.062s]
Building seed filter...  [0.009s]
Searching alignments...  [0.232s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.173s]
Building query seed array...  [0.2s]
Computing hash join...  [0.053s]
Building seed filter...  [0.012s]
Searching alignments...  [0.248s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.116s]
Building query seed array...  [0.162s]
Computing hash join...  [0.072s]
Building seed filter...  [0.01s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.158s]
Building query seed array...  [0.169s]
Computing hash join...  [0.054s]
Building seed filter...  [0.013s]
Searching alignments...  [0.244s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.218s]
Building query seed array...  [0.127s]
Computing hash join...  [0.069s]
Building seed filter...  [0.011s]
Searching alignments...  [0.244s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.161s]
Building query seed array...  [0.152s]
Computing hash join...  [0.083s]
Building seed filter...  [0.012s]
Searching alignments...  [0.241s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.161s]
Building query seed array...  [0.108s]
Computing hash join...  [0.084s]
Building seed filter...  [0.015s]
Searching alignments...  [0.216s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.16s]
Building query seed array...  [0.138s]
Computing hash join...  [0.074s]
Building seed filter...  [0.008s]
Searching alignments...  [0.199s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.135s]
Building query seed array...  [0.139s]
Computing hash join...  [0.052s]
Building seed filter...  [0.012s]
Searching alignments...  [0.233s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.162s]
Building query seed array...  [0.122s]
Computing hash join...  [0.067s]
Building seed filter...  [0.012s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.157s]
Building query seed array...  [0.173s]
Computing hash join...  [0.088s]
Building seed filter...  [0.01s]
Searching alignments...  [0.201s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.195s]
Building query seed array...  [0.145s]
Computing hash join...  [0.088s]
Building seed filter...  [0.011s]
Searching alignments...  [0.206s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.195s]
Computing hash join...  [0.069s]
Building seed filter...  [0.009s]
Searching alignments...  [0.227s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.183s]
Building query seed array...  [0.151s]
Computing hash join...  [0.076s]
Building seed filter...  [0.008s]
Searching alignments...  [0.204s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.171s]
Building query seed array...  [0.116s]
Computing hash join...  [0.086s]
Building seed filter...  [0.009s]
Searching alignments...  [0.231s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.118s]
Building query seed array...  [0.135s]
Computing hash join...  [0.056s]
Building seed filter...  [0.015s]
Searching alignments...  [0.222s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.195s]
Building query seed array...  [0.161s]
Computing hash join...  [0.06s]
Building seed filter...  [0.012s]
Searching alignments...  [0.261s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.168s]
Building query seed array...  [0.17s]
Computing hash join...  [0.06s]
Building seed filter...  [0.01s]
Searching alignments...  [0.291s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.104s]
Building query seed array...  [0.122s]
Computing hash join...  [0.074s]
Building seed filter...  [0.011s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.158s]
Computing hash join...  [0.079s]
Building seed filter...  [0.009s]
Searching alignments...  [0.225s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.16s]
Computing hash join...  [0.071s]
Building seed filter...  [0.011s]
Searching alignments...  [0.22s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.181s]
Building query seed array...  [0.149s]
Computing hash join...  [0.059s]
Building seed filter...  [0.012s]
Searching alignments...  [0.215s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.169s]
Building query seed array...  [0.169s]
Computing hash join...  [0.098s]
Building seed filter...  [0.009s]
Searching alignments...  [0.222s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.162s]
Building query seed array...  [0.155s]
Computing hash join...  [0.073s]
Building seed filter...  [0.012s]
Searching alignments...  [0.213s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.207s]
Computing hash join...  [0.054s]
Building seed filter...  [0.012s]
Searching alignments...  [0.216s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.189s]
Building query seed array...  [0.142s]
Computing hash join...  [0.089s]
Building seed filter...  [0.013s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.165s]
Building query seed array...  [0.106s]
Computing hash join...  [0.098s]
Building seed filter...  [0.013s]
Searching alignments...  [0.246s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.166s]
Building query seed array...  [0.13s]
Computing hash join...  [0.083s]
Building seed filter...  [0.016s]
Searching alignments...  [0.193s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.238s]
Building query seed array...  [0.172s]
Computing hash join...  [0.096s]
Building seed filter...  [0.009s]
Searching alignments...  [0.252s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.17s]
Building query seed array...  [0.19s]
Computing hash join...  [0.059s]
Building seed filter...  [0.014s]
Searching alignments...  [0.24s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.153s]
Building query seed array...  [0.112s]
Computing hash join...  [0.087s]
Building seed filter...  [0.014s]
Searching alignments...  [0.25s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.162s]
Building query seed array...  [0.111s]
Computing hash join...  [0.077s]
Building seed filter...  [0.027s]
Searching alignments...  [0.221s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.199s]
Building query seed array...  [0.154s]
Computing hash join...  [0.06s]
Building seed filter...  [0.012s]
Searching alignments...  [0.2s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.218s]
Building query seed array...  [0.162s]
Computing hash join...  [0.054s]
Building seed filter...  [0.009s]
Searching alignments...  [0.22s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.169s]
Building query seed array...  [0.088s]
Computing hash join...  [0.084s]
Building seed filter...  [0.011s]
Searching alignments...  [0.219s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.199s]
Building query seed array...  [0.156s]
Computing hash join...  [0.069s]
Building seed filter...  [0.009s]
Searching alignments...  [0.208s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.14s]
Building query seed array...  [0.171s]
Computing hash join...  [0.052s]
Building seed filter...  [0.013s]
Searching alignments...  [0.252s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.18s]
Computing hash join...  [0.053s]
Building seed filter...  [0.018s]
Searching alignments...  [0.212s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.11s]
Building query seed array...  [0.145s]
Computing hash join...  [0.069s]
Building seed filter...  [0.024s]
Searching alignments...  [0.238s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.112s]
Building query seed array...  [0.157s]
Computing hash join...  [0.057s]
Building seed filter...  [0.012s]
Searching alignments...  [0.205s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.2s]
Building query seed array...  [0.189s]
Computing hash join...  [0.081s]
Building seed filter...  [0.017s]
Searching alignments...  [0.243s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.163s]
Computing hash join...  [0.055s]
Building seed filter...  [0.014s]
Searching alignments...  [0.269s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.139s]
Computing hash join...  [0.052s]
Building seed filter...  [0.017s]
Searching alignments...  [0.206s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.105s]
Building query seed array...  [0.151s]
Computing hash join...  [0.063s]
Building seed filter...  [0.009s]
Searching alignments...  [0.222s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.182s]
Building query seed array...  [0.114s]
Computing hash join...  [0.074s]
Building seed filter...  [0.017s]
Searching alignments...  [0.261s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.219s]
Building query seed array...  [0.202s]
Computing hash join...  [0.069s]
Building seed filter...  [0.009s]
Searching alignments...  [0.197s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.156s]
Building query seed array...  [0.133s]
Computing hash join...  [0.064s]
Building seed filter...  [0.017s]
Searching alignments...  [0.196s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.05s]
Computing alignments...  [8.1s]
Deallocating reference...  [0.001s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0s]
Deallocating queries...  [0.002s]
Loading query sequences...  [0.001s]
Closing the input file...  [0.001s]
Closing the output file...  [0.013s]
Closing the database file...  [0.001s]
Deallocating taxonomy...  [0s]
Total time = 54.088s
Reported 293572 pairwise alignments, 293572 HSPs.
22021 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
.......................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 283030x283030 matrix with 5767986 entries to stream <out/merged.mci>
[mclIO] wrote 283030 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 283030 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 283030x283030 matrix with 5767986 entries
[mcl] pid 10963
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  55.90  3.25 0.99/0.03/4.00 2.77 2.56 2.56   0
  2  ...................  76.21 15.09 0.86/0.08/4.57 3.98 0.85 2.17   4
  3  ...................  40.46  8.91 0.83/0.10/6.08 2.51 0.69 1.50   1
  4  ...................  22.89  3.45 0.83/0.15/11.10 1.56 0.72 1.08   0
  5  ...................  17.31  1.60 0.82/0.10/5.29 1.22 0.71 0.76   0
  6  ...................  10.06  0.86 0.82/0.12/3.59 1.09 0.73 0.55   0
  7  ...................   9.85  0.57 0.82/0.14/2.45 1.03 0.78 0.43   0
  8  ...................   6.32  0.42 0.83/0.20/1.81 1.01 0.81 0.35   0
  9  ...................   5.30  0.33 0.86/0.20/1.35 1.00 0.81 0.28   0
 10  ...................   5.10  0.28 0.89/0.24/1.53 1.00 0.81 0.23   0
 11  ...................   4.81  0.23 0.93/0.21/1.25 1.00 0.81 0.19   0
 12  ...................   4.81  0.20 0.95/0.22/1.18 1.00 0.83 0.16   0
 13  ...................   4.96  0.18 0.97/0.24/1.01 1.00 0.86 0.13   0
 14  ...................   4.07  0.15 0.98/0.19/1.00 1.00 0.90 0.12   0
 15  ...................   4.66  0.15 0.99/0.24/1.00 1.00 0.92 0.11   0
 16  ...................   3.57  0.15 0.99/0.28/1.00 1.00 0.96 0.11   0
 17  ...................   3.61  0.13 1.00/0.26/1.00 1.00 0.97 0.10   0
 18  ...................   4.74  0.15 1.00/0.23/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.12 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.05  0.14 1.00/0.46/1.00 1.00 1.00 0.10   0
 21  ...................   1.11  0.13 1.00/0.57/1.00 1.00 1.00 0.10   0
 22  ...................   0.43  0.14 1.00/0.69/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.13 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.22  0.14 1.00/0.79/1.00 1.00 1.00 0.10   0
 25  ...................   0.06  0.14 1.00/0.94/1.00 1.00 1.00 0.10   0
 26  ...................   0.00  0.15 1.00/1.00/1.00 1.00 1.00 0.10   0
 27  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 33869 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3437 entries from out/pcs_contigs.csv
Read 274796 entries (dropped 2302 singletons) from out/Cyber_profiles.csv
.......... 0.89% 10000/1127880.0
.......... 1.77% 20000/1127880.0
.......... 2.66% 30000/1127880.0
.......... 3.55% 40000/1127880.0
.......... 4.43% 50000/1127880.0
.......... 5.32% 60000/1127880.0
.......... 6.21% 70000/1127880.0
.......... 7.09% 80000/1127880.0
.......... 7.98% 90000/1127880.0
.......... 8.87% 100000/1127880.0
.......... 9.75% 110000/1127880.0
..........10.64% 120000/1127880.0
..........11.53% 130000/1127880.0
..........12.41% 140000/1127880.0
..........13.30% 150000/1127880.0
..........14.19% 160000/1127880.0
..........15.07% 170000/1127880.0
..........15.96% 180000/1127880.0
..........16.85% 190000/1127880.0
..........17.73% 200000/1127880.0
..........18.62% 210000/1127880.0
..........19.51% 220000/1127880.0
..........20.39% 230000/1127880.0
..........21.28% 240000/1127880.0
..........22.17% 250000/1127880.0
..........23.05% 260000/1127880.0
..........23.94% 270000/1127880.0
..........24.83% 280000/1127880.0
..........25.71% 290000/1127880.0
..........26.60% 300000/1127880.0
..........27.49% 310000/1127880.0
..........28.37% 320000/1127880.0
..........29.26% 330000/1127880.0
..........30.15% 340000/1127880.0
..........31.03% 350000/1127880.0
..........31.92% 360000/1127880.0
..........32.80% 370000/1127880.0
..........33.69% 380000/1127880.0
..........34.58% 390000/1127880.0
..........35.46% 400000/1127880.0
..........36.35% 410000/1127880.0
..........37.24% 420000/1127880.0
..........38.12% 430000/1127880.0
..........39.01% 440000/1127880.0
..........39.90% 450000/1127880.0
..........40.78% 460000/1127880.0
..........41.67% 470000/1127880.0
..........42.56% 480000/1127880.0
..........43.44% 490000/1127880.0
..........44.33% 500000/1127880.0
..........45.22% 510000/1127880.0
..........46.10% 520000/1127880.0
..........46.99% 530000/1127880.0
..........47.88% 540000/1127880.0
..........48.76% 550000/1127880.0
..........49.65% 560000/1127880.0
..........50.54% 570000/1127880.0
..........51.42% 580000/1127880.0
..........52.31% 590000/1127880.0
..........53.20% 600000/1127880.0
..........54.08% 610000/1127880.0
..........54.97% 620000/1127880.0
..........55.86% 630000/1127880.0
..........56.74% 640000/1127880.0
..........57.63% 650000/1127880.0
..........58.52% 660000/1127880.0
..........59.40% 670000/1127880.0
..........60.29% 680000/1127880.0
..........61.18% 690000/1127880.0
..........62.06% 700000/1127880.0
..........62.95% 710000/1127880.0
..........63.84% 720000/1127880.0
..........64.72% 730000/1127880.0
..........65.61% 740000/1127880.0
..........66.50% 750000/1127880.0
..........67.38% 760000/1127880.0
..........68.27% 770000/1127880.0
..........69.16% 780000/1127880.0
..........70.04% 790000/1127880.0
..........70.93% 800000/1127880.0
..........71.82% 810000/1127880.0
..........72.70% 820000/1127880.0
..........73.59% 830000/1127880.0
..........74.48% 840000/1127880.0
..........75.36% 850000/1127880.0
..........76.25% 860000/1127880.0
..........77.14% 870000/1127880.0
..........78.02% 880000/1127880.0
..........78.91% 890000/1127880.0
..........79.80% 900000/1127880.0
..........80.68% 910000/1127880.0
..........81.57% 920000/1127880.0
..........82.46% 930000/1127880.0
..........83.34% 940000/1127880.0
..........84.23% 950000/1127880.0
..........85.12% 960000/1127880.0
..........86.00% 970000/1127880.0
..........86.89% 980000/1127880.0
..........87.78% 990000/1127880.0
..........88.66% 1000000/1127880.0
..........89.55% 1010000/1127880.0
..........90.44% 1020000/1127880.0
..........91.32% 1030000/1127880.0
..........92.21% 1040000/1127880.0
..........93.10% 1050000/1127880.0
..........93.98% 1060000/1127880.0
..........94.87% 1070000/1127880.0
..........95.75% 1080000/1127880.0
..........96.64% 1090000/1127880.0
..........97.53% 1100000/1127880.0
..........98.41% 1110000/1127880.0
..........99.30% 1120000/1127880.0
....Hypergeometric contig-similarity network:
       3437 contigs,
     147674 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (147674 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3367, 3367)
features: (3367, 512)
y: (3367,) (3367,)
mask: (3367,) (3367,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3366, 3366, 3366],
                       [ 463,  447,  436,  ...,   52,   34,   32]]),
       values=tensor([0.1010, 0.0811, 0.0957,  ..., 0.1422, 0.1266, 0.0237]),
       device='cuda:0', size=(3367, 512), nnz=82037, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    2,  ..., 1607, 2381, 3366],
                       [   0,    0,    0,  ..., 3366, 3366, 3366]]),
       values=tensor([0.0172, 0.0536, 0.0158,  ..., 0.0632, 0.0620, 0.2000]),
       device='cuda:0', size=(3367, 3367), nnz=151127, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 82037
0 20.011688232421875 0.12337421033073206
10 16.21574592590332 0.5849126718691936
20 13.38813304901123 0.856558900037161
30 11.122255325317383 0.8818283166109253
40 9.363146781921387 0.9126718691936083
50 8.004168510437012 0.9386845039018952
60 6.8877644538879395 0.9479747305834262
70 5.947556495666504 0.9557785209959123
80 5.181443214416504 0.955406911928651
90 4.417016983032227 0.9620958751393534
100 3.8537371158599854 0.9606094388703085
110 3.425835371017456 0.9594946116685247
120 2.978513717651367 0.9669267930137495
130 2.6179184913635254 0.9687848383500557
140 2.3293161392211914 0.9650687476774433
150 2.054349422454834 0.9602378298030472
160 1.786845326423645 0.9639539204756596
170 1.603914499282837 0.9658119658119658
180 1.474433422088623 0.9695280564845782
190 1.2982099056243896 0.9635823114083983
200 1.1523994207382202 0.9658119658119658
210 1.0661680698394775 0.9580081753994798
220 0.951019287109375 0.9687848383500557
230 0.8522433042526245 0.9713861018208845
240 0.8063022494316101 0.9698996655518395
250 0.7268459796905518 0.9687848383500557
260 0.6869246959686279 0.9713861018208845
270 0.5939274430274963 0.9639539204756596
280 0.5842283964157104 0.9710144927536232
290 0.5506100654602051 0.9628390932738758
300 0.5362237691879272 0.9695280564845782
310 0.47378218173980713 0.9702712746191007
320 0.4826303720474243 0.9717577108881457
330 0.4571397304534912 0.9687848383500557
340 0.44162115454673767 0.9698996655518395
350 0.3850674629211426 0.9684132292827945
360 0.3700713813304901 0.9665551839464883
370 0.37187984585762024 0.9721293199554069
380 0.375301718711853 0.9725009290226682
390 0.3493448495864868 0.9710144927536232
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.05s]
Loading sequences...  [0.424s]
Masking sequences...  [0.45s]
Writing sequences...  [0.063s]
Hashing sequences...  [0.024s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.031s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.053s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.028s]
Opening the output file...  [0s]
Loading query sequences...  [0.542s]
Masking queries...  [0.702s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.29s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.117s]
Masking reference...  [0.468s]
Initializing temporary storage...  [0.033s]
Building reference histograms...  [1.424s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.212s]
Building query seed array...  [0.174s]
Computing hash join...  [0.065s]
Building seed filter...  [0.008s]
Searching alignments...  [0.266s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.169s]
Building query seed array...  [0.199s]
Computing hash join...  [0.06s]
Building seed filter...  [0.013s]
Searching alignments...  [0.304s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.14s]
Building query seed array...  [0.15s]
Computing hash join...  [0.059s]
Building seed filter...  [0.012s]
Searching alignments...  [0.287s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.184s]
Building query seed array...  [0.244s]
Computing hash join...  [0.068s]
Building seed filter...  [0.011s]
Searching alignments...  [0.249s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.134s]
Building query seed array...  [0.117s]
Computing hash join...  [0.057s]
Building seed filter...  [0.007s]
Searching alignments...  [0.22s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.295s]
Building query seed array...  [0.118s]
Computing hash join...  [0.087s]
Building seed filter...  [0.008s]
Searching alignments...  [0.219s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.201s]
Building query seed array...  [0.216s]
Computing hash join...  [0.053s]
Building seed filter...  [0.01s]
Searching alignments...  [0.218s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.178s]
Building query seed array...  [0.161s]
Computing hash join...  [0.069s]
Building seed filter...  [0.011s]
Searching alignments...  [0.216s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.153s]
Building query seed array...  [0.126s]
Computing hash join...  [0.072s]
Building seed filter...  [0.008s]
Searching alignments...  [0.244s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.239s]
Building query seed array...  [0.13s]
Computing hash join...  [0.071s]
Building seed filter...  [0.01s]
Searching alignments...  [0.254s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.23s]
Building query seed array...  [0.15s]
Computing hash join...  [0.082s]
Building seed filter...  [0.009s]
Searching alignments...  [0.299s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.25s]
Building query seed array...  [0.114s]
Computing hash join...  [0.061s]
Building seed filter...  [0.01s]
Searching alignments...  [0.283s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.127s]
Computing hash join...  [0.073s]
Building seed filter...  [0.011s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.212s]
Building query seed array...  [0.126s]
Computing hash join...  [0.085s]
Building seed filter...  [0.008s]
Searching alignments...  [0.336s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.151s]
Computing hash join...  [0.073s]
Building seed filter...  [0.019s]
Searching alignments...  [0.322s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.17s]
Building query seed array...  [0.167s]
Computing hash join...  [0.061s]
Building seed filter...  [0.013s]
Searching alignments...  [0.322s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.337s]
Building query seed array...  [0.105s]
Computing hash join...  [0.062s]
Building seed filter...  [0.018s]
Searching alignments...  [0.324s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.214s]
Building query seed array...  [0.251s]
Computing hash join...  [0.133s]
Building seed filter...  [0.014s]
Searching alignments...  [0.325s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.18s]
Building query seed array...  [0.298s]
Computing hash join...  [0.127s]
Building seed filter...  [0.014s]
Searching alignments...  [0.366s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.23s]
Building query seed array...  [0.094s]
Computing hash join...  [0.1s]
Building seed filter...  [0.011s]
Searching alignments...  [0.238s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.17s]
Building query seed array...  [0.151s]
Computing hash join...  [0.106s]
Building seed filter...  [0.019s]
Searching alignments...  [0.247s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.233s]
Building query seed array...  [0.121s]
Computing hash join...  [0.085s]
Building seed filter...  [0.012s]
Searching alignments...  [0.238s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.211s]
Building query seed array...  [0.191s]
Computing hash join...  [0.068s]
Building seed filter...  [0.011s]
Searching alignments...  [0.258s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.109s]
Building query seed array...  [0.147s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.25s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.102s]
Building query seed array...  [0.154s]
Computing hash join...  [0.082s]
Building seed filter...  [0.01s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.128s]
Building query seed array...  [0.139s]
Computing hash join...  [0.053s]
Building seed filter...  [0.01s]
Searching alignments...  [0.256s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.206s]
Building query seed array...  [0.212s]
Computing hash join...  [0.141s]
Building seed filter...  [0.008s]
Searching alignments...  [0.252s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.133s]
Building query seed array...  [0.139s]
Computing hash join...  [0.065s]
Building seed filter...  [0.01s]
Searching alignments...  [0.215s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.192s]
Building query seed array...  [0.14s]
Computing hash join...  [0.084s]
Building seed filter...  [0.01s]
Searching alignments...  [0.249s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.234s]
Building query seed array...  [0.217s]
Computing hash join...  [0.075s]
Building seed filter...  [0.009s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.238s]
Building query seed array...  [0.151s]
Computing hash join...  [0.053s]
Building seed filter...  [0.017s]
Searching alignments...  [0.289s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.213s]
Building query seed array...  [0.141s]
Computing hash join...  [0.085s]
Building seed filter...  [0.012s]
Searching alignments...  [0.266s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.175s]
Building query seed array...  [0.152s]
Computing hash join...  [0.069s]
Building seed filter...  [0.013s]
Searching alignments...  [0.285s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.187s]
Computing hash join...  [0.059s]
Building seed filter...  [0.01s]
Searching alignments...  [0.234s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.199s]
Building query seed array...  [0.158s]
Computing hash join...  [0.073s]
Building seed filter...  [0.01s]
Searching alignments...  [0.306s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.176s]
Computing hash join...  [0.093s]
Building seed filter...  [0.018s]
Searching alignments...  [0.24s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.156s]
Computing hash join...  [0.053s]
Building seed filter...  [0.011s]
Searching alignments...  [0.252s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.165s]
Building query seed array...  [0.161s]
Computing hash join...  [0.092s]
Building seed filter...  [0.013s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.158s]
Building query seed array...  [0.18s]
Computing hash join...  [0.069s]
Building seed filter...  [0.011s]
Searching alignments...  [0.29s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.167s]
Building query seed array...  [0.088s]
Computing hash join...  [0.063s]
Building seed filter...  [0.011s]
Searching alignments...  [0.26s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.14s]
Building query seed array...  [0.143s]
Computing hash join...  [0.151s]
Building seed filter...  [0.01s]
Searching alignments...  [0.281s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.203s]
Building query seed array...  [0.121s]
Computing hash join...  [0.087s]
Building seed filter...  [0.01s]
Searching alignments...  [0.233s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.224s]
Building query seed array...  [0.208s]
Computing hash join...  [0.054s]
Building seed filter...  [0.009s]
Searching alignments...  [0.229s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.162s]
Computing hash join...  [0.072s]
Building seed filter...  [0.012s]
Searching alignments...  [0.23s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.185s]
Building query seed array...  [0.15s]
Computing hash join...  [0.075s]
Building seed filter...  [0.011s]
Searching alignments...  [0.215s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.212s]
Building query seed array...  [0.11s]
Computing hash join...  [0.09s]
Building seed filter...  [0.009s]
Searching alignments...  [0.213s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.3s]
Building query seed array...  [0.124s]
Computing hash join...  [0.058s]
Building seed filter...  [0.017s]
Searching alignments...  [0.233s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.204s]
Building query seed array...  [0.158s]
Computing hash join...  [0.059s]
Building seed filter...  [0.015s]
Searching alignments...  [0.239s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.218s]
Building query seed array...  [0.134s]
Computing hash join...  [0.078s]
Building seed filter...  [0.011s]
Searching alignments...  [0.224s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.214s]
Building query seed array...  [0.136s]
Computing hash join...  [0.129s]
Building seed filter...  [0.012s]
Searching alignments...  [0.251s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.118s]
Computing hash join...  [0.065s]
Building seed filter...  [0.011s]
Searching alignments...  [0.212s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.124s]
Building query seed array...  [0.14s]
Computing hash join...  [0.149s]
Building seed filter...  [0.011s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.143s]
Computing hash join...  [0.059s]
Building seed filter...  [0.012s]
Searching alignments...  [0.221s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.114s]
Computing hash join...  [0.055s]
Building seed filter...  [0.009s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.211s]
Computing hash join...  [0.134s]
Building seed filter...  [0.01s]
Searching alignments...  [0.278s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.114s]
Building query seed array...  [0.162s]
Computing hash join...  [0.069s]
Building seed filter...  [0.017s]
Searching alignments...  [0.241s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.102s]
Building query seed array...  [0.098s]
Computing hash join...  [0.058s]
Building seed filter...  [0.011s]
Searching alignments...  [0.318s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.231s]
Building query seed array...  [0.117s]
Computing hash join...  [0.101s]
Building seed filter...  [0.011s]
Searching alignments...  [0.247s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.211s]
Building query seed array...  [0.17s]
Computing hash join...  [0.078s]
Building seed filter...  [0.013s]
Searching alignments...  [0.288s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.103s]
Building query seed array...  [0.118s]
Computing hash join...  [0.075s]
Building seed filter...  [0.014s]
Searching alignments...  [0.258s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.126s]
Building query seed array...  [0.141s]
Computing hash join...  [0.06s]
Building seed filter...  [0.015s]
Searching alignments...  [0.219s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.267s]
Building query seed array...  [0.168s]
Computing hash join...  [0.061s]
Building seed filter...  [0.015s]
Searching alignments...  [0.285s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.241s]
Building query seed array...  [0.141s]
Computing hash join...  [0.053s]
Building seed filter...  [0.01s]
Searching alignments...  [0.209s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.173s]
Building query seed array...  [0.088s]
Computing hash join...  [0.084s]
Building seed filter...  [0.011s]
Searching alignments...  [0.211s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.048s]
Computing alignments...  [8.245s]
Deallocating reference...  [0s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0.001s]
Deallocating queries...  [0.001s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.011s]
Closing the database file...  [0.001s]
Deallocating taxonomy...  [0s]
Total time = 57.391s
Reported 326910 pairwise alignments, 326910 HSPs.
24329 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
........................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 285424x285424 matrix with 5834662 entries to stream <out/merged.mci>
[mclIO] wrote 285424 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 285424 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 285424x285424 matrix with 5834662 entries
[mcl] pid 23687
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  54.25  3.36 0.99/0.02/4.44 2.81 2.60 2.60   0
  2  ...................  81.99 15.49 0.86/0.08/5.12 3.88 0.85 2.20   5
  3  ...................  45.44  9.30 0.83/0.07/5.94 2.50 0.68 1.50   2
  4  ...................  28.38  3.55 0.82/0.10/12.37 1.57 0.72 1.07   0
  5  ...................  16.72  1.63 0.82/0.12/5.22 1.22 0.71 0.76   0
  6  ...................  10.80  0.89 0.82/0.12/3.59 1.09 0.72 0.55   0
  7  ...................   8.67  0.58 0.82/0.14/3.35 1.03 0.78 0.43   0
  8  ...................   6.18  0.43 0.84/0.19/2.13 1.01 0.81 0.35   0
  9  ...................   5.31  0.33 0.86/0.20/1.80 1.01 0.81 0.28   0
 10  ...................   5.10  0.28 0.89/0.24/1.67 1.00 0.81 0.23   0
 11  ...................   4.31  0.24 0.93/0.22/1.26 1.00 0.82 0.19   0
 12  ...................   4.81  0.20 0.95/0.22/1.18 1.00 0.83 0.15   0
 13  ...................   4.96  0.18 0.97/0.25/1.08 1.00 0.86 0.13   0
 14  ...................   5.16  0.17 0.98/0.19/1.21 1.00 0.90 0.12   0
 15  ...................   4.66  0.15 0.99/0.24/1.00 1.00 0.93 0.11   0
 16  ...................   3.57  0.15 0.99/0.28/1.00 1.00 0.96 0.11   0
 17  ...................   3.78  0.14 1.00/0.25/1.00 1.00 0.97 0.10   0
 18  ...................   4.74  0.15 1.00/0.29/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.14 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   0.77  0.13 1.00/0.57/1.00 1.00 1.00 0.10   0
 21  ...................   0.47  0.14 1.00/0.65/1.00 1.00 1.00 0.10   0
 22  ...................   0.43  0.13 1.00/0.69/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.14 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.36  0.14 1.00/0.77/1.00 1.00 1.00 0.10   0
 25  ...................   0.50  0.13 1.00/0.63/1.00 1.00 1.00 0.10   0
 26  ...................   0.31  0.13 1.00/0.78/1.00 1.00 1.00 0.10   0
 27  ...................   0.03  0.13 1.00/0.98/1.00 1.00 1.00 0.10   0
 28  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 33961 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3463 entries from out/pcs_contigs.csv
Read 277489 entries (dropped 2279 singletons) from out/Cyber_profiles.csv
.......... 0.83% 10000/1199824.0
.......... 1.67% 20000/1199824.0
.......... 2.50% 30000/1199824.0
.......... 3.33% 40000/1199824.0
.......... 4.17% 50000/1199824.0
.......... 5.00% 60000/1199824.0
.......... 5.83% 70000/1199824.0
.......... 6.67% 80000/1199824.0
.......... 7.50% 90000/1199824.0
.......... 8.33% 100000/1199824.0
.......... 9.17% 110000/1199824.0
..........10.00% 120000/1199824.0
..........10.83% 130000/1199824.0
..........11.67% 140000/1199824.0
..........12.50% 150000/1199824.0
..........13.34% 160000/1199824.0
..........14.17% 170000/1199824.0
..........15.00% 180000/1199824.0
..........15.84% 190000/1199824.0
..........16.67% 200000/1199824.0
..........17.50% 210000/1199824.0
..........18.34% 220000/1199824.0
..........19.17% 230000/1199824.0
..........20.00% 240000/1199824.0
..........20.84% 250000/1199824.0
..........21.67% 260000/1199824.0
..........22.50% 270000/1199824.0
..........23.34% 280000/1199824.0
..........24.17% 290000/1199824.0
..........25.00% 300000/1199824.0
..........25.84% 310000/1199824.0
..........26.67% 320000/1199824.0
..........27.50% 330000/1199824.0
..........28.34% 340000/1199824.0
..........29.17% 350000/1199824.0
..........30.00% 360000/1199824.0
..........30.84% 370000/1199824.0
..........31.67% 380000/1199824.0
..........32.50% 390000/1199824.0
..........33.34% 400000/1199824.0
..........34.17% 410000/1199824.0
..........35.01% 420000/1199824.0
..........35.84% 430000/1199824.0
..........36.67% 440000/1199824.0
..........37.51% 450000/1199824.0
..........38.34% 460000/1199824.0
..........39.17% 470000/1199824.0
..........40.01% 480000/1199824.0
..........40.84% 490000/1199824.0
..........41.67% 500000/1199824.0
..........42.51% 510000/1199824.0
..........43.34% 520000/1199824.0
..........44.17% 530000/1199824.0
..........45.01% 540000/1199824.0
..........45.84% 550000/1199824.0
..........46.67% 560000/1199824.0
..........47.51% 570000/1199824.0
..........48.34% 580000/1199824.0
..........49.17% 590000/1199824.0
..........50.01% 600000/1199824.0
..........50.84% 610000/1199824.0
..........51.67% 620000/1199824.0
..........52.51% 630000/1199824.0
..........53.34% 640000/1199824.0
..........54.17% 650000/1199824.0
..........55.01% 660000/1199824.0
..........55.84% 670000/1199824.0
..........56.67% 680000/1199824.0
..........57.51% 690000/1199824.0
..........58.34% 700000/1199824.0
..........59.18% 710000/1199824.0
..........60.01% 720000/1199824.0
..........60.84% 730000/1199824.0
..........61.68% 740000/1199824.0
..........62.51% 750000/1199824.0
..........63.34% 760000/1199824.0
..........64.18% 770000/1199824.0
..........65.01% 780000/1199824.0
..........65.84% 790000/1199824.0
..........66.68% 800000/1199824.0
..........67.51% 810000/1199824.0
..........68.34% 820000/1199824.0
..........69.18% 830000/1199824.0
..........70.01% 840000/1199824.0
..........70.84% 850000/1199824.0
..........71.68% 860000/1199824.0
..........72.51% 870000/1199824.0
..........73.34% 880000/1199824.0
..........74.18% 890000/1199824.0
..........75.01% 900000/1199824.0
..........75.84% 910000/1199824.0
..........76.68% 920000/1199824.0
..........77.51% 930000/1199824.0
..........78.34% 940000/1199824.0
..........79.18% 950000/1199824.0
..........80.01% 960000/1199824.0
..........80.85% 970000/1199824.0
..........81.68% 980000/1199824.0
..........82.51% 990000/1199824.0
..........83.35% 1000000/1199824.0
..........84.18% 1010000/1199824.0
..........85.01% 1020000/1199824.0
..........85.85% 1030000/1199824.0
..........86.68% 1040000/1199824.0
..........87.51% 1050000/1199824.0
..........88.35% 1060000/1199824.0
..........89.18% 1070000/1199824.0
..........90.01% 1080000/1199824.0
..........90.85% 1090000/1199824.0
..........91.68% 1100000/1199824.0
..........92.51% 1110000/1199824.0
..........93.35% 1120000/1199824.0
..........94.18% 1130000/1199824.0
..........95.01% 1140000/1199824.0
..........95.85% 1150000/1199824.0
..........96.68% 1160000/1199824.0
..........97.51% 1170000/1199824.0
..........98.35% 1180000/1199824.0
..........99.18% 1190000/1199824.0
......Hypergeometric contig-similarity network:
       3463 contigs,
     166484 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (166484 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3386, 3386)
features: (3386, 512)
y: (3386,) (3386,)
mask: (3386,) (3386,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3385, 3385, 3385],
                       [ 463,  450,  447,  ...,   32,    9,    1]]),
       values=tensor([0.0991, 0.0090, 0.0357,  ..., 0.0456, 0.0917, 0.0058]),
       device='cuda:0', size=(3386, 512), nnz=82266, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    2,  ..., 1182, 1522, 3385],
                       [   0,    0,    0,  ..., 3385, 3385, 3385]]),
       values=tensor([0.0109, 0.0104, 0.0091,  ..., 0.0786, 0.1491, 0.3333]),
       device='cuda:0', size=(3386, 3386), nnz=169932, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 82266
0 20.0208740234375 0.11148272017837235
10 16.272756576538086 0.5730211817168339
20 13.436403274536133 0.821256038647343
30 11.10680103302002 0.8680787811222593
40 9.414202690124512 0.8985507246376812
50 7.981321811676025 0.9416573764399851
60 6.887892723083496 0.944630248978075
70 5.952271938323975 0.9502043849869937
80 5.164348602294922 0.9542920847268673
90 4.446959018707275 0.9576365663322185
100 3.9141693115234375 0.955406911928651
110 3.4347469806671143 0.961352657004831
120 2.954378128051758 0.9591230026012635
130 2.6102395057678223 0.9594946116685247
140 2.3267905712127686 0.9624674842066147
150 2.0158562660217285 0.9632107023411371
160 1.791755199432373 0.9684132292827945
170 1.6137804985046387 0.966183574879227
180 1.422867774963379 0.9669267930137495
190 1.2762444019317627 0.9669267930137495
200 1.1647417545318604 0.9672984020810108
210 1.0485998392105103 0.9654403567447045
220 0.9808604717254639 0.9684132292827945
230 0.8639320731163025 0.9680416202155333
240 0.7953213453292847 0.9672984020810108
250 0.7334657907485962 0.9650687476774433
260 0.71517014503479 0.967670011148272
270 0.618514358997345 0.970642883686362
280 0.581702470779419 0.970642883686362
290 0.5664116144180298 0.9632107023411371
300 0.4776879847049713 0.9684132292827945
310 0.4962838292121887 0.9658119658119658
320 0.46827152371406555 0.9698996655518395
330 0.41601431369781494 0.969156447417317
340 0.44435858726501465 0.9650687476774433
350 0.4107664227485657 0.9698996655518395
360 0.4009247124195099 0.9643255295429208
370 0.42215412855148315 0.9710144927536232
380 0.3722669184207916 0.970642883686362
390 0.3685746490955353 0.9684132292827945
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.046s]
Loading sequences...  [0.427s]
Masking sequences...  [0.45s]
Writing sequences...  [0.068s]
Hashing sequences...  [0.024s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.007s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.034s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.053s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.028s]
Opening the output file...  [0s]
Loading query sequences...  [0.54s]
Masking queries...  [0.766s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.899s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.12s]
Masking reference...  [0.92s]
Initializing temporary storage...  [0.032s]
Building reference histograms...  [2.616s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.234s]
Building query seed array...  [0.194s]
Computing hash join...  [0.092s]
Building seed filter...  [0.009s]
Searching alignments...  [0.271s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.278s]
Building query seed array...  [0.229s]
Computing hash join...  [0.066s]
Building seed filter...  [0.022s]
Searching alignments...  [0.419s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.207s]
Building query seed array...  [0.222s]
Computing hash join...  [0.076s]
Building seed filter...  [0.019s]
Searching alignments...  [0.417s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.247s]
Building query seed array...  [0.115s]
Computing hash join...  [0.113s]
Building seed filter...  [0.017s]
Searching alignments...  [0.394s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.236s]
Building query seed array...  [0.21s]
Computing hash join...  [0.059s]
Building seed filter...  [0.012s]
Searching alignments...  [0.378s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.244s]
Building query seed array...  [0.267s]
Computing hash join...  [0.055s]
Building seed filter...  [0.015s]
Searching alignments...  [0.369s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.321s]
Building query seed array...  [0.31s]
Computing hash join...  [0.095s]
Building seed filter...  [0.012s]
Searching alignments...  [0.389s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.233s]
Building query seed array...  [0.098s]
Computing hash join...  [0.113s]
Building seed filter...  [0.011s]
Searching alignments...  [0.484s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.165s]
Building query seed array...  [0.261s]
Computing hash join...  [0.088s]
Building seed filter...  [0.016s]
Searching alignments...  [0.46s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.277s]
Building query seed array...  [0.302s]
Computing hash join...  [0.066s]
Building seed filter...  [0.012s]
Searching alignments...  [0.434s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.291s]
Building query seed array...  [0.167s]
Computing hash join...  [0.13s]
Building seed filter...  [0.013s]
Searching alignments...  [0.382s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.236s]
Building query seed array...  [0.256s]
Computing hash join...  [0.12s]
Building seed filter...  [0.013s]
Searching alignments...  [0.396s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.244s]
Building query seed array...  [0.22s]
Computing hash join...  [0.054s]
Building seed filter...  [0.014s]
Searching alignments...  [0.397s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.296s]
Building query seed array...  [0.289s]
Computing hash join...  [0.072s]
Building seed filter...  [0.01s]
Searching alignments...  [0.525s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.304s]
Building query seed array...  [0.268s]
Computing hash join...  [0.169s]
Building seed filter...  [0.012s]
Searching alignments...  [0.391s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.173s]
Building query seed array...  [0.153s]
Computing hash join...  [0.109s]
Building seed filter...  [0.014s]
Searching alignments...  [0.322s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.153s]
Building query seed array...  [0.218s]
Computing hash join...  [0.087s]
Building seed filter...  [0.013s]
Searching alignments...  [0.381s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.291s]
Building query seed array...  [0.141s]
Computing hash join...  [0.078s]
Building seed filter...  [0.012s]
Searching alignments...  [0.372s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.317s]
Building query seed array...  [0.193s]
Computing hash join...  [0.062s]
Building seed filter...  [0.013s]
Searching alignments...  [0.582s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.231s]
Computing hash join...  [0.165s]
Building seed filter...  [0.013s]
Searching alignments...  [0.449s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.248s]
Building query seed array...  [0.231s]
Computing hash join...  [0.054s]
Building seed filter...  [0.009s]
Searching alignments...  [0.382s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.312s]
Building query seed array...  [0.265s]
Computing hash join...  [0.064s]
Building seed filter...  [0.016s]
Searching alignments...  [0.381s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.312s]
Building query seed array...  [0.165s]
Computing hash join...  [0.158s]
Building seed filter...  [0.013s]
Searching alignments...  [0.361s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.238s]
Building query seed array...  [0.147s]
Computing hash join...  [0.085s]
Building seed filter...  [0.015s]
Searching alignments...  [0.41s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.241s]
Building query seed array...  [0.261s]
Computing hash join...  [0.093s]
Building seed filter...  [0.012s]
Searching alignments...  [0.472s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.276s]
Building query seed array...  [0.197s]
Computing hash join...  [0.115s]
Building seed filter...  [0.012s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.318s]
Building query seed array...  [0.269s]
Computing hash join...  [0.077s]
Building seed filter...  [0.014s]
Searching alignments...  [0.459s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.244s]
Building query seed array...  [0.161s]
Computing hash join...  [0.062s]
Building seed filter...  [0.017s]
Searching alignments...  [0.368s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.225s]
Building query seed array...  [0.145s]
Computing hash join...  [0.112s]
Building seed filter...  [0.01s]
Searching alignments...  [0.461s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.281s]
Building query seed array...  [0.18s]
Computing hash join...  [0.067s]
Building seed filter...  [0.017s]
Searching alignments...  [0.389s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.317s]
Building query seed array...  [0.27s]
Computing hash join...  [0.062s]
Building seed filter...  [0.015s]
Searching alignments...  [0.305s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.309s]
Building query seed array...  [0.214s]
Computing hash join...  [0.074s]
Building seed filter...  [0.018s]
Searching alignments...  [0.37s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.316s]
Building query seed array...  [0.153s]
Computing hash join...  [0.062s]
Building seed filter...  [0.013s]
Searching alignments...  [0.477s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.511s]
Building query seed array...  [0.325s]
Computing hash join...  [0.144s]
Building seed filter...  [0.013s]
Searching alignments...  [0.613s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.48s]
Building query seed array...  [0.143s]
Computing hash join...  [0.142s]
Building seed filter...  [0.011s]
Searching alignments...  [0.911s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.406s]
Building query seed array...  [0.254s]
Computing hash join...  [0.056s]
Building seed filter...  [0.011s]
Searching alignments...  [0.909s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.298s]
Building query seed array...  [0.142s]
Computing hash join...  [0.084s]
Building seed filter...  [0.01s]
Searching alignments...  [0.564s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.484s]
Building query seed array...  [0.255s]
Computing hash join...  [0.13s]
Building seed filter...  [0.011s]
Searching alignments...  [0.432s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.342s]
Building query seed array...  [0.19s]
Computing hash join...  [0.07s]
Building seed filter...  [0.013s]
Searching alignments...  [0.242s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.196s]
Building query seed array...  [0.112s]
Computing hash join...  [0.107s]
Building seed filter...  [0.01s]
Searching alignments...  [0.535s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.23s]
Building query seed array...  [0.149s]
Computing hash join...  [0.072s]
Building seed filter...  [0.012s]
Searching alignments...  [0.481s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.325s]
Building query seed array...  [0.284s]
Computing hash join...  [0.072s]
Building seed filter...  [0.012s]
Searching alignments...  [0.466s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.312s]
Building query seed array...  [0.207s]
Computing hash join...  [0.095s]
Building seed filter...  [0.013s]
Searching alignments...  [0.413s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.268s]
Building query seed array...  [0.215s]
Computing hash join...  [0.085s]
Building seed filter...  [0.012s]
Searching alignments...  [0.473s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.231s]
Building query seed array...  [0.185s]
Computing hash join...  [0.09s]
Building seed filter...  [0.011s]
Searching alignments...  [0.29s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.302s]
Building query seed array...  [0.121s]
Computing hash join...  [0.193s]
Building seed filter...  [0.012s]
Searching alignments...  [0.443s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.14s]
Building query seed array...  [0.268s]
Computing hash join...  [0.144s]
Building seed filter...  [0.013s]
Searching alignments...  [0.338s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.102s]
Building query seed array...  [0.211s]
Computing hash join...  [0.136s]
Building seed filter...  [0.016s]
Searching alignments...  [0.49s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.263s]
Building query seed array...  [0.161s]
Computing hash join...  [0.06s]
Building seed filter...  [0.016s]
Searching alignments...  [0.36s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.136s]
Building query seed array...  [0.311s]
Computing hash join...  [0.072s]
Building seed filter...  [0.016s]
Searching alignments...  [0.349s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.282s]
Building query seed array...  [0.296s]
Computing hash join...  [0.059s]
Building seed filter...  [0.013s]
Searching alignments...  [0.283s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.222s]
Building query seed array...  [0.135s]
Computing hash join...  [0.106s]
Building seed filter...  [0.009s]
Searching alignments...  [0.354s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.203s]
Building query seed array...  [0.227s]
Computing hash join...  [0.1s]
Building seed filter...  [0.016s]
Searching alignments...  [0.363s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.267s]
Building query seed array...  [0.177s]
Computing hash join...  [0.072s]
Building seed filter...  [0.011s]
Searching alignments...  [0.261s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.246s]
Building query seed array...  [0.276s]
Computing hash join...  [0.085s]
Building seed filter...  [0.011s]
Searching alignments...  [0.348s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.238s]
Building query seed array...  [0.204s]
Computing hash join...  [0.059s]
Building seed filter...  [0.011s]
Searching alignments...  [0.247s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.192s]
Building query seed array...  [0.22s]
Computing hash join...  [0.086s]
Building seed filter...  [0.012s]
Searching alignments...  [0.394s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.229s]
Building query seed array...  [0.267s]
Computing hash join...  [0.072s]
Building seed filter...  [0.014s]
Searching alignments...  [0.364s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.252s]
Building query seed array...  [0.347s]
Computing hash join...  [0.064s]
Building seed filter...  [0.014s]
Searching alignments...  [0.258s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.235s]
Building query seed array...  [0.23s]
Computing hash join...  [0.058s]
Building seed filter...  [0.013s]
Searching alignments...  [0.451s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.192s]
Building query seed array...  [0.16s]
Computing hash join...  [0.122s]
Building seed filter...  [0.015s]
Searching alignments...  [0.383s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.255s]
Building query seed array...  [0.281s]
Computing hash join...  [0.127s]
Building seed filter...  [0.014s]
Searching alignments...  [0.302s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.305s]
Building query seed array...  [0.282s]
Computing hash join...  [0.06s]
Building seed filter...  [0.012s]
Searching alignments...  [0.425s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.274s]
Building query seed array...  [0.202s]
Computing hash join...  [0.053s]
Building seed filter...  [0.014s]
Searching alignments...  [0.46s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.047s]
Computing alignments...  [8.4s]
Deallocating reference...  [0.001s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0s]
Deallocating queries...  [0.001s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.012s]
Closing the database file...  [0s]
Deallocating taxonomy...  [0s]
Total time = 79.966s
Reported 361375 pairwise alignments, 361375 HSPs.
26914 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
..........................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 288024x288024 matrix with 5903592 entries to stream <out/merged.mci>
[mclIO] wrote 288024 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 288024 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 288024x288024 matrix with 5903592 entries
[mcl] pid 12183
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  62.00  3.47 1.00/0.02/4.31 2.86 2.64 2.64   0
  2  ...................  75.34 16.28 0.86/0.08/4.58 3.92 0.84 2.22   5
  3  ...................  37.23  9.86 0.83/0.11/8.53 2.54 0.67 1.50   2
  4  ...................  22.85  3.59 0.82/0.12/10.13 1.57 0.71 1.07   0
  5  ...................  15.13  1.68 0.82/0.14/5.57 1.22 0.71 0.75   0
  6  ...................  13.82  0.90 0.82/0.10/3.58 1.09 0.72 0.55   0
  7  ...................   8.08  0.59 0.82/0.14/2.98 1.03 0.78 0.42   0
  8  ...................   6.18  0.43 0.84/0.20/2.67 1.01 0.81 0.34   0
  9  ...................   4.96  0.34 0.86/0.20/1.35 1.00 0.81 0.28   0
 10  ...................   5.10  0.27 0.89/0.20/1.16 1.00 0.81 0.23   0
 11  ...................   4.31  0.24 0.93/0.23/1.11 1.00 0.82 0.18   0
 12  ...................   4.66  0.21 0.95/0.20/1.18 1.00 0.83 0.15   0
 13  ...................   5.45  0.18 0.97/0.25/1.01 1.00 0.86 0.13   0
 14  ...................   4.84  0.16 0.98/0.19/1.00 1.00 0.90 0.12   0
 15  ...................   4.66  0.15 0.99/0.24/1.00 1.00 0.93 0.11   0
 16  ...................   3.57  0.15 0.99/0.22/1.00 1.00 0.96 0.10   0
 17  ...................   3.60  0.16 1.00/0.26/1.00 1.00 0.97 0.10   0
 18  ...................   4.74  0.13 1.00/0.29/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.15 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.05  0.14 1.00/0.46/1.00 1.00 1.00 0.10   0
 21  ...................   1.11  0.14 1.00/0.57/1.00 1.00 1.00 0.10   0
 22  ...................   0.42  0.14 1.00/0.70/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.13 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.15  0.14 1.00/0.85/1.00 1.00 1.00 0.10   0
 25  ...................   0.12  0.13 1.00/0.87/1.00 1.00 1.00 0.10   0
 26  ...................   0.09  0.15 1.00/0.88/1.00 1.00 1.00 0.10   0
 27  ...................   0.02  0.14 1.00/0.97/1.00 1.00 1.00 0.10   0
 28  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
 29  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 33952 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3539 entries from out/pcs_contigs.csv
Read 279958 entries (dropped 2260 singletons) from out/Cyber_profiles.csv
.......... 0.79% 10000/1268626.0
.......... 1.58% 20000/1268626.0
.......... 2.36% 30000/1268626.0
.......... 3.15% 40000/1268626.0
.......... 3.94% 50000/1268626.0
.......... 4.73% 60000/1268626.0
.......... 5.52% 70000/1268626.0
.......... 6.31% 80000/1268626.0
.......... 7.09% 90000/1268626.0
.......... 7.88% 100000/1268626.0
.......... 8.67% 110000/1268626.0
.......... 9.46% 120000/1268626.0
..........10.25% 130000/1268626.0
..........11.04% 140000/1268626.0
..........11.82% 150000/1268626.0
..........12.61% 160000/1268626.0
..........13.40% 170000/1268626.0
..........14.19% 180000/1268626.0
..........14.98% 190000/1268626.0
..........15.77% 200000/1268626.0
..........16.55% 210000/1268626.0
..........17.34% 220000/1268626.0
..........18.13% 230000/1268626.0
..........18.92% 240000/1268626.0
..........19.71% 250000/1268626.0
..........20.49% 260000/1268626.0
..........21.28% 270000/1268626.0
..........22.07% 280000/1268626.0
..........22.86% 290000/1268626.0
..........23.65% 300000/1268626.0
..........24.44% 310000/1268626.0
..........25.22% 320000/1268626.0
..........26.01% 330000/1268626.0
..........26.80% 340000/1268626.0
..........27.59% 350000/1268626.0
..........28.38% 360000/1268626.0
..........29.17% 370000/1268626.0
..........29.95% 380000/1268626.0
..........30.74% 390000/1268626.0
..........31.53% 400000/1268626.0
..........32.32% 410000/1268626.0
..........33.11% 420000/1268626.0
..........33.89% 430000/1268626.0
..........34.68% 440000/1268626.0
..........35.47% 450000/1268626.0
..........36.26% 460000/1268626.0
..........37.05% 470000/1268626.0
..........37.84% 480000/1268626.0
..........38.62% 490000/1268626.0
..........39.41% 500000/1268626.0
..........40.20% 510000/1268626.0
..........40.99% 520000/1268626.0
..........41.78% 530000/1268626.0
..........42.57% 540000/1268626.0
..........43.35% 550000/1268626.0
..........44.14% 560000/1268626.0
..........44.93% 570000/1268626.0
..........45.72% 580000/1268626.0
..........46.51% 590000/1268626.0
..........47.30% 600000/1268626.0
..........48.08% 610000/1268626.0
..........48.87% 620000/1268626.0
..........49.66% 630000/1268626.0
..........50.45% 640000/1268626.0
..........51.24% 650000/1268626.0
..........52.02% 660000/1268626.0
..........52.81% 670000/1268626.0
..........53.60% 680000/1268626.0
..........54.39% 690000/1268626.0
..........55.18% 700000/1268626.0
..........55.97% 710000/1268626.0
..........56.75% 720000/1268626.0
..........57.54% 730000/1268626.0
..........58.33% 740000/1268626.0
..........59.12% 750000/1268626.0
..........59.91% 760000/1268626.0
..........60.70% 770000/1268626.0
..........61.48% 780000/1268626.0
..........62.27% 790000/1268626.0
..........63.06% 800000/1268626.0
..........63.85% 810000/1268626.0
..........64.64% 820000/1268626.0
..........65.43% 830000/1268626.0
..........66.21% 840000/1268626.0
..........67.00% 850000/1268626.0
..........67.79% 860000/1268626.0
..........68.58% 870000/1268626.0
..........69.37% 880000/1268626.0
..........70.15% 890000/1268626.0
..........70.94% 900000/1268626.0
..........71.73% 910000/1268626.0
..........72.52% 920000/1268626.0
..........73.31% 930000/1268626.0
..........74.10% 940000/1268626.0
..........74.88% 950000/1268626.0
..........75.67% 960000/1268626.0
..........76.46% 970000/1268626.0
..........77.25% 980000/1268626.0
..........78.04% 990000/1268626.0
..........78.83% 1000000/1268626.0
..........79.61% 1010000/1268626.0
..........80.40% 1020000/1268626.0
..........81.19% 1030000/1268626.0
..........81.98% 1040000/1268626.0
..........82.77% 1050000/1268626.0
..........83.55% 1060000/1268626.0
..........84.34% 1070000/1268626.0
..........85.13% 1080000/1268626.0
..........85.92% 1090000/1268626.0
..........86.71% 1100000/1268626.0
..........87.50% 1110000/1268626.0
..........88.28% 1120000/1268626.0
..........89.07% 1130000/1268626.0
..........89.86% 1140000/1268626.0
..........90.65% 1150000/1268626.0
..........91.44% 1160000/1268626.0
..........92.23% 1170000/1268626.0
..........93.01% 1180000/1268626.0
..........93.80% 1190000/1268626.0
..........94.59% 1200000/1268626.0
..........95.38% 1210000/1268626.0
..........96.17% 1220000/1268626.0
..........96.96% 1230000/1268626.0
..........97.74% 1240000/1268626.0
..........98.53% 1250000/1268626.0
..........99.32% 1260000/1268626.0
.....Hypergeometric contig-similarity network:
       3539 contigs,
     170914 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (170914 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3465, 3465)
features: (3465, 512)
y: (3465,) (3465,)
mask: (3465,) (3465,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3464, 3464, 3464],
                       [ 466,  463,  450,  ...,   32,    7,    1]]),
       values=tensor([0.0826, 0.0515, 0.0003,  ..., 0.0508, 0.0133, 0.0036]),
       device='cuda:0', size=(3465, 512), nnz=83697, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    2,  ..., 1462, 1577, 3464],
                       [   0,    0,    0,  ..., 3464, 3464, 3464]]),
       values=tensor([0.0417, 0.0408, 0.0426,  ..., 0.1213, 0.1021, 0.2500]),
       device='cuda:0', size=(3465, 3465), nnz=174407, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 83697
0 19.986099243164062 0.10405053883314754
10 16.242725372314453 0.5722779635823114
20 13.445704460144043 0.8283166109253066
30 11.108909606933594 0.8918617614269788
40 9.344573020935059 0.916759568933482
50 7.9903998374938965 0.9431438127090301
60 6.85488748550415 0.9561501300631735
70 5.9891557693481445 0.9565217391304348
80 5.140407562255859 0.9591230026012635
90 4.456622123718262 0.9594946116685247
100 3.905688762664795 0.9624674842066147
110 3.3538155555725098 0.9665551839464883
120 2.968210458755493 0.9698996655518395
130 2.596696615219116 0.9669267930137495
140 2.2911839485168457 0.967670011148272
150 2.0565669536590576 0.9635823114083983
160 1.7851881980895996 0.9643255295429208
170 1.5854867696762085 0.9702712746191007
180 1.4262468814849854 0.9698996655518395
190 1.2822751998901367 0.9717577108881457
200 1.1590235233306885 0.9665551839464883
210 1.0348715782165527 0.967670011148272
220 0.9944034814834595 0.969156447417317
230 0.8698223233222961 0.9680416202155333
240 0.7784180045127869 0.9639539204756596
250 0.7365736961364746 0.966183574879227
260 0.7129002213478088 0.9713861018208845
270 0.643406093120575 0.967670011148272
280 0.5819137692451477 0.967670011148272
290 0.577465295791626 0.966183574879227
300 0.5103852152824402 0.9658119658119658
310 0.5088520050048828 0.9713861018208845
320 0.45562314987182617 0.969156447417317
330 0.44370681047439575 0.9721293199554069
340 0.4120386242866516 0.9721293199554069
350 0.4039652943611145 0.9739873652917131
360 0.34879475831985474 0.970642883686362
370 0.37481194734573364 0.966183574879227
380 0.3710682988166809 0.9698996655518395
390 0.3495262861251831 0.9654403567447045
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.045s]
Loading sequences...  [0.416s]
Masking sequences...  [0.447s]
Writing sequences...  [0.062s]
Hashing sequences...  [0.022s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0s]
Closing the database file...  [0.006s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.011s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.053s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.029s]
Opening the output file...  [0s]
Loading query sequences...  [0.656s]
Masking queries...  [0.883s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.751s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.116s]
Masking reference...  [0.461s]
Initializing temporary storage...  [0.029s]
Building reference histograms...  [1.352s]
Allocating buffers...  [0.001s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.154s]
Building query seed array...  [0.207s]
Computing hash join...  [0.062s]
Building seed filter...  [0.008s]
Searching alignments...  [0.361s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.168s]
Building query seed array...  [0.193s]
Computing hash join...  [0.065s]
Building seed filter...  [0.008s]
Searching alignments...  [0.34s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.204s]
Building query seed array...  [0.28s]
Computing hash join...  [0.076s]
Building seed filter...  [0.009s]
Searching alignments...  [0.295s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.122s]
Building query seed array...  [0.151s]
Computing hash join...  [0.062s]
Building seed filter...  [0.009s]
Searching alignments...  [0.33s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.126s]
Computing hash join...  [0.055s]
Building seed filter...  [0.008s]
Searching alignments...  [0.308s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.14s]
Building query seed array...  [0.145s]
Computing hash join...  [0.063s]
Building seed filter...  [0.008s]
Searching alignments...  [0.294s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.157s]
Building query seed array...  [0.155s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.316s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.116s]
Building query seed array...  [0.173s]
Computing hash join...  [0.063s]
Building seed filter...  [0.008s]
Searching alignments...  [0.26s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.152s]
Building query seed array...  [0.199s]
Computing hash join...  [0.075s]
Building seed filter...  [0.008s]
Searching alignments...  [0.293s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.194s]
Building query seed array...  [0.198s]
Computing hash join...  [0.067s]
Building seed filter...  [0.01s]
Searching alignments...  [0.325s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.147s]
Building query seed array...  [0.262s]
Computing hash join...  [0.064s]
Building seed filter...  [0.01s]
Searching alignments...  [0.322s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.116s]
Building query seed array...  [0.13s]
Computing hash join...  [0.06s]
Building seed filter...  [0.009s]
Searching alignments...  [0.319s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.112s]
Building query seed array...  [0.187s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.283s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.215s]
Computing hash join...  [0.076s]
Building seed filter...  [0.008s]
Searching alignments...  [0.281s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.146s]
Building query seed array...  [0.17s]
Computing hash join...  [0.057s]
Building seed filter...  [0.008s]
Searching alignments...  [0.31s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.112s]
Building query seed array...  [0.128s]
Computing hash join...  [0.074s]
Building seed filter...  [0.008s]
Searching alignments...  [0.281s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.139s]
Computing hash join...  [0.07s]
Building seed filter...  [0.01s]
Searching alignments...  [0.278s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.139s]
Building query seed array...  [0.22s]
Computing hash join...  [0.056s]
Building seed filter...  [0.009s]
Searching alignments...  [0.33s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.184s]
Computing hash join...  [0.056s]
Building seed filter...  [0.015s]
Searching alignments...  [0.294s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.175s]
Computing hash join...  [0.056s]
Building seed filter...  [0.008s]
Searching alignments...  [0.343s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.152s]
Building query seed array...  [0.124s]
Computing hash join...  [0.073s]
Building seed filter...  [0.008s]
Searching alignments...  [0.312s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.187s]
Building query seed array...  [0.155s]
Computing hash join...  [0.069s]
Building seed filter...  [0.009s]
Searching alignments...  [0.309s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.233s]
Building query seed array...  [0.243s]
Computing hash join...  [0.061s]
Building seed filter...  [0.008s]
Searching alignments...  [0.303s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.114s]
Building query seed array...  [0.184s]
Computing hash join...  [0.064s]
Building seed filter...  [0.009s]
Searching alignments...  [0.303s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.116s]
Building query seed array...  [0.17s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.263s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.188s]
Building query seed array...  [0.195s]
Computing hash join...  [0.073s]
Building seed filter...  [0.012s]
Searching alignments...  [0.261s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.204s]
Building query seed array...  [0.164s]
Computing hash join...  [0.055s]
Building seed filter...  [0.012s]
Searching alignments...  [0.29s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.115s]
Computing hash join...  [0.066s]
Building seed filter...  [0.008s]
Searching alignments...  [0.287s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.11s]
Building query seed array...  [0.123s]
Computing hash join...  [0.056s]
Building seed filter...  [0.009s]
Searching alignments...  [0.309s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.182s]
Building query seed array...  [0.186s]
Computing hash join...  [0.056s]
Building seed filter...  [0.009s]
Searching alignments...  [0.3s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.16s]
Computing hash join...  [0.075s]
Building seed filter...  [0.009s]
Searching alignments...  [0.296s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.102s]
Building query seed array...  [0.138s]
Computing hash join...  [0.079s]
Building seed filter...  [0.009s]
Searching alignments...  [0.298s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.127s]
Computing hash join...  [0.058s]
Building seed filter...  [0.009s]
Searching alignments...  [0.324s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.162s]
Building query seed array...  [0.232s]
Computing hash join...  [0.055s]
Building seed filter...  [0.009s]
Searching alignments...  [0.31s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.348s]
Computing hash join...  [0.056s]
Building seed filter...  [0.01s]
Searching alignments...  [0.367s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.189s]
Computing hash join...  [0.063s]
Building seed filter...  [0.01s]
Searching alignments...  [0.356s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.2s]
Building query seed array...  [0.168s]
Computing hash join...  [0.074s]
Building seed filter...  [0.01s]
Searching alignments...  [0.256s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.187s]
Building query seed array...  [0.21s]
Computing hash join...  [0.149s]
Building seed filter...  [0.009s]
Searching alignments...  [0.287s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.206s]
Building query seed array...  [0.371s]
Computing hash join...  [0.073s]
Building seed filter...  [0.012s]
Searching alignments...  [0.253s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.166s]
Computing hash join...  [0.063s]
Building seed filter...  [0.013s]
Searching alignments...  [0.32s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.355s]
Building query seed array...  [0.189s]
Computing hash join...  [0.064s]
Building seed filter...  [0.014s]
Searching alignments...  [0.359s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.182s]
Building query seed array...  [0.154s]
Computing hash join...  [0.066s]
Building seed filter...  [0.012s]
Searching alignments...  [0.276s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.213s]
Computing hash join...  [0.063s]
Building seed filter...  [0.009s]
Searching alignments...  [0.271s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.178s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.348s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.105s]
Building query seed array...  [0.135s]
Computing hash join...  [0.063s]
Building seed filter...  [0.01s]
Searching alignments...  [0.287s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.146s]
Building query seed array...  [0.205s]
Computing hash join...  [0.056s]
Building seed filter...  [0.014s]
Searching alignments...  [0.281s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.2s]
Building query seed array...  [0.15s]
Computing hash join...  [0.056s]
Building seed filter...  [0.012s]
Searching alignments...  [0.291s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.101s]
Building query seed array...  [0.15s]
Computing hash join...  [0.055s]
Building seed filter...  [0.012s]
Searching alignments...  [0.254s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.116s]
Building query seed array...  [0.163s]
Computing hash join...  [0.056s]
Building seed filter...  [0.01s]
Searching alignments...  [0.265s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.135s]
Building query seed array...  [0.155s]
Computing hash join...  [0.067s]
Building seed filter...  [0.009s]
Searching alignments...  [0.289s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.163s]
Building query seed array...  [0.151s]
Computing hash join...  [0.105s]
Building seed filter...  [0.01s]
Searching alignments...  [0.281s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.124s]
Computing hash join...  [0.063s]
Building seed filter...  [0.009s]
Searching alignments...  [0.281s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.128s]
Computing hash join...  [0.059s]
Building seed filter...  [0.01s]
Searching alignments...  [0.297s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.135s]
Building query seed array...  [0.155s]
Computing hash join...  [0.073s]
Building seed filter...  [0.01s]
Searching alignments...  [0.26s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.155s]
Building query seed array...  [0.227s]
Computing hash join...  [0.063s]
Building seed filter...  [0.01s]
Searching alignments...  [0.292s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.141s]
Computing hash join...  [0.065s]
Building seed filter...  [0.011s]
Searching alignments...  [0.292s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.119s]
Computing hash join...  [0.071s]
Building seed filter...  [0.01s]
Searching alignments...  [0.301s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.189s]
Building query seed array...  [0.15s]
Computing hash join...  [0.063s]
Building seed filter...  [0.014s]
Searching alignments...  [0.287s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.235s]
Computing hash join...  [0.063s]
Building seed filter...  [0.011s]
Searching alignments...  [0.265s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.141s]
Building query seed array...  [0.177s]
Computing hash join...  [0.074s]
Building seed filter...  [0.01s]
Searching alignments...  [0.306s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.148s]
Building query seed array...  [0.169s]
Computing hash join...  [0.071s]
Building seed filter...  [0.013s]
Searching alignments...  [0.298s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.186s]
Building query seed array...  [0.148s]
Computing hash join...  [0.063s]
Building seed filter...  [0.009s]
Searching alignments...  [0.254s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.159s]
Computing hash join...  [0.055s]
Building seed filter...  [0.01s]
Searching alignments...  [0.256s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.112s]
Computing hash join...  [0.058s]
Building seed filter...  [0.01s]
Searching alignments...  [0.252s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.055s]
Computing alignments...  [10.181s]
Deallocating reference...  [0s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0s]
Deallocating queries...  [0.001s]
Loading query sequences...  [0.001s]
Closing the input file...  [0s]
Closing the output file...  [0.012s]
Closing the database file...  [0s]
Deallocating taxonomy...  [0s]
Total time = 61.384s
Reported 411295 pairwise alignments, 411295 HSPs.
30161 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
.............................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 291674x291674 matrix with 6003432 entries to stream <out/merged.mci>
[mclIO] wrote 291674 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 291674 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 291674x291674 matrix with 6003432 entries
[mcl] pid 32709
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  62.80  3.62 1.00/0.02/4.71 2.92 2.69 2.69   0
  2  ...................  83.51 17.41 0.86/0.07/4.58 4.07 0.84 2.25   5
  3  ...................  42.79 10.58 0.83/0.11/7.69 2.60 0.67 1.51   2
  4  ...................  25.51  3.74 0.82/0.12/12.00 1.58 0.71 1.07   0
  5  ...................  18.66  1.72 0.82/0.10/4.98 1.23 0.70 0.75   0
  6  ...................  12.80  0.92 0.82/0.11/3.59 1.09 0.72 0.55   0
  7  ...................   8.67  0.59 0.82/0.14/2.77 1.04 0.78 0.42   0
  8  ...................   6.19  0.44 0.83/0.20/2.48 1.01 0.81 0.34   0
  9  ...................   6.86  0.36 0.86/0.20/1.16 1.01 0.81 0.28   0
 10  ...................   5.10  0.29 0.89/0.23/1.30 1.00 0.81 0.23   0
 11  ...................   4.31  0.23 0.93/0.23/1.11 1.00 0.81 0.18   0
 12  ...................   4.81  0.21 0.95/0.22/1.18 1.00 0.83 0.15   0
 13  ...................   4.96  0.18 0.97/0.22/1.01 1.00 0.86 0.13   0
 14  ...................   3.64  0.17 0.98/0.20/1.00 1.00 0.90 0.12   0
 15  ...................   5.14  0.16 0.99/0.24/1.00 1.00 0.92 0.11   0
 16  ...................   3.94  0.14 0.99/0.28/1.00 1.00 0.96 0.10   0
 17  ...................   3.57  0.15 1.00/0.29/1.00 1.00 0.97 0.10   0
 18  ...................   4.62  0.14 1.00/0.25/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.15 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.05  0.13 1.00/0.46/1.00 1.00 0.99 0.10   0
 21  ...................   1.11  0.14 1.00/0.57/1.00 1.00 1.00 0.10   0
 22  ...................   1.42  0.13 1.00/0.39/1.00 1.00 1.00 0.10   0
 23  ...................   0.97  0.15 1.00/0.70/1.00 1.00 1.00 0.10   0
 24  ...................   0.15  0.14 1.00/0.85/1.00 1.00 1.00 0.10   0
 25  ...................   0.12  0.14 1.00/0.86/1.00 1.00 1.00 0.10   0
 26  ...................   0.08  0.14 1.00/0.90/1.00 1.00 1.00 0.10   0
 27  ...................   0.01  0.14 1.00/0.98/1.00 1.00 1.00 0.10   0
 28  ...................   0.00  0.14 1.00/1.00/1.00 1.00 1.00 0.10   0
 29  ...................   0.00  0.13 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 34515 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3612 entries from out/pcs_contigs.csv
Read 283302 entries (dropped 2254 singletons) from out/Cyber_profiles.csv
.......... 0.73% 10000/1374793.0
.......... 1.45% 20000/1374793.0
.......... 2.18% 30000/1374793.0
.......... 2.91% 40000/1374793.0
.......... 3.64% 50000/1374793.0
.......... 4.36% 60000/1374793.0
.......... 5.09% 70000/1374793.0
.......... 5.82% 80000/1374793.0
.......... 6.55% 90000/1374793.0
.......... 7.27% 100000/1374793.0
.......... 8.00% 110000/1374793.0
.......... 8.73% 120000/1374793.0
.......... 9.46% 130000/1374793.0
..........10.18% 140000/1374793.0
..........10.91% 150000/1374793.0
..........11.64% 160000/1374793.0
..........12.37% 170000/1374793.0
..........13.09% 180000/1374793.0
..........13.82% 190000/1374793.0
..........14.55% 200000/1374793.0
..........15.28% 210000/1374793.0
..........16.00% 220000/1374793.0
..........16.73% 230000/1374793.0
..........17.46% 240000/1374793.0
..........18.18% 250000/1374793.0
..........18.91% 260000/1374793.0
..........19.64% 270000/1374793.0
..........20.37% 280000/1374793.0
..........21.09% 290000/1374793.0
..........21.82% 300000/1374793.0
..........22.55% 310000/1374793.0
..........23.28% 320000/1374793.0
..........24.00% 330000/1374793.0
..........24.73% 340000/1374793.0
..........25.46% 350000/1374793.0
..........26.19% 360000/1374793.0
..........26.91% 370000/1374793.0
..........27.64% 380000/1374793.0
..........28.37% 390000/1374793.0
..........29.10% 400000/1374793.0
..........29.82% 410000/1374793.0
..........30.55% 420000/1374793.0
..........31.28% 430000/1374793.0
..........32.00% 440000/1374793.0
..........32.73% 450000/1374793.0
..........33.46% 460000/1374793.0
..........34.19% 470000/1374793.0
..........34.91% 480000/1374793.0
..........35.64% 490000/1374793.0
..........36.37% 500000/1374793.0
..........37.10% 510000/1374793.0
..........37.82% 520000/1374793.0
..........38.55% 530000/1374793.0
..........39.28% 540000/1374793.0
..........40.01% 550000/1374793.0
..........40.73% 560000/1374793.0
..........41.46% 570000/1374793.0
..........42.19% 580000/1374793.0
..........42.92% 590000/1374793.0
..........43.64% 600000/1374793.0
..........44.37% 610000/1374793.0
..........45.10% 620000/1374793.0
..........45.83% 630000/1374793.0
..........46.55% 640000/1374793.0
..........47.28% 650000/1374793.0
..........48.01% 660000/1374793.0
..........48.73% 670000/1374793.0
..........49.46% 680000/1374793.0
..........50.19% 690000/1374793.0
..........50.92% 700000/1374793.0
..........51.64% 710000/1374793.0
..........52.37% 720000/1374793.0
..........53.10% 730000/1374793.0
..........53.83% 740000/1374793.0
..........54.55% 750000/1374793.0
..........55.28% 760000/1374793.0
..........56.01% 770000/1374793.0
..........56.74% 780000/1374793.0
..........57.46% 790000/1374793.0
..........58.19% 800000/1374793.0
..........58.92% 810000/1374793.0
..........59.65% 820000/1374793.0
..........60.37% 830000/1374793.0
..........61.10% 840000/1374793.0
..........61.83% 850000/1374793.0
..........62.55% 860000/1374793.0
..........63.28% 870000/1374793.0
..........64.01% 880000/1374793.0
..........64.74% 890000/1374793.0
..........65.46% 900000/1374793.0
..........66.19% 910000/1374793.0
..........66.92% 920000/1374793.0
..........67.65% 930000/1374793.0
..........68.37% 940000/1374793.0
..........69.10% 950000/1374793.0
..........69.83% 960000/1374793.0
..........70.56% 970000/1374793.0
..........71.28% 980000/1374793.0
..........72.01% 990000/1374793.0
..........72.74% 1000000/1374793.0
..........73.47% 1010000/1374793.0
..........74.19% 1020000/1374793.0
..........74.92% 1030000/1374793.0
..........75.65% 1040000/1374793.0
..........76.38% 1050000/1374793.0
..........77.10% 1060000/1374793.0
..........77.83% 1070000/1374793.0
..........78.56% 1080000/1374793.0
..........79.28% 1090000/1374793.0
..........80.01% 1100000/1374793.0
..........80.74% 1110000/1374793.0
..........81.47% 1120000/1374793.0
..........82.19% 1130000/1374793.0
..........82.92% 1140000/1374793.0
..........83.65% 1150000/1374793.0
..........84.38% 1160000/1374793.0
..........85.10% 1170000/1374793.0
..........85.83% 1180000/1374793.0
..........86.56% 1190000/1374793.0
..........87.29% 1200000/1374793.0
..........88.01% 1210000/1374793.0
..........88.74% 1220000/1374793.0
..........89.47% 1230000/1374793.0
..........90.20% 1240000/1374793.0
..........90.92% 1250000/1374793.0
..........91.65% 1260000/1374793.0
..........92.38% 1270000/1374793.0
..........93.10% 1280000/1374793.0
..........93.83% 1290000/1374793.0
..........94.56% 1300000/1374793.0
..........95.29% 1310000/1374793.0
..........96.01% 1320000/1374793.0
..........96.74% 1330000/1374793.0
..........97.47% 1340000/1374793.0
..........98.20% 1350000/1374793.0
..........98.92% 1360000/1374793.0
..........99.65% 1370000/1374793.0
.Hypergeometric contig-similarity network:
       3612 contigs,
     163574 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (163574 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3576, 3576)
features: (3576, 512)
y: (3576,) (3576,)
mask: (3576,) (3576,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3575, 3575, 3575],
                       [ 466,  463,  447,  ...,   52,   34,   32]]),
       values=tensor([0.0078, 0.0731, 0.0697,  ..., 0.0446, 0.0375, 0.0839]),
       device='cuda:0', size=(3576, 512), nnz=87236, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    2,  ..., 1418, 2485, 3575],
                       [   0,    0,    0,  ..., 3575, 3575, 3575]]),
       values=tensor([0.0588, 0.0506, 0.0542,  ..., 0.1118, 0.0497, 0.2000]),
       device='cuda:0', size=(3576, 3576), nnz=167222, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 87236
0 20.016616821289062 0.10326894502228826
10 16.224605560302734 0.5880386329866271
20 13.438791275024414 0.8183506686478454
30 11.128621101379395 0.8900445765230312
40 9.41274642944336 0.9045319465081724
50 7.993298530578613 0.9338781575037147
60 6.90817403793335 0.937592867756315
70 5.934419631958008 0.9468796433878157
80 5.15736198425293 0.9479940564635958
90 4.493810176849365 0.9531946508172363
100 3.856579542160034 0.9587667161961367
110 3.383063793182373 0.9591381872213968
120 2.960418224334717 0.9595096582466568
130 2.6086902618408203 0.9658246656760773
140 2.374840497970581 0.9676820208023774
150 2.0237064361572266 0.9665676077265973
160 1.8245861530303955 0.9680534918276374
170 1.6122735738754272 0.9647102526002972
180 1.499350905418396 0.9591381872213968
190 1.2548364400863647 0.9635958395245171
200 1.1574933528900146 0.9658246656760773
210 1.1385202407836914 0.9658246656760773
220 0.939751386642456 0.9691679049034175
230 0.873009443283081 0.9695393759286776
240 0.807168185710907 0.9673105497771174
250 0.7479446530342102 0.9691679049034175
260 0.6748060584068298 0.9673105497771174
270 0.641198992729187 0.9658246656760773
280 0.611779510974884 0.9684249628528975
290 0.5287061929702759 0.9680534918276374
300 0.5589344501495361 0.9687964338781575
310 0.49667084217071533 0.9728826151560178
320 0.46568453311920166 0.9691679049034175
330 0.4430370628833771 0.9687964338781575
340 0.432326078414917 0.9695393759286776
350 0.3984380066394806 0.9739970282317979
360 0.3997303247451782 0.9739970282317979
370 0.3842463493347168 0.9713967310549777
380 0.3730481266975403 0.9684249628528975
390 0.3714334964752197 0.962852897473997
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Database input file: database/Caudovirales_protein.fasta
Opening the database file...  [0.045s]
Loading sequences...  [0.446s]
Masking sequences...  [0.445s]
Writing sequences...  [0.063s]
Hashing sequences...  [0.023s]
Loading sequences...  [0s]
Writing trailer...  [0.002s]
Closing the input file...  [0.001s]
Closing the database file...  [0.006s]
Database hash = e87d3f91501e3951d74e013b0aae738c
Processed 285215 sequences, 58680927 letters.
Total time = 1.042s
diamond v2.0.6.144 (C) Max Planck Society for the Advancement of Science
Documentation, support and updates available at http://www.diamondsearch.org

#CPU threads: 8
Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)
Temporary directory: out
Opening the database...  [0.053s]
#Target sequences to report alignments for: 25
Reference = database/database.dmnd
Sequences = 285215
Letters = 58680927
Block size = 2000000000
Opening the input file...  [0.027s]
Opening the output file...  [0.001s]
Loading query sequences...  [0.454s]
Masking queries...  [0.587s]
Building query seed set...  [0.003s]
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39mAlgorithm: Double-indexed
Building query histograms...  [1.138s]
Allocating buffers...  [0.001s]
Loading reference sequences...  [0.13s]
Masking reference...  [0.468s]
Initializing temporary storage...  [0.028s]
Building reference histograms...  [1.43s]
Allocating buffers...  [0s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 1/4.
Building reference seed array...  [0.158s]
Building query seed array...  [0.103s]
Computing hash join...  [0.073s]
Building seed filter...  [0.006s]
Searching alignments...  [0.231s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 2/4.
Building reference seed array...  [0.197s]
Building query seed array...  [0.164s]
Computing hash join...  [0.071s]
Building seed filter...  [0.007s]
Searching alignments...  [0.221s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 3/4.
Building reference seed array...  [0.21s]
Building query seed array...  [0.184s]
Computing hash join...  [0.055s]
Building seed filter...  [0.007s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 1/16, index chunk 4/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.115s]
Computing hash join...  [0.067s]
Building seed filter...  [0.007s]
Searching alignments...  [0.226s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 1/4.
Building reference seed array...  [0.117s]
Building query seed array...  [0.096s]
Computing hash join...  [0.058s]
Building seed filter...  [0.008s]
Searching alignments...  [0.202s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 2/4.
Building reference seed array...  [0.192s]
Building query seed array...  [0.147s]
Computing hash join...  [0.051s]
Building seed filter...  [0.008s]
Searching alignments...  [0.194s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 3/4.
Building reference seed array...  [0.209s]
Building query seed array...  [0.164s]
Computing hash join...  [0.067s]
Building seed filter...  [0.009s]
Searching alignments...  [0.195s]
Processing query block 1, reference block 1/1, shape 2/16, index chunk 4/4.
Building reference seed array...  [0.117s]
Building query seed array...  [0.112s]
Computing hash join...  [0.052s]
Building seed filter...  [0.007s]
Searching alignments...  [0.184s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 1/4.
Building reference seed array...  [0.104s]
Building query seed array...  [0.098s]
Computing hash join...  [0.054s]
Building seed filter...  [0.008s]
Searching alignments...  [0.225s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 2/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.111s]
Computing hash join...  [0.075s]
Building seed filter...  [0.008s]
Searching alignments...  [0.193s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 3/4.
Building reference seed array...  [0.174s]
Building query seed array...  [0.119s]
Computing hash join...  [0.06s]
Building seed filter...  [0.01s]
Searching alignments...  [0.222s]
Processing query block 1, reference block 1/1, shape 3/16, index chunk 4/4.
Building reference seed array...  [0.15s]
Building query seed array...  [0.088s]
Computing hash join...  [0.055s]
Building seed filter...  [0.007s]
Searching alignments...  [0.225s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 1/4.
Building reference seed array...  [0.129s]
Building query seed array...  [0.121s]
Computing hash join...  [0.071s]
Building seed filter...  [0.007s]
Searching alignments...  [0.211s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 2/4.
Building reference seed array...  [0.138s]
Building query seed array...  [0.158s]
Computing hash join...  [0.061s]
Building seed filter...  [0.007s]
Searching alignments...  [0.189s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 3/4.
Building reference seed array...  [0.201s]
Building query seed array...  [0.172s]
Computing hash join...  [0.054s]
Building seed filter...  [0.008s]
Searching alignments...  [0.24s]
Processing query block 1, reference block 1/1, shape 4/16, index chunk 4/4.
Building reference seed array...  [0.112s]
Building query seed array...  [0.085s]
Computing hash join...  [0.059s]
Building seed filter...  [0.007s]
Searching alignments...  [0.238s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 1/4.
Building reference seed array...  [0.161s]
Building query seed array...  [0.115s]
Computing hash join...  [0.068s]
Building seed filter...  [0.008s]
Searching alignments...  [0.23s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 2/4.
Building reference seed array...  [0.142s]
Building query seed array...  [0.147s]
Computing hash join...  [0.051s]
Building seed filter...  [0.007s]
Searching alignments...  [0.202s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 3/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.152s]
Computing hash join...  [0.054s]
Building seed filter...  [0.006s]
Searching alignments...  [0.197s]
Processing query block 1, reference block 1/1, shape 5/16, index chunk 4/4.
Building reference seed array...  [0.119s]
Building query seed array...  [0.08s]
Computing hash join...  [0.054s]
Building seed filter...  [0.008s]
Searching alignments...  [0.2s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 1/4.
Building reference seed array...  [0.152s]
Building query seed array...  [0.119s]
Computing hash join...  [0.059s]
Building seed filter...  [0.008s]
Searching alignments...  [0.217s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 2/4.
Building reference seed array...  [0.142s]
Building query seed array...  [0.152s]
Computing hash join...  [0.052s]
Building seed filter...  [0.007s]
Searching alignments...  [0.191s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 3/4.
Building reference seed array...  [0.198s]
Building query seed array...  [0.139s]
Computing hash join...  [0.069s]
Building seed filter...  [0.007s]
Searching alignments...  [0.202s]
Processing query block 1, reference block 1/1, shape 6/16, index chunk 4/4.
Building reference seed array...  [0.115s]
Building query seed array...  [0.119s]
Computing hash join...  [0.06s]
Building seed filter...  [0.007s]
Searching alignments...  [0.212s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 1/4.
Building reference seed array...  [0.11s]
Building query seed array...  [0.1s]
Computing hash join...  [0.051s]
Building seed filter...  [0.008s]
Searching alignments...  [0.192s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 2/4.
Building reference seed array...  [0.187s]
Building query seed array...  [0.099s]
Computing hash join...  [0.068s]
Building seed filter...  [0.007s]
Searching alignments...  [0.173s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 3/4.
Building reference seed array...  [0.206s]
Building query seed array...  [0.118s]
Computing hash join...  [0.057s]
Building seed filter...  [0.007s]
Searching alignments...  [0.197s]
Processing query block 1, reference block 1/1, shape 7/16, index chunk 4/4.
Building reference seed array...  [0.119s]
Building query seed array...  [0.077s]
Computing hash join...  [0.05s]
Building seed filter...  [0.011s]
Searching alignments...  [0.189s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 1/4.
Building reference seed array...  [0.12s]
Building query seed array...  [0.081s]
Computing hash join...  [0.066s]
Building seed filter...  [0.007s]
Searching alignments...  [0.2s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 2/4.
Building reference seed array...  [0.128s]
Building query seed array...  [0.146s]
Computing hash join...  [0.054s]
Building seed filter...  [0.007s]
Searching alignments...  [0.189s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 3/4.
Building reference seed array...  [0.205s]
Building query seed array...  [0.126s]
Computing hash join...  [0.052s]
Building seed filter...  [0.007s]
Searching alignments...  [0.202s]
Processing query block 1, reference block 1/1, shape 8/16, index chunk 4/4.
Building reference seed array...  [0.101s]
Building query seed array...  [0.101s]
Computing hash join...  [0.051s]
Building seed filter...  [0.009s]
Searching alignments...  [0.205s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 1/4.
Building reference seed array...  [0.171s]
Building query seed array...  [0.086s]
Computing hash join...  [0.053s]
Building seed filter...  [0.011s]
Searching alignments...  [0.212s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 2/4.
Building reference seed array...  [0.181s]
Building query seed array...  [0.153s]
Computing hash join...  [0.062s]
Building seed filter...  [0.007s]
Searching alignments...  [0.208s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 3/4.
Building reference seed array...  [0.201s]
Building query seed array...  [0.129s]
Computing hash join...  [0.054s]
Building seed filter...  [0.007s]
Searching alignments...  [0.211s]
Processing query block 1, reference block 1/1, shape 9/16, index chunk 4/4.
Building reference seed array...  [0.125s]
Building query seed array...  [0.085s]
Computing hash join...  [0.059s]
Building seed filter...  [0.009s]
Searching alignments...  [0.223s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 1/4.
Building reference seed array...  [0.102s]
Building query seed array...  [0.109s]
Computing hash join...  [0.054s]
Building seed filter...  [0.007s]
Searching alignments...  [0.188s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 2/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.11s]
Computing hash join...  [0.056s]
Building seed filter...  [0.007s]
Searching alignments...  [0.187s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 3/4.
Building reference seed array...  [0.146s]
Building query seed array...  [0.101s]
Computing hash join...  [0.049s]
Building seed filter...  [0.007s]
Searching alignments...  [0.168s]
Processing query block 1, reference block 1/1, shape 10/16, index chunk 4/4.
Building reference seed array...  [0.146s]
Building query seed array...  [0.076s]
Computing hash join...  [0.066s]
Building seed filter...  [0.008s]
Searching alignments...  [0.182s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 1/4.
Building reference seed array...  [0.143s]
Building query seed array...  [0.121s]
Computing hash join...  [0.053s]
Building seed filter...  [0.008s]
Searching alignments...  [0.193s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 2/4.
Building reference seed array...  [0.142s]
Building query seed array...  [0.117s]
Computing hash join...  [0.06s]
Building seed filter...  [0.008s]
Searching alignments...  [0.182s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 3/4.
Building reference seed array...  [0.137s]
Building query seed array...  [0.141s]
Computing hash join...  [0.065s]
Building seed filter...  [0.008s]
Searching alignments...  [0.195s]
Processing query block 1, reference block 1/1, shape 11/16, index chunk 4/4.
Building reference seed array...  [0.11s]
Building query seed array...  [0.105s]
Computing hash join...  [0.053s]
Building seed filter...  [0.011s]
Searching alignments...  [0.194s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 1/4.
Building reference seed array...  [0.119s]
Building query seed array...  [0.076s]
Computing hash join...  [0.054s]
Building seed filter...  [0.007s]
Searching alignments...  [0.189s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 2/4.
Building reference seed array...  [0.143s]
Building query seed array...  [0.104s]
Computing hash join...  [0.049s]
Building seed filter...  [0.008s]
Searching alignments...  [0.188s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 3/4.
Building reference seed array...  [0.136s]
Building query seed array...  [0.138s]
Computing hash join...  [0.055s]
Building seed filter...  [0.021s]
Searching alignments...  [0.194s]
Processing query block 1, reference block 1/1, shape 12/16, index chunk 4/4.
Building reference seed array...  [0.101s]
Building query seed array...  [0.076s]
Computing hash join...  [0.059s]
Building seed filter...  [0.008s]
Searching alignments...  [0.186s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 1/4.
Building reference seed array...  [0.147s]
Building query seed array...  [0.076s]
Computing hash join...  [0.096s]
Building seed filter...  [0.008s]
Searching alignments...  [0.189s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 2/4.
Building reference seed array...  [0.133s]
Building query seed array...  [0.139s]
Computing hash join...  [0.054s]
Building seed filter...  [0.008s]
Searching alignments...  [0.169s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 3/4.
Building reference seed array...  [0.136s]
Building query seed array...  [0.116s]
Computing hash join...  [0.06s]
Building seed filter...  [0.008s]
Searching alignments...  [0.197s]
Processing query block 1, reference block 1/1, shape 13/16, index chunk 4/4.
Building reference seed array...  [0.158s]
Building query seed array...  [0.075s]
Computing hash join...  [0.059s]
Building seed filter...  [0.01s]
Searching alignments...  [0.209s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 1/4.
Building reference seed array...  [0.103s]
Building query seed array...  [0.096s]
Computing hash join...  [0.056s]
Building seed filter...  [0.015s]
Searching alignments...  [0.203s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 2/4.
Building reference seed array...  [0.19s]
Building query seed array...  [0.164s]
Computing hash join...  [0.058s]
Building seed filter...  [0.008s]
Searching alignments...  [0.193s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 3/4.
Building reference seed array...  [0.2s]
Building query seed array...  [0.151s]
Computing hash join...  [0.058s]
Building seed filter...  [0.008s]
Searching alignments...  [0.174s]
Processing query block 1, reference block 1/1, shape 14/16, index chunk 4/4.
Building reference seed array...  [0.109s]
Building query seed array...  [0.112s]
Computing hash join...  [0.056s]
Building seed filter...  [0.009s]
Searching alignments...  [0.194s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 1/4.
Building reference seed array...  [0.151s]
Building query seed array...  [0.081s]
Computing hash join...  [0.062s]
Building seed filter...  [0.009s]
Searching alignments...  [0.202s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 2/4.
Building reference seed array...  [0.143s]
Building query seed array...  [0.143s]
Computing hash join...  [0.057s]
Building seed filter...  [0.011s]
Searching alignments...  [0.207s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 3/4.
Building reference seed array...  [0.213s]
Building query seed array...  [0.107s]
Computing hash join...  [0.065s]
Building seed filter...  [0.012s]
Searching alignments...  [0.175s]
Processing query block 1, reference block 1/1, shape 15/16, index chunk 4/4.
Building reference seed array...  [0.149s]
Building query seed array...  [0.08s]
Computing hash join...  [0.056s]
Building seed filter...  [0.009s]
Searching alignments...  [0.211s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 1/4.
Building reference seed array...  [0.11s]
Building query seed array...  [0.11s]
Computing hash join...  [0.064s]
Building seed filter...  [0.009s]
Searching alignments...  [0.18s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 2/4.
Building reference seed array...  [0.191s]
Building query seed array...  [0.108s]
Computing hash join...  [0.066s]
Building seed filter...  [0.008s]
Searching alignments...  [0.174s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 3/4.
Building reference seed array...  [0.197s]
Building query seed array...  [0.127s]
Computing hash join...  [0.065s]
Building seed filter...  [0.008s]
Searching alignments...  [0.167s]
Processing query block 1, reference block 1/1, shape 16/16, index chunk 4/4.
Building reference seed array...  [0.109s]
Building query seed array...  [0.111s]
Computing hash join...  [0.049s]
Building seed filter...  [0.008s]
Searching alignments...  [0.195s]
Deallocating buffers...  [0.001s]
Clearing query masking...  [0.039s]
Computing alignments...  [6.469s]
Deallocating reference...  [0.001s]
Loading reference sequences...  [0s]
Deallocating buffers...  [0s]
Deallocating queries...  [0.001s]
Loading query sequences...  [0.001s]
Closing the input file...  [0.001s]
Closing the output file...  [0.009s]
Closing the database file...  [0.001s]
Deallocating taxonomy...  [0s]
Total time = 45.471s
Reported 218297 pairwise alignments, 218297 HSPs.
17006 queries aligned.
[1;33mThe host system is detected to have 269 GB of RAM. It is recommended to use this parameter for better performance: -c1
[0;39m.................................................. 1M
.................................................. 2M
.................................................. 3M
.................................................. 4M
...................
[mclIO] writing <out/merged.mci>
.......................................
[mclIO] wrote native interchange 278054x278054 matrix with 5617436 entries to stream <out/merged.mci>
[mclIO] wrote 278054 tab entries to stream <out/merged_mcxload.tab>
[mcxload] tab has 278054 entries
[mclIO] reading <out/merged.mci>
.......................................
[mclIO] read native interchange 278054x278054 matrix with 5617436 entries
[mcl] pid 17338
 ite -------------------  chaos  time hom(avg,lo,hi) m-ie m-ex i-ex fmv
  1  ...................  55.34  2.76 0.98/0.02/3.72 2.60 2.43 2.43   0
  2  ...................  69.80 12.42 0.87/0.08/4.57 3.81 0.86 2.10   3
  3  ...................  44.44  7.34 0.83/0.11/5.77 2.43 0.71 1.50   1
  4  ...................  31.18  2.96 0.83/0.12/11.57 1.54 0.72 1.09   0
  5  ...................  19.81  1.44 0.82/0.09/6.72 1.22 0.71 0.77   0
  6  ...................  11.75  0.77 0.82/0.12/3.47 1.09 0.73 0.56   0
  7  ...................  10.35  0.50 0.82/0.14/4.05 1.03 0.78 0.44   0
  8  ...................   5.20  0.37 0.83/0.20/1.81 1.01 0.81 0.35   0
  9  ...................   4.69  0.30 0.86/0.20/1.28 1.00 0.81 0.29   0
 10  ...................   5.10  0.25 0.89/0.27/1.31 1.00 0.81 0.23   0
 11  ...................   4.56  0.21 0.93/0.23/1.36 1.00 0.82 0.19   0
 12  ...................   4.54  0.18 0.95/0.20/1.18 1.00 0.83 0.16   0
 13  ...................   4.96  0.16 0.97/0.22/1.01 1.00 0.86 0.14   0
 14  ...................   4.07  0.14 0.98/0.19/1.00 1.00 0.90 0.12   0
 15  ...................   4.65  0.14 0.99/0.34/1.00 1.00 0.92 0.11   0
 16  ...................   3.13  0.12 0.99/0.28/1.00 1.00 0.95 0.11   0
 17  ...................   3.61  0.13 1.00/0.28/1.00 1.00 0.97 0.10   0
 18  ...................   4.70  0.12 1.00/0.27/1.00 1.00 0.98 0.10   0
 19  ...................   2.41  0.13 1.00/0.44/1.00 1.00 0.99 0.10   0
 20  ...................   1.05  0.12 1.00/0.46/1.00 1.00 0.99 0.10   0
 21  ...................   1.11  0.12 1.00/0.57/1.00 1.00 1.00 0.10   0
 22  ...................   0.49  0.13 1.00/0.64/1.00 1.00 1.00 0.10   0
 23  ...................   0.25  0.12 1.00/0.76/1.00 1.00 1.00 0.10   0
 24  ...................   0.16  0.12 1.00/0.84/1.00 1.00 1.00 0.10   0
 25  ...................   0.03  0.14 1.00/0.96/1.00 1.00 1.00 0.10   0
 26  ...................   0.00  0.13 1.00/1.00/1.00 1.00 1.00 0.10   0
 27  ...................   0.00  0.12 1.00/1.00/1.00 1.00 1.00 0.10   0
[mcl] jury pruning marks: <99,98,99>, out of 100
[mcl] jury pruning synopsis: <98.8 or marvelous> (cf -scheme, -do log)
[mcl] output is in out/merged_mcl20.clusters
[mcl] 33876 clusters found
[mcl] output is in out/merged_mcl20.clusters

Please cite:
    Stijn van Dongen, Graph Clustering by Flow Simulation.  PhD thesis,
    University of Utrecht, May 2000.
       (  http://www.library.uu.nl/digiarchief/dip/diss/1895620/full.pdf
       or  http://micans.org/mcl/lit/svdthesis.pdf.gz)
OR
    Stijn van Dongen, A cluster algorithm for graphs. Technical
    Report INS-R0010, National Research Institute for Mathematics
    and Computer Science in the Netherlands, Amsterdam, May 2000.
       (  http://www.cwi.nl/ftp/CWIreports/INS/INS-R0010.ps.Z
       or  http://micans.org/mcl/lit/INS-R0010.ps.Z)

/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/Bio/Seq.py:2576: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.
  BiopythonWarning)
run_KnowledgeGraph.py:437: RuntimeWarning: divide by zero encountered in log10
  sig = min(max_sig, np.nan_to_num(-np.log10(pval) - logT))
folder out/ exist... cleaning dictionary
Dictionary cleaned
folder single_contig/ exist... cleaning dictionary
Dictionary cleaned
folder all_proteins/ exist... cleaning dictionary
Dictionary cleaned


---------------------------------Diamond BLASTp---------------------------------
Creating Diamond database and running Diamond...
Creating Diamond database...
Running Diamond...


-------------------------------Protein clustering-------------------------------
Loading proteins...
Running MCL...
Building the cluster and profiles (this may take some time...)
Using MCL to generate PCs.
Saving files
Read 3439 entries from out/pcs_contigs.csv
Read 270431 entries (dropped 2288 singletons) from out/Cyber_profiles.csv
.......... 1.00% 10000/998936.0
.......... 2.00% 20000/998936.0
.......... 3.00% 30000/998936.0
.......... 4.00% 40000/998936.0
.......... 5.01% 50000/998936.0
.......... 6.01% 60000/998936.0
.......... 7.01% 70000/998936.0
.......... 8.01% 80000/998936.0
.......... 9.01% 90000/998936.0
..........10.01% 100000/998936.0
..........11.01% 110000/998936.0
..........12.01% 120000/998936.0
..........13.01% 130000/998936.0
..........14.01% 140000/998936.0
..........15.02% 150000/998936.0
..........16.02% 160000/998936.0
..........17.02% 170000/998936.0
..........18.02% 180000/998936.0
..........19.02% 190000/998936.0
..........20.02% 200000/998936.0
..........21.02% 210000/998936.0
..........22.02% 220000/998936.0
..........23.02% 230000/998936.0
..........24.03% 240000/998936.0
..........25.03% 250000/998936.0
..........26.03% 260000/998936.0
..........27.03% 270000/998936.0
..........28.03% 280000/998936.0
..........29.03% 290000/998936.0
..........30.03% 300000/998936.0
..........31.03% 310000/998936.0
..........32.03% 320000/998936.0
..........33.04% 330000/998936.0
..........34.04% 340000/998936.0
..........35.04% 350000/998936.0
..........36.04% 360000/998936.0
..........37.04% 370000/998936.0
..........38.04% 380000/998936.0
..........39.04% 390000/998936.0
..........40.04% 400000/998936.0
..........41.04% 410000/998936.0
..........42.04% 420000/998936.0
..........43.05% 430000/998936.0
..........44.05% 440000/998936.0
..........45.05% 450000/998936.0
..........46.05% 460000/998936.0
..........47.05% 470000/998936.0
..........48.05% 480000/998936.0
..........49.05% 490000/998936.0
..........50.05% 500000/998936.0
..........51.05% 510000/998936.0
..........52.06% 520000/998936.0
..........53.06% 530000/998936.0
..........54.06% 540000/998936.0
..........55.06% 550000/998936.0
..........56.06% 560000/998936.0
..........57.06% 570000/998936.0
..........58.06% 580000/998936.0
..........59.06% 590000/998936.0
..........60.06% 600000/998936.0
..........61.06% 610000/998936.0
..........62.07% 620000/998936.0
..........63.07% 630000/998936.0
..........64.07% 640000/998936.0
..........65.07% 650000/998936.0
..........66.07% 660000/998936.0
..........67.07% 670000/998936.0
..........68.07% 680000/998936.0
..........69.07% 690000/998936.0
..........70.07% 700000/998936.0
..........71.08% 710000/998936.0
..........72.08% 720000/998936.0
..........73.08% 730000/998936.0
..........74.08% 740000/998936.0
..........75.08% 750000/998936.0
..........76.08% 760000/998936.0
..........77.08% 770000/998936.0
..........78.08% 780000/998936.0
..........79.08% 790000/998936.0
..........80.09% 800000/998936.0
..........81.09% 810000/998936.0
..........82.09% 820000/998936.0
..........83.09% 830000/998936.0
..........84.09% 840000/998936.0
..........85.09% 850000/998936.0
..........86.09% 860000/998936.0
..........87.09% 870000/998936.0
..........88.09% 880000/998936.0
..........89.09% 890000/998936.0
..........90.10% 900000/998936.0
..........91.10% 910000/998936.0
..........92.10% 920000/998936.0
..........93.10% 930000/998936.0
..........94.10% 940000/998936.0
..........95.10% 950000/998936.0
..........96.10% 960000/998936.0
..........97.10% 970000/998936.0
..........98.10% 980000/998936.0
..........99.11% 990000/998936.0
.....Hypergeometric contig-similarity network:
       3439 contigs,
     135342 edges (min:1.0max: 3e+02, threshold was 1)
Saving network in file out/network.ntw (135342 lines).


------------------------------Calculating E-edges-------------------------------


------------------------------Calculating P-edges-------------------------------


---------------------------Generating Knowledge graph---------------------------
/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:52: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  i = i[:, dropout_mask]
/mnt/ufs18/home-153/yannisun/kenneth/PhaGCN/utils.py:53: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)
  v = v[dropout_mask]
Namespace(dataset='cora', dropout=0, epochs=200, hidden=64, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)
adj: (3357, 3357)
features: (3357, 512)
y: (3357,) (3357,)
mask: (3357,) (3357,)
x : tensor(indices=tensor([[   0,    0,    0,  ..., 3356, 3356, 3356],
                       [ 466,  463,  447,  ...,   32,    9,    7]]),
       values=tensor([0.0111, 0.0607, 0.1179,  ..., 0.0758, 0.0186, 0.0004]),
       device='cuda:0', size=(3357, 512), nnz=81550, layout=torch.sparse_coo)
sp: tensor(indices=tensor([[   0,    1,    3,  ..., 1723, 2499, 3356],
                       [   0,    0,    0,  ..., 3356, 3356, 3356]]),
       values=tensor([0.0435, 0.0629, 0.0326,  ..., 0.0698, 0.0603, 0.2000]),
       device='cuda:0', size=(3357, 3357), nnz=138861, layout=torch.sparse_coo)
input dim: 512
output dim: 8
num_features_nonzero: 81550
0 20.032682418823242 0.1326644370122631
10 16.233016967773438 0.6157562244518766
20 13.43758487701416 0.842809364548495
30 11.107741355895996 0.89520624303233
40 9.394603729248047 0.9230769230769231
50 7.974106788635254 0.936826458565589
60 6.853383541107178 0.9587513935340022
70 5.966154098510742 0.9572649572649573
80 5.176921367645264 0.9617242660720922
90 4.433291912078857 0.9643255295429208
100 3.8819189071655273 0.9646971386101821
110 3.349799633026123 0.967670011148272
120 2.9788551330566406 0.9639539204756596
130 2.602353096008301 0.969156447417317
140 2.275629758834839 0.969156447417317
150 2.00744366645813 0.9672984020810108
160 1.8129678964614868 0.9639539204756596
170 1.5955604314804077 0.978446674098848
rm: cannot remove â€˜validation/*â€™: No such file or directory
rm: cannot remove â€˜stride50_val/*â€™: No such file or directory
rm: cannot remove â€˜int_val/*â€™: No such file or directory
rm: cannot remove â€˜filtered_val/*â€™: No such file or directory
rm: cannot remove â€˜dataset/*â€™: No such file or directory
rm: cannot remove â€˜split_long_reads_val/*â€™: No such file or directory
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_420.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_590.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_420.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Dictionary cleaned
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
DTR_892009
error length < 2000bp
DTR_893116
error length < 2000bp
DTR_892980
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_228.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_691.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_751.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_228.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_004058275.1
error length < 2000bp
GCA_004207075.1
error length < 2000bp
GCA_003624435.1
error length < 2000bp
GCA_002830245.1
error length < 2000bp
GCA_003985805.1
error length < 2000bp
GCA_004061555.1
error length < 2000bp
GCA_003619835.1
error length < 2000bp
GCA_003532905.1
error length < 2000bp
GCA_000868625.1
error length < 2000bp
GCA_004071415.1
error length < 2000bp
GCA_004036055.1
error length < 2000bp
GCA_004093795.1
error length < 2000bp
GCA_002988085.1
error length < 2000bp
GCA_001589635.1
error length < 2000bp
GCA_003616335.1
error length < 2000bp
GCA_004199335.1
error length < 2000bp
GCA_003656805.1
error length < 2000bp
GCA_003616735.1
error length < 2000bp
GCA_003652805.1
error length < 2000bp
GCA_000873365.1
error length < 2000bp
GCA_004080835.1
error length < 2000bp
GCA_003985505.1
error length < 2000bp
GCA_004069875.1
error length < 2000bp
GCA_003620895.1
error length < 2000bp
GCA_004200995.1
error length < 2000bp
GCA_004204255.1
error length < 2000bp
GCA_004204475.1
error length < 2000bp
GCA_000868325.1
error length < 2000bp
GCA_003659005.1
error length < 2000bp
GCA_004042795.1
error length < 2000bp
GCA_004036835.1
error length < 2000bp
GCA_004197415.1
error length < 2000bp
GCA_000890255.1
error length < 2000bp
GCA_003536685.1
error length < 2000bp
GCA_000845625.1
error length < 2000bp
GCA_004078795.1
error length < 2000bp
GCA_003073075.1
error length < 2000bp
GCA_002987805.1
error length < 2000bp
GCA_003617835.1
error length < 2000bp
GCA_003033015.1
error length < 2000bp
GCA_002988005.1
error length < 2000bp
GCA_002145985.1
error length < 2000bp
GCA_004207875.1
error length < 2000bp
GCA_000871565.1
error length < 2000bp
GCA_004202595.1
error length < 2000bp
GCA_003033995.1
error length < 2000bp
GCA_003652345.1
error length < 2000bp
GCA_004205995.1
error length < 2000bp
GCA_004207175.1
error length < 2000bp
GCA_004050195.1
error length < 2000bp
GCA_000865005.1
error length < 2000bp
GCA_003047915.1
error length < 2000bp
GCA_003658325.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_104.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_134.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_157.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_162.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_189.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_195.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_218.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_225.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_229.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_244.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_249.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_273.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_290.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_305.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_310.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_325.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_327.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_332.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_362.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_374.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_396.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_402.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_431.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_447.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_452.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_454.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_476.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_494.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_509.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_528.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_560.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_589.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_604.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_627.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_639.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_647.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_655.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_663.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_680.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_690.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_717.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_718.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_720.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_741.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_75.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_790.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_792.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_80.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_811.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_814.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_837.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_848.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_861.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_877.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_882.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_883.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_892.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_894.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_909.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_926.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_928.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_935.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_946.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_950.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_969.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_987.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_104.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_004787915.1
error length < 2000bp
GCA_003653705.1
error length < 2000bp
GCA_004208055.1
error length < 2000bp
GCA_004086455.1
error length < 2000bp
GCA_001960615.1
error length < 2000bp
GCA_003656945.1
error length < 2000bp
GCA_003623575.1
error length < 2000bp
GCA_004095915.1
error length < 2000bp
GCA_004200475.1
error length < 2000bp
GCA_000887815.1
error length < 2000bp
GCA_003033985.1
error length < 2000bp
GCA_000917995.1
error length < 2000bp
GCA_002819765.1
error length < 2000bp
GCA_003655585.1
error length < 2000bp
GCA_000861305.1
error length < 2000bp
GCA_004042815.1
error length < 2000bp
GCA_003658885.1
error length < 2000bp
GCA_004062855.1
error length < 2000bp
GCA_004201295.1
error length < 2000bp
GCA_004201795.1
error length < 2000bp
GCA_000890495.1
error length < 2000bp
GCA_004129095.1
error length < 2000bp
GCA_001646395.1
error length < 2000bp
GCA_004200735.1
error length < 2000bp
GCA_001020035.1
error length < 2000bp
GCA_003652445.1
error length < 2000bp
GCA_000886955.1
error length < 2000bp
GCA_000892615.1
error length < 2000bp
GCA_003028935.1
error length < 2000bp
GCA_000889475.1
error length < 2000bp
GCA_003656265.1
error length < 2000bp
GCA_000843905.1
error length < 2000bp
GCA_004290555.1
error length < 2000bp
GCA_003654045.1
error length < 2000bp
GCA_001589455.1
error length < 2000bp
GCA_003654785.1
error length < 2000bp
GCA_004047295.1
error length < 2000bp
GCA_000871765.1
error length < 2000bp
GCA_001589815.1
error length < 2000bp
GCA_000897295.1
error length < 2000bp
GCA_003652965.1
error length < 2000bp
GCA_000909475.1
error length < 2000bp
GCA_003653485.1
error length < 2000bp
GCA_004290795.1
error length < 2000bp
GCA_000856565.1
error length < 2000bp
GCA_004065775.1
error length < 2000bp
GCA_003656765.1
error length < 2000bp
GCA_004199535.1
error length < 2000bp
GCA_003657425.1
error length < 2000bp
GCA_001590195.1
error length < 2000bp
GCA_003617495.1
error length < 2000bp
GCA_004047015.1
error length < 2000bp
GCA_000900915.1
error length < 2000bp
GCA_000881315.1
error length < 2000bp
GCA_000872785.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_0.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_105.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_115.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_152.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_158.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_161.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_174.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_177.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_186.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_224.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_231.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_235.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_248.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_25.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_281.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_30.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_331.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_338.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_345.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_361.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_363.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_372.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_374.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_385.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_395.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_401.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_402.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_413.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_421.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_432.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_437.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_438.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_440.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_488.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_515.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_518.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_527.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_550.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_574.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_578.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_582.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_584.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_587.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_591.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_596.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_618.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_628.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_648.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_656.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_658.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_66.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_674.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_678.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_690.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_693.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_70.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_71.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_718.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_730.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_750.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_793.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_801.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_827.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_858.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_863.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_867.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_882.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_890.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_895.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_906.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_909.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_915.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_925.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_968.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_970.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_987.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_988.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_0.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_004092595.1
error length < 2000bp
GCA_003986025.1
error length < 2000bp
GCA_004050435.1
error length < 2000bp
GCA_003540355.1
error length < 2000bp
GCA_000837585.1
error length < 2000bp
GCA_004206475.1
error length < 2000bp
GCA_003654605.1
error length < 2000bp
GCA_003986325.1
error length < 2000bp
GCA_003624755.1
error length < 2000bp
GCA_003655305.1
error length < 2000bp
GCA_004088615.1
error length < 2000bp
GCA_003654845.1
error length < 2000bp
GCA_002830225.1
error length < 2000bp
GCA_004082235.1
error length < 2000bp
GCA_003623735.1
error length < 2000bp
GCA_000891415.1
error length < 2000bp
GCA_003659805.1
error length < 2000bp
GCA_000874005.1
error length < 2000bp
GCA_002820025.1
error length < 2000bp
GCA_000925255.1
error length < 2000bp
GCA_003029585.1
error length < 2000bp
GCA_003034065.1
error length < 2000bp
GCA_000921555.1
error length < 2000bp
GCA_004098955.1
error length < 2000bp
GCA_003622255.1
error length < 2000bp
GCA_003536355.1
error length < 2000bp
GCA_004205875.1
error length < 2000bp
GCA_003532875.1
error length < 2000bp
GCA_003618755.1
error length < 2000bp
GCA_004207435.1
error length < 2000bp
GCA_000865025.1
error length < 2000bp
GCA_004198195.1
error length < 2000bp
GCA_004063815.1
error length < 2000bp
GCA_000901935.1
error length < 2000bp
GCA_002819845.1
error length < 2000bp
GCA_002830185.1
error length < 2000bp
GCA_002820165.1
error length < 2000bp
GCA_003657965.1
error length < 2000bp
GCA_004197715.1
error length < 2000bp
GCA_004046095.1
error length < 2000bp
GCA_000846785.1
error length < 2000bp
GCA_001579335.1
error length < 2000bp
GCA_000858085.1
error length < 2000bp
GCA_002819785.1
error length < 2000bp
GCA_002830145.1
error length < 2000bp
GCA_003659225.1
error length < 2000bp
GCA_002830445.1
error length < 2000bp
GCA_004198955.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_109.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_152.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_168.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_183.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_185.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_192.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_204.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_212.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_22.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_23.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_259.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_26.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_268.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_271.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_279.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_288.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_331.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_37.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_383.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_393.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_397.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_412.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_418.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_419.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_493.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_504.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_545.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_546.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_547.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_555.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_571.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_582.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_594.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_598.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_616.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_63.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_630.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_640.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_641.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_714.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_720.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_742.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_766.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_783.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_79.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_8.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_808.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_818.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_82.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_829.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_834.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_859.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_864.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_876.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_903.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_906.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_942.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_947.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_981.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_990.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_109.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_003622815.1
error length < 2000bp
GCA_002819585.1
error length < 2000bp
GCA_003624035.1
error length < 2000bp
GCA_000902015.1
error length < 2000bp
GCA_004132065.1
error length < 2000bp
GCA_004131285.1
error length < 2000bp
GCA_000923375.1
error length < 2000bp
GCA_000897035.1
error length < 2000bp
GCA_004055715.1
error length < 2000bp
GCA_003617935.1
error length < 2000bp
GCA_003985985.1
error length < 2000bp
GCA_003622075.1
error length < 2000bp
GCA_003655945.1
error length < 2000bp
GCA_002819805.1
error length < 2000bp
GCA_003657445.1
error length < 2000bp
GCA_002819825.1
error length < 2000bp
GCA_003621395.1
error length < 2000bp
GCA_001654225.1
error length < 2000bp
GCA_004076615.1
error length < 2000bp
GCA_003537395.1
error length < 2000bp
GCA_004065935.1
error length < 2000bp
GCA_003028985.1
error length < 2000bp
GCA_003622235.1
error length < 2000bp
GCA_004369305.1
error length < 2000bp
GCA_000921035.1
error length < 2000bp
GCA_003531735.1
error length < 2000bp
GCA_004033635.1
error length < 2000bp
GCA_002830385.1
error length < 2000bp
GCA_000884735.1
error length < 2000bp
GCA_000923815.1
error length < 2000bp
GCA_003619215.1
error length < 2000bp
GCA_002830345.1
error length < 2000bp
GCA_004207655.1
error length < 2000bp
GCA_003622215.1
error length < 2000bp
GCA_004066075.1
error length < 2000bp
GCA_003618355.1
error length < 2000bp
GCA_003972145.1
error length < 2000bp
GCA_003654925.1
error length < 2000bp
GCA_000902995.1
error length < 2000bp
GCA_000865705.1
error length < 2000bp
GCA_003653645.1
error length < 2000bp
GCA_004045335.1
error length < 2000bp
GCA_004085055.1
error length < 2000bp
GCA_000929875.1
error length < 2000bp
GCA_000914395.1
error length < 2000bp
GCA_000885355.1
error length < 2000bp
GCA_000890515.1
error length < 2000bp
GCA_004290495.1
error length < 2000bp
GCA_000837045.1
error length < 2000bp
GCA_000919655.1
error length < 2000bp
GCA_002987955.1
error length < 2000bp
GCA_003654585.1
error length < 2000bp
GCA_002210655.1
error length < 2000bp
GCA_003847165.1
error length < 2000bp
GCA_003622515.1
error length < 2000bp
GCA_000899255.1
error length < 2000bp
GCA_004208275.1
error length < 2000bp
GCA_003623515.1
error length < 2000bp
GCA_003653245.1
error length < 2000bp
GCA_002830065.1
error length < 2000bp
GCA_002987995.1
error length < 2000bp
GCA_003615495.1
error length < 2000bp
GCA_003617195.1
error length < 2000bp
GCA_004057035.1
error length < 2000bp
GCA_003658905.1
error length < 2000bp
GCA_000871745.1
error length < 2000bp
GCA_003619815.1
error length < 2000bp
GCA_003623435.1
error length < 2000bp
GCA_000837325.1
error length < 2000bp
GCA_003847085.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_128.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_131.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_138.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_159.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_163.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_173.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_18.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_184.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_192.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_198.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_199.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_2.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_208.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_210.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_213.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_225.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_228.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_236.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_242.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_249.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_253.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_256.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_274.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_313.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_321.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_322.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_326.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_338.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_342.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_366.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_395.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_414.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_421.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_426.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_470.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_478.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_49.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_499.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_50.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_502.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_51.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_521.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_523.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_533.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_540.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_545.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_549.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_560.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_582.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_594.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_598.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_621.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_625.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_63.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_632.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_641.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_680.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_687.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_69.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_690.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_70.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_727.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_731.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_736.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_737.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_766.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_768.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_78.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_782.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_795.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_814.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_829.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_839.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_87.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_878.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_889.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_898.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_908.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_92.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_94.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_941.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_956.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_958.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_959.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_969.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_000955555.1
error length < 2000bp
GCA_004202855.1
error length < 2000bp
GCA_004076075.1
error length < 2000bp
GCA_002987935.1
error length < 2000bp
GCA_003615555.1
error length < 2000bp
GCA_000919015.1
error length < 2000bp
GCA_002987895.1
error length < 2000bp
GCA_004198055.1
error length < 2000bp
GCA_000866685.1
error length < 2000bp
GCA_001430615.1
error length < 2000bp
GCA_002830025.1
error length < 2000bp
GCA_001963675.1
error length < 2000bp
GCA_000871725.1
error length < 2000bp
GCA_003659565.1
error length < 2000bp
GCA_003621375.1
error length < 2000bp
GCA_003615395.1
error length < 2000bp
GCA_004202975.1
error length < 2000bp
GCA_000872085.1
error length < 2000bp
GCA_003985625.1
error length < 2000bp
GCA_004205255.1
error length < 2000bp
GCA_003653265.1
error length < 2000bp
GCA_003619715.1
error length < 2000bp
GCA_000891475.1
error length < 2000bp
GCA_003615615.1
error length < 2000bp
GCA_000897335.1
error length < 2000bp
GCA_004203635.1
error length < 2000bp
GCA_003617715.1
error length < 2000bp
GCA_003653985.1
error length < 2000bp
GCA_004207735.1
error length < 2000bp
GCA_003658645.1
error length < 2000bp
GCA_003621475.1
error length < 2000bp
GCA_000870925.1
error length < 2000bp
GCA_004206635.1
error length < 2000bp
GCA_000943745.1
error length < 2000bp
GCA_000915235.1
error length < 2000bp
GCA_004076115.1
error length < 2000bp
GCA_003615795.1
error length < 2000bp
GCA_004205375.1
error length < 2000bp
GCA_003616755.1
error length < 2000bp
GCA_003034075.1
error length < 2000bp
GCA_000875665.1
error length < 2000bp
GCA_003652425.1
error length < 2000bp
GCA_003659045.1
error length < 2000bp
GCA_002820045.1
error length < 2000bp
GCA_004058335.1
error length < 2000bp
GCA_000844325.1
error length < 2000bp
GCA_004069815.1
error length < 2000bp
GCA_003659245.1
error length < 2000bp
GCA_004320305.1
error length < 2000bp
GCA_004074395.1
error length < 2000bp
GCA_000869485.1
error length < 2000bp
GCA_002819705.1
error length < 2000bp
GCA_004063615.1
error length < 2000bp
GCA_003034055.1
error length < 2000bp
GCA_000842325.1
error length < 2000bp
GCA_002820185.1
error length < 2000bp
GCA_004199495.1
error length < 2000bp
GCA_004011295.1
error length < 2000bp
GCA_003616435.1
error length < 2000bp
GCA_004203395.1
error length < 2000bp
GCA_001184985.1
error length < 2000bp
GCA_001589855.1
error length < 2000bp
GCA_002820005.1
error length < 2000bp
GCA_004206755.1
error length < 2000bp
GCA_004133105.1
error length < 2000bp
GCA_004058395.1
error length < 2000bp
GCA_003618715.1
error length < 2000bp
GCA_002987825.1
error length < 2000bp
GCA_000870705.1
error length < 2000bp
GCA_004061815.1
error length < 2000bp
GCA_004032855.1
error length < 2000bp
GCA_000869545.1
error length < 2000bp
GCA_003985545.1
error length < 2000bp
GCA_004197515.1
error length < 2000bp
GCA_004091875.1
error length < 2000bp
GCA_000889375.1
error length < 2000bp
GCA_004129075.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_106.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_126.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_131.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_152.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_158.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_165.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_166.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_192.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_198.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_20.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_200.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_202.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_218.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_224.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_254.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_256.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_268.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_284.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_297.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_298.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_308.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_319.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_32.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_324.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_359.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_367.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_370.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_380.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_383.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_394.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_395.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_398.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_40.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_418.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_427.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_443.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_444.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_468.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_496.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_498.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_515.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_521.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_525.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_562.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_565.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_576.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_583.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_585.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_594.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_600.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_606.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_608.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_614.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_618.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_633.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_640.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_650.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_665.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_667.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_683.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_686.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_69.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_691.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_694.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_709.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_722.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_728.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_739.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_744.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_747.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_749.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_75.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_750.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_756.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_759.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_76.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_760.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_782.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_791.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_793.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_794.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_797.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_803.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_837.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_838.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_86.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_877.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_895.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_897.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_901.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_917.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_918.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_969.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_974.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_978.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_979.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_106.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_000922955.1
error length < 2000bp
GCA_002922235.1
error length < 2000bp
GCA_003656585.1
error length < 2000bp
GCA_000846485.1
error length < 2000bp
GCA_000889595.1
error length < 2000bp
GCA_004036495.1
error length < 2000bp
GCA_002830005.1
error length < 2000bp
GCA_003618335.1
error length < 2000bp
GCA_003847125.1
error length < 2000bp
GCA_004042775.1
error length < 2000bp
GCA_000886575.1
error length < 2000bp
GCA_004061995.1
error length < 2000bp
GCA_003654225.1
error length < 2000bp
GCA_004061755.1
error length < 2000bp
GCA_004207995.1
error length < 2000bp
GCA_003616815.1
error length < 2000bp
GCA_003656165.1
error length < 2000bp
GCA_003618055.1
error length < 2000bp
GCA_000848965.1
error length < 2000bp
GCA_003657325.1
error length < 2000bp
GCA_000989035.1
error length < 2000bp
GCA_000846465.1
error length < 2000bp
GCA_000843925.1
error length < 2000bp
GCA_000869525.1
error length < 2000bp
GCA_004033535.1
error length < 2000bp
GCA_000855465.1
error length < 2000bp
GCA_002922265.1
error length < 2000bp
GCA_003544255.1
error length < 2000bp
GCA_003656605.1
error length < 2000bp
GCA_004205935.1
error length < 2000bp
GCA_003028915.1
error length < 2000bp
GCA_003617135.1
error length < 2000bp
GCA_004289915.1
error length < 2000bp
GCA_001550985.1
error length < 2000bp
GCA_003655105.1
error length < 2000bp
GCA_000922135.1
error length < 2000bp
GCA_003653825.1
error length < 2000bp
GCA_003619995.1
error length < 2000bp
GCA_000923235.1
error length < 2000bp
GCA_000844745.1
error length < 2000bp
GCA_004063355.1
error length < 2000bp
GCA_000839285.1
error length < 2000bp
GCA_001429995.1
error length < 2000bp
GCA_003527845.1
error length < 2000bp
GCA_004047135.1
error length < 2000bp
GCA_004203815.1
error length < 2000bp
GCA_003654205.1
error length < 2000bp
GCA_000879715.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_100.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_104.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_107.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_126.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_13.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_14.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_141.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_143.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_171.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_172.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_18.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_213.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_258.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_265.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_266.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_272.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_304.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_32.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_320.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_328.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_341.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_342.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_371.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_436.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_440.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_462.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_48.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_486.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_49.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_531.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_537.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_551.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_583.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_585.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_637.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_644.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_668.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_70.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_71.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_717.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_732.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_746.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_754.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_767.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_773.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_779.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_80.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_802.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_813.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_816.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_847.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_848.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_849.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_878.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_888.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_891.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_9.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_938.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_940.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_941.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_972.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_974.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_100.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_000955295.1
error length < 2000bp
GCA_003618875.1
error length < 2000bp
GCA_004036235.1
error length < 2000bp
GCA_000888815.1
error length < 2000bp
GCA_000850505.1
error length < 2000bp
GCA_000844785.1
error length < 2000bp
GCA_000930515.1
error length < 2000bp
GCA_004206375.1
error length < 2000bp
GCA_004202515.1
error length < 2000bp
GCA_000888115.1
error length < 2000bp
GCA_003617215.1
error length < 2000bp
GCA_004202495.1
error length < 2000bp
GCA_004095615.1
error length < 2000bp
GCA_003655725.1
error length < 2000bp
GCA_003033975.1
error length < 2000bp
GCA_000870865.1
error length < 2000bp
GCA_003653165.1
error length < 2000bp
GCA_002819665.1
error length < 2000bp
GCA_004208395.1
error length < 2000bp
GCA_004369685.1
error length < 2000bp
GCA_004206855.1
error length < 2000bp
GCA_004061875.1
error length < 2000bp
GCA_000845565.1
error length < 2000bp
GCA_003029525.1
error length < 2000bp
GCA_004063675.1
error length < 2000bp
GCA_004042875.1
error length < 2000bp
GCA_003033645.1
error length < 2000bp
GCA_003618175.1
error length < 2000bp
GCA_003537185.1
error length < 2000bp
GCA_003653465.1
error length < 2000bp
GCA_003034035.1
error length < 2000bp
GCA_004059715.1
error length < 2000bp
GCA_000890035.1
error length < 2000bp
GCA_002820145.1
error length < 2000bp
GCA_004290255.1
error length < 2000bp
GCA_000843965.1
error length < 2000bp
GCA_000848105.1
error length < 2000bp
GCA_003655905.1
error length < 2000bp
GCA_003622195.1
error length < 2000bp
GCA_002819685.1
error length < 2000bp
GCA_002830325.1
error length < 2000bp
GCA_004078335.1
error length < 2000bp
GCA_003653365.1
error length < 2000bp
GCA_003653805.1
error length < 2000bp
GCA_000865285.1
error length < 2000bp
GCA_003615775.1
error length < 2000bp
GCA_004289755.1
error length < 2000bp
GCA_000897935.1
error length < 2000bp
GCA_000884675.1
error length < 2000bp
GCA_003660045.1
error length < 2000bp
GCA_002819545.1
error length < 2000bp
GCA_003625395.1
error length < 2000bp
GCA_003624415.1
error length < 2000bp
GCA_004132025.1
error length < 2000bp
GCA_003620155.1
error length < 2000bp
GCA_000864065.1
error length < 2000bp
GCA_004201655.1
error length < 2000bp
GCA_004050255.1
error length < 2000bp
GCA_004034075.1
error length < 2000bp
GCA_001274465.1
error length < 2000bp
GCA_004206235.1
error length < 2000bp
GCA_004207235.1
error length < 2000bp
GCA_000916635.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_103.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_106.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_129.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_131.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_141.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_150.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_175.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_178.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_18.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_201.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_210.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_220.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_222.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_232.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_248.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_254.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_258.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_259.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_268.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_270.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_279.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_284.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_293.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_295.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_3.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_340.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_351.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_366.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_375.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_433.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_436.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_455.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_456.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_468.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_480.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_482.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_50.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_506.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_524.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_532.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_545.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_559.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_562.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_563.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_572.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_581.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_587.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_615.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_636.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_64.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_654.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_661.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_678.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_710.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_738.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_767.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_777.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_78.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_798.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_803.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_829.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_833.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_847.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_920.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_932.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_933.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_938.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_941.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_943.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_971.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_974.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_976.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_996.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_997.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_103.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_001646195.1
error length < 2000bp
GCA_003616935.1
error length < 2000bp
GCA_003621535.1
error length < 2000bp
GCA_000915275.1
error length < 2000bp
GCA_003655045.1
error length < 2000bp
GCA_003627175.1
error length < 2000bp
GCA_003653585.1
error length < 2000bp
GCA_003651825.1
error length < 2000bp
GCA_003659105.1
error length < 2000bp
GCA_003618015.1
error length < 2000bp
GCA_000846005.2
error length < 2000bp
GCA_003622615.1
error length < 2000bp
GCA_004199655.1
error length < 2000bp
GCA_004097775.1
error length < 2000bp
GCA_002819905.1
error length < 2000bp
GCA_004099095.1
error length < 2000bp
GCA_003620435.1
error length < 2000bp
GCA_004201595.1
error length < 2000bp
GCA_003655625.1
error length < 2000bp
GCA_000889335.1
error length < 2000bp
GCA_004285955.1
error length < 2000bp
GCA_003620495.1
error length < 2000bp
GCA_000866525.1
error length < 2000bp
GCA_001961415.1
error length < 2000bp
GCA_004033615.1
error length < 2000bp
GCA_004095835.1
error length < 2000bp
GCA_004063715.1
error length < 2000bp
GCA_004201435.1
error length < 2000bp
GCA_004062275.1
error length < 2000bp
GCA_000849185.1
error length < 2000bp
GCA_004197995.1
error length < 2000bp
GCA_000904615.1
error length < 2000bp
GCA_000867705.1
error length < 2000bp
GCA_003622775.1
error length < 2000bp
GCA_004369325.1
error length < 2000bp
GCA_004076735.1
error length < 2000bp
GCA_003621635.1
error length < 2000bp
GCA_003622915.1
error length < 2000bp
GCA_002830165.1
error length < 2000bp
GCA_002830485.1
error length < 2000bp
GCA_004369365.1
error length < 2000bp
GCA_003971865.1
error length < 2000bp
GCA_003654565.1
error length < 2000bp
GCA_004061515.1
error length < 2000bp
GCA_000874365.1
error length < 2000bp
GCA_004132085.1
error length < 2000bp
GCA_003622495.1
error length < 2000bp
GCA_000846445.1
error length < 2000bp
GCA_002819925.1
error length < 2000bp
GCA_004200255.1
error length < 2000bp
GCA_004063835.1
error length < 2000bp
GCA_004076435.1
error length < 2000bp
GCA_004063895.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_114.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_193.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_213.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_22.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_226.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_246.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_272.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_285.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_310.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_320.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_343.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_35.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_373.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_383.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_399.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_414.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_427.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_459.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_461.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_48.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_485.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_489.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_512.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_515.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_53.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_543.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_570.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_574.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_6.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_605.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_611.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_617.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_62.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_631.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_640.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_655.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_656.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_66.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_664.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_665.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_67.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_675.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_71.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_735.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_739.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_744.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_769.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_816.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_82.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_849.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_86.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_861.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_864.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_867.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_868.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_880.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_881.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_885.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_920.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_924.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_942.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_957.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_960.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_962.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_966.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_970.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_971.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_983.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_985.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_991.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_003847105.1
error length < 2000bp
GCA_004198575.1
error length < 2000bp
GCA_003986125.1
error length < 2000bp
GCA_003615535.1
error length < 2000bp
GCA_003655885.1
error length < 2000bp
GCA_004289855.1
error length < 2000bp
GCA_004047215.1
error length < 2000bp
GCA_002830285.1
error length < 2000bp
GCA_003616075.1
error length < 2000bp
GCA_004073495.1
error length < 2000bp
GCA_001974535.1
error length < 2000bp
GCA_004202055.1
error length < 2000bp
GCA_004133245.1
error length < 2000bp
GCA_004203555.1
error length < 2000bp
GCA_003652925.1
error length < 2000bp
GCA_004320265.1
error length < 2000bp
GCA_003658365.1
error length < 2000bp
GCA_004198775.1
error length < 2000bp
GCA_003652865.1
error length < 2000bp
GCA_004289775.1
error length < 2000bp
GCA_001960015.1
error length < 2000bp
GCA_003033635.1
error length < 2000bp
GCA_003619435.1
error length < 2000bp
GCA_003656005.1
error length < 2000bp
GCA_003847205.1
error length < 2000bp
GCA_000844465.1
error length < 2000bp
GCA_004050735.1
error length < 2000bp
GCA_004289835.1
error length < 2000bp
GCA_002987965.1
error length < 2000bp
GCA_003034015.1
error length < 2000bp
GCA_003621675.1
error length < 2000bp
GCA_004061735.1
error length < 2000bp
GCA_004205275.1
error length < 2000bp
GCA_002819985.1
error length < 2000bp
GCA_003619415.1
error length < 2000bp
GCA_003653885.1
error length < 2000bp
GCA_003655465.1
error length < 2000bp
GCA_002987885.1
error length < 2000bp
GCA_000908515.1
error length < 2000bp
GCA_004035995.1
error length < 2000bp
GCA_003653405.1
error length < 2000bp
GCA_002819645.1
error length < 2000bp
GCA_003619315.1
error length < 2000bp
GCA_000923555.1
error length < 2000bp
GCA_004206575.1
error length < 2000bp
GCA_003660005.1
error length < 2000bp
GCA_003621975.1
error length < 2000bp
GCA_002149165.1
error length < 2000bp
GCA_000915415.1
error length < 2000bp
GCA_000887975.1
error length < 2000bp
GCA_002830105.1
error length < 2000bp
GCA_003847025.1
error length < 2000bp
GCA_002819465.1
error length < 2000bp
GCA_001430315.1
error length < 2000bp
GCA_004076055.1
error length < 2000bp
GCA_000845285.1
error length < 2000bp
GCA_004290875.1
error length < 2000bp
GCA_000895995.1
error length < 2000bp
GCA_004203575.1
error length < 2000bp
GCA_004097935.1
error length < 2000bp
GCA_004040095.1
error length < 2000bp
GCA_001651125.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_155.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_158.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_17.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_187.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_190.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_201.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_205.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_226.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_229.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_239.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_244.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_249.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_277.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_284.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_286.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_3.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_307.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_330.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_352.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_358.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_378.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_404.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_408.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_410.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_428.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_442.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_454.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_471.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_481.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_502.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_550.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_553.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_563.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_570.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_577.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_578.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_582.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_595.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_598.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_611.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_62.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_622.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_64.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_663.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_694.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_702.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_71.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_72.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_731.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_734.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_747.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_755.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_762.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_81.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_819.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_845.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_852.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_856.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_863.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_889.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_892.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_893.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_918.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_923.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_94.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_942.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_952.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_954.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_960.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_962.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_964.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_975.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_004207115.1
error length < 2000bp
GCA_004036255.1
error length < 2000bp
GCA_002830265.1
error length < 2000bp
GCA_000885115.1
error length < 2000bp
GCA_004206775.1
error length < 2000bp
GCA_002219925.1
error length < 2000bp
GCA_003655005.1
error length < 2000bp
GCA_000873325.1
error length < 2000bp
GCA_000871425.1
error length < 2000bp
GCA_003621695.1
error length < 2000bp
GCA_000898635.1
error length < 2000bp
GCA_003654645.1
error length < 2000bp
GCA_004197375.1
error length < 2000bp
GCA_003532615.1
error length < 2000bp
GCA_004202955.1
error length < 2000bp
GCA_003621875.1
error length < 2000bp
GCA_004079315.1
error length < 2000bp
GCA_003034085.1
error length < 2000bp
GCA_004074775.1
error length < 2000bp
GCA_002826085.1
error length < 2000bp
GCA_004290675.1
error length < 2000bp
GCA_002819485.1
error length < 2000bp
GCA_004199155.1
error length < 2000bp
GCA_003619935.1
error length < 2000bp
GCA_004206955.1
error length < 2000bp
GCA_000845605.1
error length < 2000bp
GCA_002820225.1
error length < 2000bp
GCA_004289895.1
error length < 2000bp
GCA_002987915.1
error length < 2000bp
GCA_000915855.1
error length < 2000bp
GCA_004207855.1
error length < 2000bp
GCA_000869565.1
error length < 2000bp
GCA_003620995.1
error length < 2000bp
GCA_004047255.1
error length < 2000bp
GCA_002080175.1
error length < 2000bp
GCA_000954515.1
error length < 2000bp
GCA_000903535.1
error length < 2000bp
GCA_001045325.1
error length < 2000bp
GCA_004203255.1
error length < 2000bp
GCA_004097335.1
error length < 2000bp
GCA_003656405.1
error length < 2000bp
GCA_002819945.1
error length < 2000bp
GCA_000920495.1
error length < 2000bp
GCA_003656065.1
error length < 2000bp
GCA_004206975.1
error length < 2000bp
GCA_004200415.1
error length < 2000bp
GCA_001551325.1
error length < 2000bp
GCA_001995595.1
error length < 2000bp
GCA_004206015.1
error length < 2000bp
GCA_000864405.1
error length < 2000bp
GCA_003617735.1
error length < 2000bp
GCA_004206415.1
error length < 2000bp
GCA_002820065.1
error length < 2000bp
GCA_000842005.1
error length < 2000bp
GCA_003029425.1
error length < 2000bp
GCA_003617055.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_10.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_106.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_139.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_140.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_151.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_156.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_163.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_167.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_189.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_204.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_209.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_213.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_22.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_222.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_237.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_243.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_250.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_264.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_27.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_28.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_284.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_291.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_30.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_325.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_371.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_378.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_381.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_384.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_400.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_401.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_406.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_416.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_419.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_42.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_428.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_447.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_48.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_486.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_511.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_513.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_517.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_52.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_522.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_525.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_528.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_540.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_567.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_598.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_600.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_623.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_628.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_637.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_646.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_654.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_687.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_702.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_71.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_743.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_795.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_797.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_811.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_839.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_866.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_870.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_901.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_903.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_908.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_937.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_952.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_960.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_965.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_982.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_10.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_003532585.1
error length < 2000bp
GCA_003658765.1
error length < 2000bp
GCA_003047655.1
error length < 2000bp
GCA_004208235.1
error length < 2000bp
GCA_004035895.1
error length < 2000bp
GCA_003623495.1
error length < 2000bp
GCA_000875105.1
error length < 2000bp
GCA_000918115.1
error length < 2000bp
GCA_004080915.1
error length < 2000bp
GCA_000925235.1
error length < 2000bp
GCA_003029175.1
error length < 2000bp
GCA_000865445.1
error length < 2000bp
GCA_001967255.1
error length < 2000bp
GCA_003615515.1
error length < 2000bp
GCA_003657885.1
error length < 2000bp
GCA_003657265.1
error length < 2000bp
GCA_000887695.1
error length < 2000bp
GCA_003659825.1
error length < 2000bp
GCA_003659745.1
error length < 2000bp
GCA_003616715.1
error length < 2000bp
GCA_001589475.1
error length < 2000bp
GCA_004202315.1
error length < 2000bp
GCA_003654005.1
error length < 2000bp
GCA_004201075.1
error length < 2000bp
GCA_004054915.1
error length < 2000bp
GCA_003620075.1
error length < 2000bp
GCA_000905135.1
error length < 2000bp
GCA_000873345.1
error length < 2000bp
GCA_004058255.1
error length < 2000bp
GCA_003615375.1
error length < 2000bp
GCA_003653565.1
error length < 2000bp
GCA_003621215.1
error length < 2000bp
GCA_004035435.1
error length < 2000bp
GCA_000899235.1
error length < 2000bp
GCA_004203975.1
error length < 2000bp
GCA_004061235.1
error length < 2000bp
GCA_000926155.1
error length < 2000bp
GCA_003620455.1
error length < 2000bp
GCA_003656365.1
error length < 2000bp
GCA_004208135.1
error length < 2000bp
GCA_003656285.1
error length < 2000bp
GCA_003659205.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_123.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_134.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_146.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_161.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_173.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_181.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_203.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_222.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_263.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_270.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_289.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_306.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_313.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_354.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_374.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_395.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_409.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_423.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_437.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_438.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_445.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_454.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_483.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_504.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_523.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_529.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_567.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_570.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_572.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_585.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_598.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_603.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_641.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_647.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_682.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_683.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_698.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_714.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_717.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_732.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_752.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_753.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_775.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_778.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_779.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_781.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_803.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_818.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_842.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_846.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_859.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_875.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_881.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_912.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_923.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_935.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_947.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_982.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_990.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_992.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_003621855.1
error length < 2000bp
GCA_002830085.1
error length < 2000bp
GCA_003624175.1
error length < 2000bp
GCA_004041475.1
error length < 2000bp
GCA_000904215.1
error length < 2000bp
GCA_000921295.1
error length < 2000bp
GCA_001629965.1
error length < 2000bp
GCA_003617755.1
error length < 2000bp
GCA_003616315.1
error length < 2000bp
GCA_000923295.1
error length < 2000bp
GCA_004204975.1
error length < 2000bp
GCA_004097595.1
error length < 2000bp
GCA_000955535.1
error length < 2000bp
GCA_004078195.1
error length < 2000bp
GCA_002987875.1
error length < 2000bp
GCA_003623555.1
error length < 2000bp
GCA_003533065.1
error length < 2000bp
GCA_001962155.1
error length < 2000bp
GCA_000882795.1
error length < 2000bp
GCA_000869425.1
error length < 2000bp
GCA_004207415.1
error length < 2000bp
GCA_000864325.1
error length < 2000bp
GCA_003619235.1
error length < 2000bp
GCA_004207475.1
error length < 2000bp
GCA_004206895.1
error length < 2000bp
GCA_004201735.1
error length < 2000bp
GCA_000923395.1
error length < 2000bp
GCA_003540795.1
error length < 2000bp
GCA_000954455.1
error length < 2000bp
GCA_001430415.1
error length < 2000bp
GCA_002987905.1
error length < 2000bp
GCA_002819965.1
error length < 2000bp
GCA_002819525.1
error length < 2000bp
GCA_002830305.1
error length < 2000bp
GCA_003624915.1
error length < 2000bp
GCA_003622095.1
error length < 2000bp
GCA_003615935.1
error length < 2000bp
GCA_000890935.1
error length < 2000bp
GCA_000872385.1
error length < 2000bp
GCA_002819745.1
error length < 2000bp
GCA_003657945.1
error length < 2000bp
GCA_004197135.1
error length < 2000bp
GCA_000866905.1
error length < 2000bp
GCA_000891395.1
error length < 2000bp
GCA_004080955.1
error length < 2000bp
GCA_003624455.1
error length < 2000bp
GCA_003654905.1
error length < 2000bp
GCA_004787875.1
error length < 2000bp
GCA_003029185.1
error length < 2000bp
GCA_000898675.1
error length < 2000bp
GCA_000874105.1
error length < 2000bp
GCA_003619555.1
error length < 2000bp
GCA_003616055.1
error length < 2000bp
GCA_000885695.1
error length < 2000bp
GCA_004290475.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_11.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_114.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_137.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_14.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_151.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_155.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_166.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_17.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_170.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_185.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_198.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_211.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_218.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_221.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_259.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_270.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_294.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_308.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_313.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_32.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_338.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_362.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_37.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_379.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_398.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_411.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_422.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_438.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_44.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_452.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_465.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_472.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_474.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_495.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_504.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_51.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_522.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_545.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_546.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_547.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_55.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_555.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_57.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_63.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_646.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_650.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_675.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_679.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_707.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_719.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_721.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_750.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_757.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_758.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_769.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_794.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_798.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_802.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_81.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_820.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_828.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_837.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_847.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_851.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_858.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_862.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_868.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_871.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_880.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_903.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_914.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_916.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_946.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_965.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_972.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_981.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_982.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_11.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
GCA_003620395.1
error length < 2000bp
GCA_004201855.1
error length < 2000bp
GCA_003047635.1
error length < 2000bp
GCA_003985565.1
error length < 2000bp
GCA_004068915.1
error length < 2000bp
GCA_004095895.1
error length < 2000bp
GCA_000916115.1
error length < 2000bp
GCA_004205975.1
error length < 2000bp
GCA_002987855.1
error length < 2000bp
GCA_003659465.1
error length < 2000bp
GCA_002819605.1
error length < 2000bp
GCA_003616975.1
error length < 2000bp
GCA_002830365.1
error length < 2000bp
GCA_003618095.1
error length < 2000bp
GCA_002820105.1
error length < 2000bp
GCA_003029195.1
error length < 2000bp
GCA_000920515.2
error length < 2000bp
GCA_004198615.1
error length < 2000bp
GCA_003541105.1
error length < 2000bp
GCA_003658165.1
error length < 2000bp
GCA_003623035.1
error length < 2000bp
GCA_004199915.1
error length < 2000bp
GCA_000905935.1
error length < 2000bp
GCA_004206035.1
error length < 2000bp
GCA_004199035.1
error length < 2000bp
GCA_003047595.1
error length < 2000bp
GCA_000919055.1
error length < 2000bp
GCA_003047615.1
error length < 2000bp
GCA_004199995.1
error length < 2000bp
GCA_003658745.1
error length < 2000bp
GCA_003616255.1
error length < 2000bp
GCA_003622835.1
error length < 2000bp
GCA_001669805.1
error length < 2000bp
GCA_003537255.1
error length < 2000bp
GCA_002830125.1
error length < 2000bp
GCA_003536795.1
error length < 2000bp
GCA_003615635.1
error length < 2000bp
GCA_000884935.1
error length < 2000bp
GCA_000900355.1
error length < 2000bp
GCA_004066055.1
error length < 2000bp
GCA_003623455.1
error length < 2000bp
GCA_000918035.1
error length < 2000bp
GCA_004206595.1
error length < 2000bp
GCA_004063645.1
error length < 2000bp
GCA_003653965.1
error length < 2000bp
GCA_000864785.1
error length < 2000bp
GCA_003034005.1
error length < 2000bp
GCA_002820085.1
error length < 2000bp
GCA_002830045.1
error length < 2000bp
GCA_003847065.1
error length < 2000bp
GCA_004204055.1
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_104.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_113.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_118.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_136.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_143.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_187.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_198.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_205.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_215.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_229.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_258.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_263.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_265.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_266.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_284.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_32.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_325.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_332.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_34.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_361.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_368.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_416.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_442.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_445.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_49.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_498.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_515.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_521.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_544.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_551.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_564.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_574.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_583.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_587.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_619.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_627.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_64.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_655.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_661.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_677.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_678.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_688.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_701.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_716.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_726.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_746.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_749.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_752.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_754.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_764.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_766.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_773.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_785.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_79.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_8.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_82.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_823.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_827.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_829.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_838.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_840.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_855.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_874.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_899.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_924.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_972.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_976.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_979.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_996.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_104.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
KZY1-k141_1893985 flag=3 multi=7.2581 len=1985
error length < 2000bp
GCA_004204075.1
error length < 2000bp
HSd1-k141_2677854 flag=1 multi=4.0000 len=1779
error length < 2000bp
HSd1-k141_5375053 flag=3 multi=5.0029 len=1845
error length < 2000bp
HSd1-k141_4627772 flag=3 multi=132.8591 len=2000
error length < 2000bp
HSd1-k141_4791067 flag=0 multi=93.8842 len=1928
error length < 2000bp
HSd1-k141_5345113 flag=3 multi=17.0095 len=1930
error length < 2000bp
HSd1-k141_5344804 flag=3 multi=11.0064 len=1849
error length < 2000bp
KZY1-k141_1258210 flag=1 multi=11.0000 len=1801
error length < 2000bp
HSd1-k141_5352704 flag=3 multi=15.0091 len=1785
error length < 2000bp
BH1-k141_1219400 flag=3 multi=14.0000 len=1986
error length < 2000bp
KZY3-k141_757748 flag=1 multi=749.0000 len=1495
error length < 2000bp
HSd1-k141_2626493 flag=1 multi=4.0000 len=1834
error length < 2000bp
GX1-k141_437171 flag=3 multi=24.0157 len=1674
error length < 2000bp
HSd1-k141_5348993 flag=3 multi=27.0180 len=1637
error length < 2000bp
HSd1-k141_18553 flag=1 multi=5.0000 len=1899
error length < 2000bp
HSd1-k141_351071 flag=0 multi=14.6376 len=1841
error length < 2000bp
HSd1-k141_993773 flag=1 multi=4.0000 len=1700
error length < 2000bp
HSd1-k141_5374935 flag=3 multi=46.0256 len=1940
error length < 2000bp
HSd1-k141_5353201 flag=3 multi=23.0127 len=1951
error length < 2000bp
HSd1-k141_5363583 flag=3 multi=27.0147 len=1975
error length < 2000bp
HSd1-k141_5354109 flag=3 multi=13.0071 len=1977
error length < 2000bp
HSd1-k141_5359712 flag=3 multi=5.0030 len=1812
error length < 2000bp
HSd1-k141_5354433 flag=3 multi=6.0033 len=1970
error length < 2000bp
KZY3-k141_316090 flag=1 multi=89.4235 len=1801
error length < 2000bp
HSd1-k141_5361992 flag=3 multi=6.0041 len=1592
error length < 2000bp
KZY1-k141_1575781 flag=1 multi=1538.3990 len=1948
error length < 2000bp
GCA_004076795.1
error length < 2000bp
HSd1-k141_5361908 flag=3 multi=15.0091 len=1787
error length < 2000bp
GCA_004207275.1
error length < 2000bp
HSd1-k141_5351356 flag=3 multi=11.0060 len=1979
error length < 2000bp
HSd1-k141_5352233 flag=3 multi=11.0066 len=1801
error length < 2000bp
GCA_001505475.1
error length < 2000bp
KZY3-k141_977901 flag=3 multi=9.0049 len=1979
error length < 2000bp
GCA_004063595.1
error length < 2000bp
HSd1-k141_3722875 flag=1 multi=7.0000 len=1810
error length < 2000bp
GCA_003615955.1
error length < 2000bp
HSd1-k141_5349139 flag=3 multi=6.0036 len=1814
error length < 2000bp
KZY2-k141_150003 flag=1 multi=41.1446 len=1649
error length < 2000bp
BH2-k141_198145 flag=3 multi=10.0000 len=1856
error length < 2000bp
HSd1-k141_1216676 flag=0 multi=7.7425 len=1749
error length < 2000bp
KZY1-k141_1675121 flag=1 multi=6.0000 len=1938
error length < 2000bp
HSd1-k141_398354 flag=0 multi=9.1759 len=1994
error length < 2000bp
KZY1-k141_983029 flag=3 multi=67.0000 len=1888
error length < 2000bp
HSd1-k141_5352300 flag=3 multi=8.0046 len=1869
error length < 2000bp
HSd1-k141_5365597 flag=3 multi=7.0046 len=1652
error length < 2000bp
KZY2-k141_632778 flag=1 multi=9.0000 len=1939
error length < 2000bp
HSd1-k141_861362 flag=1 multi=4.0000 len=1861
error length < 2000bp
HSd1-k141_2296962 flag=1 multi=26.0000 len=1792
error length < 2000bp
HSd1-k141_5376421 flag=3 multi=82.0448 len=1970
error length < 2000bp
HSd1-k141_1584924 flag=1 multi=3.0000 len=1541
error length < 2000bp
HSd1-k141_5363821 flag=3 multi=23.0158 len=1595
error length < 2000bp
KZY2-k141_464712 flag=0 multi=312.8987 len=1800
error length < 2000bp
HSd1-k141_4513082 flag=1 multi=9.0000 len=1720
error length < 2000bp
KZY1-k141_1971827 flag=3 multi=10.0060 len=1802
error length < 2000bp
HSd1-k141_1647461 flag=1 multi=9.0000 len=1983
error length < 2000bp
HSd1-k141_3436341 flag=1 multi=14.8570 len=1903
error length < 2000bp
BH1-k141_1226882 flag=3 multi=14.0000 len=1915
error length < 2000bp
KZY1-k141_1326853 flag=3 multi=23.0000 len=1789
error length < 2000bp
KZY3-k141_167495 flag=0 multi=117.3095 len=1860
error length < 2000bp
HSd1-k141_5345938 flag=3 multi=30.0176 len=1846
error length < 2000bp
HSd2-k141_560934 flag=3 multi=39.0226 len=1866
error length < 2000bp
HSd1-k141_5133912 flag=3 multi=187.0000 len=1607
error length < 2000bp
HSd1-k141_5371616 flag=3 multi=4.0022 len=1991
error length < 2000bp
BH1-k141_510557 flag=3 multi=5.8696 len=1821
error length < 2000bp
HSd1-k141_5347680 flag=3 multi=22.0130 len=1830
error length < 2000bp
HSd1-k141_2274309 flag=1 multi=4.0000 len=1882
error length < 2000bp
GCA_000878995.1
error length < 2000bp
KZY2-k141_761220 flag=0 multi=16.0000 len=1707
error length < 2000bp
KZY1-k141_1968699 flag=3 multi=8.0043 len=1997
error length < 2000bp
GCA_004204195.1
error length < 2000bp
KZY1-k141_1470317 flag=1 multi=37.0000 len=1652
error length < 2000bp
HSd1-k141_5349984 flag=3 multi=84.0472 len=1919
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_100.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_109.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_112.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_134.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_143.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_150.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_156.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_158.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_240.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_314.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_327.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_335.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_354.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_355.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_361.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_367.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_371.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_387.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_388.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_392.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_40.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_401.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_411.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_425.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_428.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_43.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_435.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_462.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_476.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_479.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_482.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_484.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_500.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_502.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_508.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_52.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_525.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_526.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_55.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_560.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_588.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_607.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_609.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_611.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_614.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_630.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_64.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_644.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_646.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_66.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_68.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_688.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_734.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_740.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_759.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_78.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_795.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_81.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_817.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_820.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_826.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_836.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_845.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_853.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_854.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_858.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_860.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_861.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_863.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_864.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_877.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_890.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_903.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_926.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_933.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_948.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_953.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_965.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_100.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
SZd1-k141_211558 flag=1 multi=7.4739 len=1844
error length < 2000bp
T2S3-k141_444438 flag=1 multi=33.0000 len=1929
error length < 2000bp
SZd1-k141_255056 flag=0 multi=248.0000 len=1843
error length < 2000bp
LJr1-k141_89779 flag=3 multi=13.0078 len=1816
error length < 2000bp
LJd1-k141_121648 flag=3 multi=111.0661 len=1821
error length < 2000bp
LJr1-k141_88521 flag=3 multi=10.0063 len=1722
error length < 2000bp
LJr1-k141_79732 flag=3 multi=23.4334 len=1927
error length < 2000bp
ML1-k141_6480 flag=3 multi=61.0000 len=1914
error length < 2000bp
QZr2-k141_47280 flag=3 multi=32.0000 len=1940
error length < 2000bp
SZd1-k141_350581 flag=3 multi=6.0038 len=1738
error length < 2000bp
T2S1-k141_915081 flag=3 multi=24.6863 len=1974
error length < 2000bp
LJd1-k141_86357 flag=1 multi=6.7221 len=1674
error length < 2000bp
LJr1-k141_88617 flag=3 multi=3.0016 len=1964
error length < 2000bp
SZr1-k141_239145 flag=3 multi=19.0117 len=1771
error length < 2000bp
LJr1-k141_53270 flag=1 multi=3.7408 len=1738
error length < 2000bp
ML2-k141_28164 flag=3 multi=37.0215 len=1862
error length < 2000bp
LJr1-k141_30934 flag=3 multi=10.7079 len=1935
error length < 2000bp
SZd1-k141_349038 flag=3 multi=18.0098 len=1977
error length < 2000bp
LJr1-k141_45963 flag=3 multi=22.0000 len=1605
error length < 2000bp
ML1-k141_35348 flag=3 multi=17.0093 len=1978
error length < 2000bp
LJr1-k141_88996 flag=3 multi=6.0039 len=1685
error length < 2000bp
SZd1-k141_107447 flag=3 multi=12.0000 len=1947
error length < 2000bp
SZd1-k141_352163 flag=3 multi=6.9200 len=1954
error length < 2000bp
QZr3-k141_37148 flag=3 multi=14.0085 len=1786
error length < 2000bp
LJr1-k141_88401 flag=3 multi=11.0065 len=1840
error length < 2000bp
SZd1-k141_327509 flag=3 multi=43.0000 len=1890
error length < 2000bp
LJr1-k141_57032 flag=1 multi=6.8251 len=1879
error length < 2000bp
SZd1-k141_28996 flag=1 multi=22.2770 len=1863
error length < 2000bp
QZr1-k141_121636 flag=3 multi=132.0000 len=1929
error length < 2000bp
SZd1-k141_235985 flag=3 multi=210.0000 len=1889
error length < 2000bp
LJr1-k141_89073 flag=3 multi=5.0028 len=1899
error length < 2000bp
LJr1-k141_89201 flag=3 multi=10.0064 len=1704
error length < 2000bp
SZd1-k141_203202 flag=1 multi=372.7656 len=1822
error length < 2000bp
LJr1-k141_36612 flag=0 multi=7.8894 len=1787
error length < 2000bp
ML1-k141_34979 flag=3 multi=14.0080 len=1883
error length < 2000bp
SZr1-k141_238943 flag=3 multi=5.0028 len=1905
error length < 2000bp
LJd1-k141_122268 flag=3 multi=13.0098 len=1465
error length < 2000bp
T2S1-k141_915092 flag=3 multi=29.0157 len=1985
error length < 2000bp
ML1-k141_4932 flag=1 multi=27.6211 len=1780
error length < 2000bp
LJd1-k141_120313 flag=3 multi=15.8544 len=1933
error length < 2000bp
SZd1-k141_118839 flag=3 multi=43.0000 len=1875
error length < 2000bp
LJd1-k141_118942 flag=3 multi=11.0064 len=1863
error length < 2000bp
SZd3-k141_31180 flag=1 multi=34.8335 len=1739
error length < 2000bp
LJd1-k141_75136 flag=1 multi=4.0000 len=1802
error length < 2000bp
LJr1-k141_833 flag=3 multi=3.9624 len=1897
error length < 2000bp
SZd1-k141_321581 flag=3 multi=58.0000 len=1877
error length < 2000bp
LJr1-k141_66156 flag=3 multi=182.6191 len=1845
error length < 2000bp
LJr1-k141_88999 flag=3 multi=9.0049 len=1984
error length < 2000bp
SZd1-k141_24801 flag=1 multi=3.7357 len=1855
error length < 2000bp
SZd1-k141_345227 flag=0 multi=42.4272 len=1754
error length < 2000bp
LJr1-k141_79282 flag=1 multi=12.0000 len=1820
error length < 2000bp
LJr1-k141_88702 flag=3 multi=5.0028 len=1929
error length < 2000bp
LJr1-k141_25906 flag=0 multi=8.9019 len=1936
error length < 2000bp
ML1-k141_35085 flag=3 multi=7.0040 len=1889
error length < 2000bp
LJr1-k141_89182 flag=3 multi=7.0044 len=1725
error length < 2000bp
LJd1-k141_108790 flag=3 multi=120.0000 len=1521
error length < 2000bp
ML1-k141_19161 flag=1 multi=104.7960 len=1886
error length < 2000bp
KZY1-k141_1983386 flag=3 multi=7.9049 len=1908
error length < 2000bp
LJd1-k141_118918 flag=3 multi=14.0090 len=1689
error length < 2000bp
QZr2-k141_34849 flag=3 multi=42.0000 len=1766
error length < 2000bp
LJr1-k141_89980 flag=3 multi=5.0034 len=1626
error length < 2000bp
LJd1-k141_118948 flag=3 multi=11.0061 len=1941
error length < 2000bp
LJr1-k141_88939 flag=3 multi=5.9310 len=1823
error length < 2000bp
ML1-k141_13314 flag=1 multi=7.0000 len=1784
error length < 2000bp
LJr1-k141_88298 flag=3 multi=8.0043 len=1981
error length < 2000bp
SZd1-k141_145603 flag=0 multi=5.0000 len=1869
error length < 2000bp
LJr1-k141_53824 flag=3 multi=35.0000 len=1564
error length < 2000bp
ML1-k141_12851 flag=3 multi=24.0000 len=1362
error length < 2000bp
LJr1-k141_56322 flag=0 multi=9.0000 len=1856
error length < 2000bp
T4S1-k141_853732 flag=3 multi=7.0048 len=1588
error length < 2000bp
LJd1-k141_119159 flag=3 multi=19.0119 len=1736
error length < 2000bp
LJd1-k141_8124 flag=0 multi=26.0000 len=1997
error length < 2000bp
LJr1-k141_88596 flag=3 multi=9.8808 len=1886
error length < 2000bp
LJd1-k141_18144 flag=1 multi=7.0000 len=1905
error length < 2000bp
T2S2-k141_280585 flag=3 multi=106.0000 len=1885
error length < 2000bp
LJd1-k141_117996 flag=3 multi=19.0106 len=1932
error length < 2000bp
LJr1-k141_39147 flag=0 multi=26.0000 len=1623
error length < 2000bp
SZd1-k141_230987 flag=3 multi=10.0000 len=1874
error length < 2000bp
QZr1-k141_19370 flag=0 multi=26.0000 len=1945
error length < 2000bp
T4S1-k141_356861 flag=0 multi=3.8364 len=1681
error length < 2000bp
ML1-k141_16567 flag=1 multi=6.0000 len=1779
error length < 2000bp
SZd1-k141_189542 flag=1 multi=7.9132 len=1916
error length < 2000bp
LJr1-k141_37707 flag=3 multi=197.4487 len=1857
error length < 2000bp
LJr1-k141_88346 flag=3 multi=4.0022 len=1928
error length < 2000bp
SZd1-k141_306660 flag=1 multi=6.0000 len=1960
error length < 2000bp
LJr1-k141_88932 flag=3 multi=89.0497 len=1930
error length < 2000bp
LJd1-k141_119906 flag=3 multi=42.0244 len=1862
error length < 2000bp
SZd1-k141_95279 flag=3 multi=264.0000 len=1868
error length < 2000bp
SZd1-k141_66942 flag=3 multi=11.0000 len=1943
error length < 2000bp
LJd1-k141_120953 flag=3 multi=79.0432 len=1971
error length < 2000bp
LJr1-k141_68743 flag=3 multi=23.7039 len=1850
error length < 2000bp
SZr1-k141_238901 flag=3 multi=5.0029 len=1843
error length < 2000bp
LJd1-k141_64865 flag=1 multi=5.0000 len=1712
error length < 2000bp
SZd1-k141_13435 flag=1 multi=8.9044 len=1982
error length < 2000bp
QZd1-k141_2132 flag=0 multi=6.0000 len=1942
error length < 2000bp
LJd1-k141_120020 flag=3 multi=40.0225 len=1920
error length < 2000bp
LJr1-k141_88496 flag=3 multi=12.0070 len=1858
error length < 2000bp
LJd1-k141_119959 flag=3 multi=16.0091 len=1893
error length < 2000bp
LJd1-k141_35887 flag=1 multi=51.0000 len=1994
error length < 2000bp
T2S1-k141_419593 flag=1 multi=9.0000 len=1629
error length < 2000bp
LJr1-k141_89216 flag=3 multi=10.0060 len=1800
error length < 2000bp
SZd1-k141_246792 flag=3 multi=93.0000 len=1927
error length < 2000bp
SZd1-k141_235453 flag=3 multi=24.0000 len=1991
error length < 2000bp
LJd1-k141_120605 flag=3 multi=11.0065 len=1840
error length < 2000bp
LJr1-k141_88917 flag=3 multi=8.0053 len=1660
error length < 2000bp
ML3-k141_854 flag=0 multi=120.1388 len=1805
error length < 2000bp
LJd1-k141_97263 flag=1 multi=2.6560 len=1731
error length < 2000bp
SZd3-k141_62882 flag=1 multi=16.0000 len=1751
error length < 2000bp
SZd1-k141_283919 flag=1 multi=15.0000 len=1817
error length < 2000bp
LJr1-k141_88938 flag=3 multi=5.0033 len=1639
error length < 2000bp
LJd1-k141_122388 flag=3 multi=9.0049 len=1969
error length < 2000bp
SZd1-k141_80079 flag=1 multi=67.0000 len=1635
error length < 2000bp
LJr1-k141_88952 flag=3 multi=26.0154 len=1828
error length < 2000bp
SZd1-k141_246575 flag=3 multi=40.0000 len=1921
error length < 2000bp
SZd1-k141_170792 flag=1 multi=940.0638 len=1786
error length < 2000bp
SZd1-k141_8279 flag=3 multi=20.0000 len=1923
error length < 2000bp
SZd1-k141_236511 flag=1 multi=6.1956 len=1854
error length < 2000bp
LJd1-k141_39208 flag=3 multi=13.8169 len=1916
error length < 2000bp
SZd1-k141_347811 flag=1 multi=7.3922 len=1944
error length < 2000bp
SZr1-k141_238976 flag=3 multi=5.0030 len=1808
error length < 2000bp
SZd1-k141_123211 flag=0 multi=16.6157 len=1848
error length < 2000bp
LJr1-k141_88499 flag=3 multi=6.0033 len=1974
error length < 2000bp
LJr1-k141_10093 flag=1 multi=6.0000 len=1791
error length < 2000bp
T4S2-k141_505761 flag=3 multi=7.0040 len=1896
error length < 2000bp
LJr1-k141_88897 flag=3 multi=20.7685 len=1951
error length < 2000bp
LJd1-k141_47099 flag=1 multi=7.7457 len=1812
error length < 2000bp
SZd1-k141_12229 flag=3 multi=10.0000 len=1785
error length < 2000bp
LJr1-k141_88518 flag=3 multi=25.0148 len=1832
error length < 2000bp
LJr1-k141_89327 flag=3 multi=19.0123 len=1684
error length < 2000bp
QZr1-k141_223445 flag=3 multi=12.0071 len=1836
error length < 2000bp
SZd1-k141_252209 flag=3 multi=8.0000 len=1906
error length < 2000bp
SZr1-k141_239517 flag=3 multi=4.0027 len=1615
error length < 2000bp
ML1-k141_35185 flag=3 multi=92.0517 len=1922
error length < 2000bp
ML1-k141_14334 flag=1 multi=52.7843 len=1629
error length < 2000bp
LJr1-k141_88899 flag=3 multi=27.0159 len=1836
error length < 2000bp
LJd1-k141_126054 flag=3 multi=5.0030 len=1828
error length < 2000bp
LJr1-k141_89782 flag=3 multi=9.0052 len=1865
error length < 2000bp
LJd1-k141_73669 flag=1 multi=279.7032 len=1950
error length < 2000bp
SZr1-k141_238947 flag=3 multi=9.0052 len=1857
error length < 2000bp
LJd1-k141_46701 flag=3 multi=28.0000 len=1703
error length < 2000bp
QZr1-k141_171333 flag=3 multi=11.0000 len=1924
error length < 2000bp
LJr1-k141_88407 flag=3 multi=37.0214 len=1872
error length < 2000bp
QZr1-k141_175302 flag=1 multi=4.0000 len=1963
error length < 2000bp
SZd1-k141_302693 flag=3 multi=90.0000 len=1832
error length < 2000bp
LJd1-k141_68172 flag=1 multi=20.0877 len=1909
error length < 2000bp
QZr1-k141_115609 flag=3 multi=18.0000 len=1700
error length < 2000bp
LJd1-k141_121662 flag=3 multi=8.5936 len=1898
error length < 2000bp
LJd1-k141_31883 flag=1 multi=8.0000 len=1798
error length < 2000bp
QZr1-k141_223142 flag=3 multi=13.0078 len=1809
error length < 2000bp
T4S2-k141_155429 flag=1 multi=6.0000 len=1847
error length < 2000bp
SZd1-k141_24205 flag=1 multi=20.8560 len=1947
error length < 2000bp
ML1-k141_35477 flag=3 multi=13.0076 len=1849
error length < 2000bp
SZd1-k141_28433 flag=1 multi=8.0000 len=1936
error length < 2000bp
LJr1-k141_88863 flag=3 multi=6.0036 len=1829
error length < 2000bp
ML1-k141_32362 flag=1 multi=11.0000 len=1852
error length < 2000bp
LJd1-k141_122869 flag=3 multi=15.0082 len=1960
error length < 2000bp
LJd1-k141_118770 flag=3 multi=23.0127 len=1948
error length < 2000bp
LJr1-k141_4412 flag=1 multi=5.0000 len=1786
error length < 2000bp
LJd1-k141_84626 flag=1 multi=9.9119 len=1979
error length < 2000bp
ML2-k141_22096 flag=1 multi=11.0000 len=1274
error length < 2000bp
ML1-k141_33574 flag=3 multi=18.0000 len=1994
error length < 2000bp
QZd1-k141_23939 flag=1 multi=6.5599 len=1786
error length < 2000bp
SZd1-k141_76451 flag=1 multi=4.4230 len=1971
error length < 2000bp
SZd1-k141_4903 flag=3 multi=11.0000 len=1976
error length < 2000bp
SZd1-k141_172825 flag=1 multi=49.1539 len=1804
error length < 2000bp
LJd1-k141_121133 flag=3 multi=15.8192 len=1905
error length < 2000bp
ML1-k141_11270 flag=1 multi=5.0000 len=1730
error length < 2000bp
SZd1-k141_349694 flag=3 multi=20.0109 len=1981
error length < 2000bp
LJr1-k141_89917 flag=3 multi=8.0044 len=1971
error length < 2000bp
LJr1-k141_18889 flag=1 multi=4.0000 len=1792
error length < 2000bp
ML1-k141_12080 flag=3 multi=6.8749 len=1372
error length < 2000bp
LJr1-k141_90251 flag=3 multi=6.0037 len=1783
error length < 2000bp
LJr1-k141_88199 flag=0 multi=14.8174 len=1948
error length < 2000bp
SZr1-k141_184862 flag=1 multi=5.0000 len=1850
error length < 2000bp
SZr1-k141_23822 flag=0 multi=6.0000 len=1927
error length < 2000bp
SZd1-k141_349457 flag=3 multi=6.0032 len=1999
error length < 2000bp
ML1-k141_32683 flag=3 multi=7.5098 len=1934
error length < 2000bp
LJr1-k141_64659 flag=1 multi=3.0000 len=1531
error length < 2000bp
SZd1-k141_168215 flag=3 multi=21.0000 len=1940
error length < 2000bp
LJd1-k141_48885 flag=1 multi=5.0000 len=1652
error length < 2000bp
LJr1-k141_88716 flag=3 multi=5.0029 len=1881
error length < 2000bp
LJr1-k141_34040 flag=3 multi=67.0000 len=1811
error length < 2000bp
LJr1-k141_1783 flag=1 multi=7.5547 len=1933
error length < 2000bp
QZr1-k141_223018 flag=3 multi=4.0024 len=1816
error length < 2000bp
LJr1-k141_43560 flag=1 multi=8.0000 len=1847
error length < 2000bp
QZr2-k141_34652 flag=3 multi=30.0000 len=1835
error length < 2000bp
SZd1-k141_276963 flag=1 multi=6.0000 len=1754
error length < 2000bp
ML2-k141_16682 flag=0 multi=1836.0000 len=1800
error length < 2000bp
LJr1-k141_88753 flag=3 multi=20.0117 len=1845
error length < 2000bp
SZd1-k141_183395 flag=1 multi=5.4208 len=1897
error length < 2000bp
SZr1-k141_239302 flag=3 multi=42.0244 len=1860
error length < 2000bp
LJr1-k141_89475 flag=3 multi=8.0044 len=1960
error length < 2000bp
SZd1-k141_300404 flag=0 multi=22.4279 len=1833
error length < 2000bp
LJd1-k141_107279 flag=1 multi=6.1496 len=1933
error length < 2000bp
T2S1-k141_919641 flag=3 multi=10.0075 len=1479
error length < 2000bp
QZr2-k141_1205 flag=3 multi=92.0000 len=1983
error length < 2000bp
LJd1-k141_81975 flag=0 multi=35.4977 len=1680
error length < 2000bp
SZd1-k141_47402 flag=1 multi=12.3760 len=1838
error length < 2000bp
SZr1-k141_239146 flag=3 multi=7.0040 len=1898
error length < 2000bp
ML3-k141_12299 flag=1 multi=63.4612 len=1661
error length < 2000bp
SZr1-k141_8026 flag=1 multi=4.0000 len=1958
error length < 2000bp
LJd1-k141_54793 flag=1 multi=5.0000 len=1845
error length < 2000bp
LJd1-k141_89791 flag=1 multi=6.8173 len=1882
error length < 2000bp
SZd1-k141_11955 flag=1 multi=3.9357 len=1868
error length < 2000bp
SZr1-k141_239198 flag=3 multi=5.0029 len=1843
error length < 2000bp
QZd1-k141_27586 flag=1 multi=6.0000 len=1655
error length < 2000bp
LJd1-k141_117871 flag=1 multi=8.0000 len=1678
error length < 2000bp
SZd1-k141_245279 flag=3 multi=24.0000 len=1915
error length < 2000bp
SZd1-k141_347272 flag=3 multi=146.0000 len=1455
error length < 2000bp
LJd1-k141_64842 flag=1 multi=309.1373 len=1918
error length < 2000bp
ML1-k141_25679 flag=1 multi=9.0000 len=1674
error length < 2000bp
ML1-k141_35077 flag=3 multi=46.0283 len=1768
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_106.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_110.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_12.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_122.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_125.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_129.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_131.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_136.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_139.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_147.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_149.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_152.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_153.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_162.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_165.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_181.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_185.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_189.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_206.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_212.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_215.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_217.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_229.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_233.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_240.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_244.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_247.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_252.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_253.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_254.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_255.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_258.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_260.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_263.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_266.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_268.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_271.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_272.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_273.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_278.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_280.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_283.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_284.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_286.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_287.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_29.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_292.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_293.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_296.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_299.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_30.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_300.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_301.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_304.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_306.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_311.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_312.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_313.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_315.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_326.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_328.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_331.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_335.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_336.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_337.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_339.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_341.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_342.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_344.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_345.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_346.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_347.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_35.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_354.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_355.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_356.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_358.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_359.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_36.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_363.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_364.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_365.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_369.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_370.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_375.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_377.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_378.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_380.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_383.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_397.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_398.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_40.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_401.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_419.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_421.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_430.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_442.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_443.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_454.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_473.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_479.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_486.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_489.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_49.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_503.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_505.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_507.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_510.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_513.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_52.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_521.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_527.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_528.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_53.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_545.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_567.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_571.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_572.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_576.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_579.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_580.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_581.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_583.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_584.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_585.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_586.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_587.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_588.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_590.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_592.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_593.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_603.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_604.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_609.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_61.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_615.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_620.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_623.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_627.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_636.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_638.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_639.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_640.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_641.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_647.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_652.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_655.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_656.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_657.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_659.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_663.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_665.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_666.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_67.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_670.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_673.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_676.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_678.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_681.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_682.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_683.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_689.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_691.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_692.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_694.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_696.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_699.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_709.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_711.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_714.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_716.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_721.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_722.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_726.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_73.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_730.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_731.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_733.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_739.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_743.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_745.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_748.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_749.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_759.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_76.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_761.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_782.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_785.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_787.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_788.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_789.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_790.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_791.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_792.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_793.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_794.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_795.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_796.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_797.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_806.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_816.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_817.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_838.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_84.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_841.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_85.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_878.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_880.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_89.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_91.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_950.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_962.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_106.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
ZHd2-k141_139156 flag=1 multi=5.0000 len=1635
error length < 2000bp
ZHd1-k141_475385 flag=1 multi=3.0000 len=1852
error length < 2000bp
ZHd2-k141_122687 flag=1 multi=26.0000 len=1987
error length < 2000bp
YJd1-k141_350053 flag=3 multi=5.0030 len=1823
error length < 2000bp
YJr1-k141_92537 flag=1 multi=430.4231 len=1987
error length < 2000bp
YJr1-k141_137666 flag=0 multi=15.7977 len=1881
error length < 2000bp
YJd1-k141_269828 flag=1 multi=33.0000 len=1848
error length < 2000bp
TWr1-k141_36554 flag=3 multi=15.0087 len=1867
error length < 2000bp
TWd1-k141_42432 flag=1 multi=24.0000 len=1807
error length < 2000bp
YJr1-k141_179478 flag=3 multi=15.7891 len=1706
error length < 2000bp
YJr1-k141_114211 flag=3 multi=8.8952 len=1925
error length < 2000bp
YJr1-k141_252474 flag=3 multi=12.0070 len=1849
error length < 2000bp
YJr1-k141_171723 flag=1 multi=3.0000 len=1983
error length < 2000bp
YJd1-k141_141357 flag=1 multi=79.0000 len=1895
error length < 2000bp
YJr1-k141_210156 flag=3 multi=48.0000 len=1907
error length < 2000bp
YJd1-k141_359335 flag=3 multi=12.0072 len=1811
error length < 2000bp
YJr2-k141_38208 flag=3 multi=12.0000 len=1942
error length < 2000bp
YJr1-k141_249778 flag=3 multi=6.0035 len=1845
error length < 2000bp
YJr1-k141_252246 flag=3 multi=3.0020 len=1609
error length < 2000bp
VIs1-k141_142764 flag=3 multi=42.0276 len=1664
error length < 2000bp
YJr1-k141_22530 flag=1 multi=4.0000 len=1631
error length < 2000bp
ZHd2-k141_122746 flag=1 multi=585.9184 len=1722
error length < 2000bp
YJr1-k141_187426 flag=1 multi=4.0000 len=1963
error length < 2000bp
YJr1-k141_229727 flag=0 multi=12.9238 len=1846
error length < 2000bp
TWr1-k141_36183 flag=3 multi=61.0331 len=1985
error length < 2000bp
YJd1-k141_35142 flag=1 multi=8.6968 len=1467
error length < 2000bp
TWr1-k141_13722 flag=3 multi=113.0000 len=1801
error length < 2000bp
YJr1-k141_16749 flag=0 multi=3.9480 len=1833
error length < 2000bp
TWd1-k141_59247 flag=3 multi=10.0000 len=1784
error length < 2000bp
YJr1-k141_250536 flag=3 multi=11.0066 len=1818
error length < 2000bp
ZHd2-k141_24353 flag=3 multi=61.0000 len=1989
error length < 2000bp
YJr1-k141_55439 flag=3 multi=48.0000 len=1972
error length < 2000bp
TWr1-k141_35990 flag=3 multi=30.0175 len=1853
error length < 2000bp
TWr1-k141_35716 flag=3 multi=5.0030 len=1803
error length < 2000bp
TWr1-k141_36777 flag=3 multi=4.0024 len=1784
error length < 2000bp
TWr1-k141_36074 flag=3 multi=12.8396 len=1756
error length < 2000bp
YJr1-k141_250401 flag=3 multi=24.0129 len=1998
error length < 2000bp
TWd1-k141_40310 flag=1 multi=4.0000 len=1679
error length < 2000bp
YJr2-k141_38532 flag=1 multi=6.0000 len=1667
error length < 2000bp
YJr1-k141_60677 flag=1 multi=4.0000 len=1691
error length < 2000bp
ZHd1-k141_456548 flag=1 multi=3.0000 len=1910
error length < 2000bp
YJr1-k141_57040 flag=3 multi=102.0000 len=1833
error length < 2000bp
T6S1-k141_92883 flag=1 multi=5.0000 len=1938
error length < 2000bp
TWd1-k141_18394 flag=0 multi=159.1107 len=1370
error length < 2000bp
YJr1-k141_2386 flag=3 multi=29.0000 len=1851
error length < 2000bp
YJd1-k141_353350 flag=3 multi=4.0026 len=1666
error length < 2000bp
YJr1-k141_236478 flag=3 multi=20.0000 len=1814
error length < 2000bp
YJr1-k141_96836 flag=0 multi=4.4339 len=1872
error length < 2000bp
YJd1-k141_88973 flag=1 multi=3.8708 len=1813
error length < 2000bp
T6S1-k141_191909 flag=0 multi=20.7377 len=1681
error length < 2000bp
YJd1-k141_261225 flag=0 multi=33.5644 len=1858
error length < 2000bp
YJd1-k141_158803 flag=0 multi=12.0000 len=1987
error length < 2000bp
TWr1-k141_23407 flag=3 multi=6.0000 len=1838
error length < 2000bp
YJr1-k141_252454 flag=3 multi=7.0039 len=1925
error length < 2000bp
TWd1-k141_61932 flag=3 multi=9.0051 len=1900
error length < 2000bp
YJr1-k141_99972 flag=3 multi=32.0000 len=1924
error length < 2000bp
YJr1-k141_150021 flag=1 multi=4.6200 len=1854
error length < 2000bp
YJr1-k141_121480 flag=3 multi=4.7018 len=1841
error length < 2000bp
YJr1-k141_252501 flag=3 multi=49.0284 len=1867
error length < 2000bp
YJr2-k141_17687 flag=3 multi=15.0000 len=1819
error length < 2000bp
YJr1-k141_83831 flag=1 multi=2.0000 len=1760
error length < 2000bp
YJr2-k141_17543 flag=1 multi=34.0868 len=1627
error length < 2000bp
TWr1-k141_8334 flag=1 multi=5.0000 len=1898
error length < 2000bp
YJd1-k141_217580 flag=0 multi=21.4718 len=1898
error length < 2000bp
TWr1-k141_36502 flag=3 multi=10.0059 len=1835
error length < 2000bp
YJd1-k141_344351 flag=3 multi=12.0076 len=1724
error length < 2000bp
T8S1-k141_227461 flag=1 multi=10.0000 len=1962
error length < 2000bp
TWr1-k141_35838 flag=3 multi=35.0233 len=1642
error length < 2000bp
YJd1-k141_56913 flag=3 multi=64.0000 len=1888
error length < 2000bp
TWd1-k141_62390 flag=3 multi=13.0080 len=1772
error length < 2000bp
TWr1-k141_36837 flag=3 multi=7.0040 len=1900
error length < 2000bp
YJr1-k141_246412 flag=0 multi=17.7765 len=1913
error length < 2000bp
ZHd1-k141_56669 flag=1 multi=446.1068 len=1892
error length < 2000bp
TWr1-k141_15140 flag=1 multi=4.0000 len=1774
error length < 2000bp
YJr1-k141_80902 flag=1 multi=3.0000 len=1994
error length < 2000bp
YJr1-k141_253687 flag=3 multi=18.0107 len=1825
error length < 2000bp
YJr1-k141_249596 flag=3 multi=7.0040 len=1902
error length < 2000bp
ZHd1-k141_81485 flag=0 multi=5.6973 len=1945
error length < 2000bp
TWd1-k141_1610 flag=0 multi=2.1164 len=1937
error length < 2000bp
T5S1-k141_742178 flag=3 multi=7.0040 len=1909
error length < 2000bp
YJd1-k141_2101 flag=1 multi=5.0000 len=1910
error length < 2000bp
ZHd1-k141_568925 flag=3 multi=18.0102 len=1898
error length < 2000bp
YJr1-k141_253208 flag=3 multi=26.0153 len=1843
error length < 2000bp
YJd1-k141_196960 flag=1 multi=9.5824 len=1827
error length < 2000bp
TWd1-k141_63267 flag=3 multi=7.0039 len=1932
error length < 2000bp
TWd1-k141_63388 flag=3 multi=41.0239 len=1856
error length < 2000bp
YJr1-k141_177468 flag=3 multi=9.0000 len=1838
error length < 2000bp
YJr1-k141_89168 flag=1 multi=5.0000 len=1906
error length < 2000bp
ZHd2-k141_165370 flag=0 multi=14.2072 len=1898
error length < 2000bp
ZHd2-k141_69012 flag=1 multi=86.0000 len=1708
error length < 2000bp
TWd1-k141_21669 flag=1 multi=2.0000 len=1874
error length < 2000bp
YJr1-k141_251804 flag=3 multi=3.0019 len=1728
error length < 2000bp
TWd1-k141_56701 flag=1 multi=3.0000 len=1974
error length < 2000bp
YJd1-k141_343056 flag=3 multi=44.0275 len=1743
error length < 2000bp
YJr1-k141_249616 flag=3 multi=7.9090 len=1745
error length < 2000bp
TWd1-k141_63731 flag=3 multi=9.0055 len=1774
error length < 2000bp
TWr1-k141_36086 flag=3 multi=58.0366 len=1725
error length < 2000bp
YJd1-k141_39474 flag=3 multi=91.0000 len=1653
error length < 2000bp
YJr1-k141_5066 flag=0 multi=5.9247 len=1601
error length < 2000bp
TWd1-k141_62286 flag=3 multi=156.0844 len=1990
error length < 2000bp
YJd1-k141_119502 flag=0 multi=46.6106 len=1977
error length < 2000bp
YJd1-k141_225551 flag=0 multi=11.8533 len=1791
error length < 2000bp
TWr1-k141_36304 flag=3 multi=26.0143 len=1958
error length < 2000bp
YJr1-k141_38384 flag=3 multi=14.0000 len=1741
error length < 2000bp
YJr1-k141_251811 flag=3 multi=4.0022 len=1937
error length < 2000bp
T8S1-k141_12276 flag=3 multi=13.8363 len=1912
error length < 2000bp
YJr1-k141_165605 flag=3 multi=16.0000 len=1871
error length < 2000bp
T5S1-k141_155765 flag=3 multi=71.3790 len=1666
error length < 2000bp
YJr1-k141_249903 flag=3 multi=10.0066 len=1662
error length < 2000bp
YJr1-k141_175685 flag=0 multi=71.1559 len=1937
error length < 2000bp
YJr3-k141_5201 flag=1 multi=2.6592 len=1664
error length < 2000bp
YJr1-k141_250894 flag=3 multi=3.0016 len=1996
error length < 2000bp
YJr1-k141_251171 flag=3 multi=3.0018 len=1824
error length < 2000bp
YJr1-k141_252185 flag=3 multi=4.0022 len=1976
error length < 2000bp
ZHd1-k141_575323 flag=3 multi=17.0106 len=1743
error length < 2000bp
ZHd1-k141_6866 flag=0 multi=41.4456 len=1768
error length < 2000bp
YJr1-k141_250437 flag=3 multi=4.0024 len=1807
error length < 2000bp
YJr2-k141_51061 flag=3 multi=12.0000 len=1837
error length < 2000bp
YJr1-k141_251409 flag=3 multi=32.0174 len=1978
error length < 2000bp
YJd1-k141_300671 flag=1 multi=14.0000 len=1908
error length < 2000bp
YJd1-k141_159191 flag=1 multi=4.8045 len=1844
error length < 2000bp
YJr1-k141_250571 flag=3 multi=16.0105 len=1666
error length < 2000bp
T5S1-k141_500999 flag=1 multi=4.6975 len=1989
error length < 2000bp
YJd1-k141_341566 flag=3 multi=7.0041 len=1828
error length < 2000bp
TWr1-k141_35781 flag=3 multi=20.0117 len=1852
error length < 2000bp
YJr1-k141_222463 flag=0 multi=6.4959 len=1724
error length < 2000bp
TWr1-k141_34910 flag=0 multi=15.8398 len=1926
error length < 2000bp
YJr1-k141_250706 flag=3 multi=10.0062 len=1762
error length < 2000bp
YJd1-k141_45178 flag=1 multi=27.9738 len=1708
error length < 2000bp
YJr1-k141_188030 flag=0 multi=2.4307 len=1541
error length < 2000bp
YJr1-k141_8303 flag=0 multi=14.0562 len=1920
error length < 2000bp
YJr1-k141_253145 flag=3 multi=6.0036 len=1829
error length < 2000bp
YJr1-k141_250655 flag=3 multi=13.0079 len=1785
error length < 2000bp
T6S1-k141_104719 flag=1 multi=4.8226 len=1928
error length < 2000bp
ZHd2-k141_135936 flag=0 multi=2.6398 len=1640
error length < 2000bp
YJd1-k141_332403 flag=3 multi=8.0047 len=1850
error length < 2000bp
TWd1-k141_64310 flag=3 multi=22.0125 len=1903
error length < 2000bp
YJr1-k141_69900 flag=0 multi=73.9258 len=1677
error length < 2000bp
TWr1-k141_35758 flag=3 multi=52.0321 len=1762
error length < 2000bp
YJr1-k141_250346 flag=3 multi=7.0038 len=1981
error length < 2000bp
TWr1-k141_36309 flag=3 multi=9.0055 len=1788
error length < 2000bp
TWd1-k141_62064 flag=3 multi=11.0072 len=1679
error length < 2000bp
ZHd2-k141_171325 flag=3 multi=4.0024 len=1837
error length < 2000bp
YJr1-k141_250239 flag=3 multi=13.0081 len=1740
error length < 2000bp
YJr2-k141_28666 flag=3 multi=14.0000 len=1868
error length < 2000bp
YJr1-k141_236069 flag=0 multi=4.5043 len=1650
error length < 2000bp
YJr1-k141_18865 flag=0 multi=1.8663 len=1951
error length < 2000bp
TWr1-k141_36612 flag=3 multi=17.0104 len=1770
error length < 2000bp
YJr1-k141_90386 flag=3 multi=82.0000 len=1914
error length < 2000bp
YJr1-k141_65560 flag=3 multi=21.7128 len=1760
error length < 2000bp
T6S2-k141_208046 flag=1 multi=9.0000 len=1910
error length < 2000bp
YJd1-k141_362292 flag=3 multi=7.0039 len=1917
error length < 2000bp
YJr2-k141_21866 flag=3 multi=21.0000 len=1682
error length < 2000bp
TWr1-k141_36581 flag=3 multi=30.0190 len=1719
error length < 2000bp
YJr1-k141_251193 flag=3 multi=13.0088 len=1613
error length < 2000bp
YJr1-k141_252509 flag=3 multi=13.0078 len=1817
error length < 2000bp
ZHd1-k141_568809 flag=3 multi=1018.5640 len=1946
error length < 2000bp
YJr1-k141_25539 flag=0 multi=1.9924 len=1851
error length < 2000bp
TWr1-k141_35783 flag=3 multi=56.0325 len=1865
error length < 2000bp
YJr1-k141_252060 flag=3 multi=4.0026 len=1672
error length < 2000bp
YJr1-k141_46480 flag=1 multi=5.0000 len=1644
error length < 2000bp
YJr1-k141_239854 flag=1 multi=4.0000 len=1982
error length < 2000bp
ZHd1-k141_356752 flag=3 multi=64.0000 len=1948
error length < 2000bp
TWd1-k141_63012 flag=3 multi=56.0307 len=1963
error length < 2000bp
T5S1-k141_742636 flag=3 multi=18.0103 len=1889
error length < 2000bp
T6S1-k141_565711 flag=3 multi=15.0086 len=1888
error length < 2000bp
ZHd1-k141_575006 flag=3 multi=5.0027 len=1964
error length < 2000bp
T5S1-k141_739833 flag=3 multi=5.0028 len=1927
error length < 2000bp
ZHd2-k141_13020 flag=1 multi=2.6322 len=1922
error length < 2000bp
YJr2-k141_61473 flag=3 multi=26.0164 len=1730
error length < 2000bp
TWr1-k141_36091 flag=3 multi=5.0031 len=1767
error length < 2000bp
TWr1-k141_35789 flag=3 multi=6.0033 len=1954
error length < 2000bp
YJr1-k141_249493 flag=3 multi=6.0040 len=1638
error length < 2000bp
YJd1-k141_183304 flag=1 multi=147.6505 len=1786
error length < 2000bp
YJr1-k141_251564 flag=3 multi=6.0035 len=1836
error length < 2000bp
YJd1-k141_221093 flag=0 multi=34.0000 len=1949
error length < 2000bp
YJr1-k141_252466 flag=3 multi=30.0193 len=1699
error length < 2000bp
TWr1-k141_23132 flag=1 multi=9.0000 len=1807
error length < 2000bp
YJd1-k141_344786 flag=3 multi=5.0028 len=1923
error length < 2000bp
YJr1-k141_251198 flag=3 multi=14.0093 len=1650
error length < 2000bp
YJr1-k141_157573 flag=0 multi=4.9290 len=1691
error length < 2000bp
YJd1-k141_350149 flag=3 multi=8.0050 len=1741
error length < 2000bp
TWr1-k141_36420 flag=3 multi=25.0164 len=1667
error length < 2000bp
YJr1-k141_249589 flag=3 multi=14.0084 len=1812
error length < 2000bp
YJr1-k141_222431 flag=3 multi=15.0000 len=1832
error length < 2000bp
YJd1-k141_234989 flag=0 multi=8.8972 len=1853
error length < 2000bp
YJr3-k141_16914 flag=0 multi=137.1425 len=1924
error length < 2000bp
YJr1-k141_249335 flag=3 multi=21.0119 len=1901
error length < 2000bp
YJd1-k141_328622 flag=3 multi=15.0087 len=1864
error length < 2000bp
T5S1-k141_399595 flag=1 multi=8.0000 len=1833
error length < 2000bp
T6S1-k141_406540 flag=0 multi=5.8297 len=1903
error length < 2000bp
YJr3-k141_6418 flag=3 multi=24.0000 len=1881
error length < 2000bp
YJd1-k141_325741 flag=0 multi=20.5460 len=1859
error length < 2000bp
T6S1-k141_549192 flag=3 multi=15.0088 len=1844
error length < 2000bp
T6S1-k141_548329 flag=3 multi=43.0242 len=1918
error length < 2000bp
YJr1-k141_249540 flag=3 multi=8.0045 len=1934
error length < 2000bp
YJr1-k141_249637 flag=3 multi=9.0055 len=1770
error length < 2000bp
YJr1-k141_252949 flag=3 multi=9.0051 len=1892
error length < 2000bp
YJr1-k141_129161 flag=3 multi=4.7840 len=1854
error length < 2000bp
YJd1-k141_345402 flag=3 multi=19.0111 len=1859
error length < 2000bp
YJd1-k141_94365 flag=1 multi=18.0000 len=1993
error length < 2000bp
YJr1-k141_159693 flag=3 multi=5.8707 len=1843
error length < 2000bp
YJr1-k141_98695 flag=3 multi=16.0000 len=1980
error length < 2000bp
TWr1-k141_36224 flag=3 multi=68.0393 len=1870
error length < 2000bp
YJd1-k141_340934 flag=3 multi=21.0135 len=1691
error length < 2000bp
TWd1-k141_62544 flag=3 multi=22.0132 len=1810
error length < 2000bp
YJr1-k141_253525 flag=3 multi=4.0023 len=1902
error length < 2000bp
YJr1-k141_111120 flag=1 multi=3.0000 len=1907
error length < 2000bp
T6S1-k141_262961 flag=1 multi=48.6081 len=1904
error length < 2000bp
VIh1-k141_91094 flag=1 multi=8.0000 len=1926
error length < 2000bp
YJr1-k141_253508 flag=3 multi=7.8963 len=1645
error length < 2000bp
TWr1-k141_36348 flag=3 multi=8.0049 len=1771
error length < 2000bp
YJr1-k141_36539 flag=0 multi=6.0000 len=1989
error length < 2000bp
YJr1-k141_142184 flag=0 multi=3.3800 len=1599
error length < 2000bp
YJr1-k141_108531 flag=3 multi=10.0000 len=1790
error length < 2000bp
YJr3-k141_30327 flag=1 multi=5.3908 len=1881
error length < 2000bp
YJr1-k141_250658 flag=3 multi=4.0023 len=1849
error length < 2000bp
YJd1-k141_160499 flag=3 multi=86.0000 len=1901
error length < 2000bp
YJr1-k141_251630 flag=3 multi=29.0174 len=1810
error length < 2000bp
YJr1-k141_226539 flag=1 multi=3.0000 len=1735
error length < 2000bp
TWr1-k141_36198 flag=3 multi=14.0085 len=1788
error length < 2000bp
YJr1-k141_251721 flag=3 multi=10.0055 len=1973
error length < 2000bp
YJd1-k141_45335 flag=1 multi=5.4295 len=1929
error length < 2000bp
TWd1-k141_63190 flag=3 multi=13.0070 len=1989
error length < 2000bp
YJd1-k141_335360 flag=3 multi=19.0106 len=1933
error length < 2000bp
TWd1-k141_62141 flag=3 multi=13.0073 len=1930
error length < 2000bp
YJr1-k141_119732 flag=0 multi=22.4155 len=1797
error length < 2000bp
YJr1-k141_252645 flag=3 multi=12.0075 len=1742
error length < 2000bp
TWr1-k141_36969 flag=3 multi=52.0317 len=1779
error length < 2000bp
YJr1-k141_133071 flag=3 multi=16.0000 len=1990
error length < 2000bp
TWr1-k141_35806 flag=3 multi=7.9169 len=1898
error length < 2000bp
YJr1-k141_175508 flag=3 multi=23.0000 len=1825
error length < 2000bp
YJr1-k141_4880 flag=3 multi=81.0000 len=1605
error length < 2000bp
TWd1-k141_63042 flag=3 multi=15.0081 len=1992
error length < 2000bp
T5S1-k141_739299 flag=3 multi=24.0129 len=1995
error length < 2000bp
YJr1-k141_253118 flag=3 multi=16.0104 len=1674
error length < 2000bp
TWr1-k141_5758 flag=1 multi=18.0000 len=1956
error length < 2000bp
TWr1-k141_19320 flag=0 multi=27.4448 len=1843
error length < 2000bp
YJd1-k141_329657 flag=3 multi=11.0068 len=1756
error length < 2000bp
TWd1-k141_63489 flag=3 multi=63.0375 len=1821
error length < 2000bp
YJr1-k141_139827 flag=3 multi=43.0000 len=1876
error length < 2000bp
TWr1-k141_35942 flag=3 multi=6.0036 len=1816
error length < 2000bp
T5S1-k141_382578 flag=1 multi=3.0000 len=1392
error length < 2000bp
TWr1-k141_36140 flag=3 multi=9.0056 len=1741
error length < 2000bp
YJr1-k141_249877 flag=3 multi=3.0018 len=1781
error length < 2000bp
T8S2-k141_117001 flag=3 multi=67.0000 len=1994
error length < 2000bp
T6S1-k141_221113 flag=0 multi=17.6859 len=1669
error length < 2000bp
YJr1-k141_249709 flag=3 multi=26.0148 len=1896
error length < 2000bp
YJr1-k141_83229 flag=3 multi=10.0000 len=1888
error length < 2000bp
YJd1-k141_329291 flag=3 multi=11.0065 len=1831
error length < 2000bp
ZHd1-k141_88101 flag=3 multi=116.0000 len=1942
error length < 2000bp
YJd1-k141_336715 flag=3 multi=6.0034 len=1907
error length < 2000bp
YJr1-k141_180466 flag=3 multi=44.0000 len=1869
error length < 2000bp
TWd1-k141_63140 flag=3 multi=27.0175 len=1680
error length < 2000bp
YJr1-k141_131644 flag=3 multi=12.0000 len=1818
error length < 2000bp
TWd1-k141_62731 flag=3 multi=6.0036 len=1787
error length < 2000bp
YJr1-k141_183359 flag=1 multi=7.0000 len=1608
error length < 2000bp
TWr1-k141_35905 flag=3 multi=18.0120 len=1640
error length < 2000bp
YJr1-k141_250394 flag=3 multi=7.0040 len=1875
error length < 2000bp
TWd3-k141_10518 flag=1 multi=8.8968 len=1943
error length < 2000bp
YJd1-k141_334459 flag=3 multi=67.0394 len=1843
error length < 2000bp
TWd1-k141_59186 flag=1 multi=4.0000 len=1971
error length < 2000bp
YJr1-k141_249900 flag=3 multi=42.0240 len=1893
error length < 2000bp
YJr1-k141_253073 flag=3 multi=15.0089 len=1824
error length < 2000bp
TWd1-k141_55167 flag=1 multi=10.7775 len=1939
error length < 2000bp
YJd1-k141_247424 flag=1 multi=29.3258 len=1832
error length < 2000bp
YJr1-k141_252753 flag=3 multi=8.0043 len=1999
error length < 2000bp
YJr1-k141_131284 flag=0 multi=4.0000 len=1828
error length < 2000bp
T8S1-k141_172337 flag=1 multi=38.0000 len=1899
error length < 2000bp
YJr1-k141_249908 flag=3 multi=51.0321 len=1730
error length < 2000bp
YJd1-k141_218708 flag=1 multi=174.2972 len=1948
error length < 2000bp
YJd1-k141_331609 flag=3 multi=18.0106 len=1833
error length < 2000bp
YJd1-k141_66415 flag=3 multi=31.0000 len=1812
error length < 2000bp
TWd1-k141_43813 flag=1 multi=20.0000 len=1944
error length < 2000bp
YJd1-k141_284524 flag=1 multi=6.0000 len=1734
error length < 2000bp
YJr1-k141_149924 flag=0 multi=8.0000 len=1710
error length < 2000bp
YJd1-k141_374311 flag=3 multi=54.0318 len=1840
error length < 2000bp
YJr1-k141_211490 flag=0 multi=2.8935 len=1568
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_104.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_106.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_12.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_123.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_129.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_130.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_132.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_133.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_134.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_142.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_144.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_151.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_152.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_156.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_193.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_197.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_201.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_212.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_225.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_228.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_237.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_238.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_24.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_243.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_246.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_25.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_254.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_258.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_266.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_271.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_277.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_278.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_279.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_280.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_287.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_293.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_294.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_295.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_30.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_300.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_302.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_307.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_309.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_314.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_320.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_323.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_325.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_327.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_333.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_336.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_337.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_338.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_339.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_342.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_345.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_347.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_350.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_353.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_354.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_355.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_356.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_357.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_358.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_360.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_367.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_368.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_369.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_370.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_372.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_375.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_376.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_377.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_378.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_380.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_385.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_387.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_390.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_394.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_395.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_396.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_397.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_399.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_400.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_407.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_436.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_476.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_482.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_488.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_495.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_496.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_497.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_5.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_505.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_506.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_507.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_509.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_513.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_516.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_517.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_524.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_529.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_533.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_534.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_537.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_540.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_543.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_545.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_546.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_549.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_553.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_557.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_558.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_567.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_568.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_572.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_575.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_578.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_589.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_591.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_592.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_593.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_596.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_600.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_602.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_604.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_609.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_614.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_617.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_621.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_622.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_623.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_627.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_628.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_629.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_632.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_636.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_638.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_640.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_644.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_646.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_647.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_653.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_656.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_657.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_658.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_659.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_660.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_661.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_662.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_663.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_664.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_667.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_668.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_671.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_673.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_674.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_676.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_678.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_681.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_682.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_687.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_688.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_690.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_692.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_693.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_695.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_696.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_701.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_704.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_707.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_708.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_709.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_710.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_711.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_712.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_713.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_716.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_717.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_718.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_719.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_720.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_723.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_724.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_726.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_728.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_731.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_733.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_735.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_736.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_738.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_740.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_743.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_744.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_747.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_753.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_758.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_760.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_764.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_765.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_766.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_767.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_768.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_769.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_771.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_778.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_781.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_783.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_785.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_788.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_789.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_791.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_792.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_794.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_797.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_798.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_800.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_801.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_802.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_805.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_807.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_808.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_815.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_817.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_818.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_819.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_820.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_821.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_822.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_826.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_832.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_833.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_834.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_838.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_839.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_841.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_842.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_844.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_847.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_852.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_854.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_856.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_857.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_859.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_860.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_861.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_862.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_863.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_864.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_865.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_866.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_868.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_869.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_871.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_872.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_874.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_875.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_876.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_900.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_901.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_905.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_907.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_908.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_914.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_915.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_916.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_918.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_93.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_941.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_948.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_949.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_969.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_973.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_975.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_978.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_986.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_991.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_994.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_995.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_104.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
error length < 2000bp
ZHr1-k141_49769 flag=3 multi=36.0213 len=1835
error length < 2000bp
ZHd1-k141_120592 flag=3 multi=12.0000 len=1968
error length < 2000bp
ZHd1-k141_244775 flag=0 multi=3.6362 len=1900
error length < 2000bp
ZHd1-k141_11853 flag=1 multi=2.0000 len=1774
error length < 2000bp
ZHr1-k141_49246 flag=3 multi=21.0118 len=1921
error length < 2000bp
ZHd1-k141_570709 flag=3 multi=35.0208 len=1822
error length < 2000bp
ZHd1-k141_572468 flag=3 multi=5.0030 len=1785
error length < 2000bp
ZHd1-k141_103500 flag=1 multi=3.0000 len=1814
error length < 2000bp
ZHd1-k141_572518 flag=3 multi=5.2443 len=1860
error length < 2000bp
all-k141_4057734 flag=1 multi=3.9288 len=1784
error length < 2000bp
ZHd1-k141_311630 flag=0 multi=16.0035 len=1880
error length < 2000bp
ZHd1-k141_27420 flag=3 multi=50.0000 len=1944
error length < 2000bp
ZHd1-k141_402308 flag=1 multi=2.4576 len=1590
error length < 2000bp
ZHd1-k141_580510 flag=3 multi=3.0020 len=1626
error length < 2000bp
all-k141_1434385 flag=0 multi=26.6716 len=1910
error length < 2000bp
ZHd1-k141_155098 flag=0 multi=14.4400 len=1891
error length < 2000bp
ZHr1-k141_49320 flag=3 multi=5.0028 len=1907
error length < 2000bp
all-k141_153144 flag=0 multi=10.7721 len=1782
error length < 2000bp
ZHr1-k141_45832 flag=1 multi=4.0000 len=1721
error length < 2000bp
os1-k141_29985 flag=0 multi=13.0000 len=1960
error length < 2000bp
ZHd1-k141_20069 flag=0 multi=17.7935 len=1952
error length < 2000bp
ZHd1-k141_185953 flag=3 multi=94.2777 len=1916
error length < 2000bp
ZHd1-k141_117746 flag=1 multi=2.0000 len=1890
error length < 2000bp
ZHr1-k141_3956 flag=1 multi=4.0000 len=1528
error length < 2000bp
ZHr1-k141_50329 flag=3 multi=4.0024 len=1811
error length < 2000bp
ZHr1-k141_51095 flag=3 multi=24.0130 len=1982
error length < 2000bp
ZHd1-k141_210047 flag=3 multi=8.8976 len=1859
error length < 2000bp
all-k141_1995823 flag=1 multi=5.0000 len=1768
error length < 2000bp
ZHd1-k141_553354 flag=0 multi=5.9230 len=1856
error length < 2000bp
ZHd1-k141_280142 flag=3 multi=14.0000 len=1206
error length < 2000bp
ZHd1-k141_167827 flag=3 multi=31.0000 len=1959
error length < 2000bp
ZHr1-k141_50502 flag=3 multi=7.0047 len=1643
error length < 2000bp
ZHd1-k141_79249 flag=3 multi=28.0000 len=1940
error length < 2000bp
ZHd1-k141_199565 flag=1 multi=10.0000 len=1869
error length < 2000bp
ZHd1-k141_566500 flag=3 multi=14.0075 len=1996
error length < 2000bp
ZHd1-k141_296829 flag=3 multi=221.0000 len=1831
error length < 2000bp
ZHd1-k141_367353 flag=3 multi=35.0000 len=1958
error length < 2000bp
ZHd1-k141_361682 flag=0 multi=4.8874 len=1899
error length < 2000bp
os1-k141_40473 flag=3 multi=7.0038 len=1987
error length < 2000bp
ZHd1-k141_332628 flag=1 multi=4.0000 len=1754
error length < 2000bp
ZHd1-k141_297629 flag=3 multi=23.0000 len=1966
error length < 2000bp
ZHr1-k141_10809 flag=0 multi=58.8701 len=1850
error length < 2000bp
ZHd1-k141_322798 flag=3 multi=21.0000 len=1889
error length < 2000bp
ZHd1-k141_574116 flag=3 multi=5.0029 len=1877
error length < 2000bp
ZHr1-k141_49994 flag=3 multi=7.0040 len=1905
error length < 2000bp
ZHd1-k141_140242 flag=3 multi=41.0000 len=1756
error length < 2000bp
ZHr1-k141_49158 flag=3 multi=27.0150 len=1942
error length < 2000bp
all-k141_898790 flag=1 multi=3.0000 len=1918
error length < 2000bp
ZHd1-k141_567517 flag=3 multi=5.0027 len=1963
error length < 2000bp
all-k141_355490 flag=1 multi=3.0000 len=1820
error length < 2000bp
ZHd1-k141_275917 flag=0 multi=9.0000 len=1949
error length < 2000bp
ZHd1-k141_554233 flag=1 multi=5.5434 len=1937
error length < 2000bp
ZHr1-k141_33697 flag=1 multi=56.0000 len=1937
error length < 2000bp
ZHd1-k141_173312 flag=0 multi=5.9274 len=1960
error length < 2000bp
os1-k141_22228 flag=0 multi=12.0000 len=1979
error length < 2000bp
ZHd1-k141_10601 flag=0 multi=58.9083 len=1994
error length < 2000bp
ZHd1-k141_502474 flag=3 multi=5.9348 len=1829
error length < 2000bp
ZHd1-k141_158818 flag=3 multi=143.0000 len=1897
error length < 2000bp
ZHd1-k141_429091 flag=0 multi=14.9084 len=1800
error length < 2000bp
ZHd1-k141_20681 flag=0 multi=274.7517 len=1921
error length < 2000bp
all-k141_4112725 flag=1 multi=4.0000 len=1967
error length < 2000bp
ZHd1-k141_301477 flag=1 multi=3.0000 len=1668
error length < 2000bp
ZHd1-k141_221409 flag=1 multi=6.5077 len=1896
error length < 2000bp
ZHd1-k141_570699 flag=3 multi=4.8916 len=1921
error length < 2000bp
ZHd1-k141_581979 flag=3 multi=13.0089 len=1603
error length < 2000bp
ZHd1-k141_145117 flag=3 multi=214.0000 len=1851
error length < 2000bp
ZHd1-k141_577589 flag=3 multi=8.0044 len=1950
error length < 2000bp
ZHd1-k141_72066 flag=1 multi=7.0584 len=1869
error length < 2000bp
ZHr1-k141_49852 flag=3 multi=12.0076 len=1727
error length < 2000bp
ZHr1-k141_49180 flag=3 multi=20.0119 len=1826
error length < 2000bp
all-k141_582997 flag=1 multi=13.0000 len=1872
error length < 2000bp
ZHd1-k141_547231 flag=3 multi=134.0000 len=1821
error length < 2000bp
ZHd1-k141_395962 flag=3 multi=101.0000 len=1816
error length < 2000bp
ZHd1-k141_197337 flag=1 multi=3.7511 len=1901
error length < 2000bp
all-k141_1652281 flag=1 multi=16.0000 len=1894
error length < 2000bp
ZHd1-k141_573207 flag=3 multi=11.0061 len=1937
error length < 2000bp
ZHd1-k141_33967 flag=3 multi=22.0000 len=1880
error length < 2000bp
ZHd1-k141_250927 flag=0 multi=5.7537 len=1879
error length < 2000bp
ZHd1-k141_158969 flag=3 multi=123.0000 len=1997
error length < 2000bp
ZHd1-k141_33093 flag=3 multi=45.0000 len=1961
error length < 2000bp
ZHd1-k141_315647 flag=1 multi=6.0000 len=1392
error length < 2000bp
ZHd1-k141_3147 flag=3 multi=48.0000 len=1791
error length < 2000bp
ZHr1-k141_50303 flag=3 multi=4.0025 len=1772
error length < 2000bp
ZHd1-k141_202746 flag=0 multi=7.5478 len=1698
error length < 2000bp
ZHr1-k141_49238 flag=3 multi=9.0053 len=1825
error length < 2000bp
ZHd1-k141_371999 flag=1 multi=9.0000 len=1873
error length < 2000bp
os1-k141_40456 flag=3 multi=24.0144 len=1807
error length < 2000bp
ZHr1-k141_49390 flag=3 multi=29.0173 len=1817
error length < 2000bp
ZHd1-k141_366296 flag=0 multi=26.8197 len=1944
error length < 2000bp
ZHd1-k141_566242 flag=3 multi=19.0106 len=1933
error length < 2000bp
ZHd1-k141_569511 flag=3 multi=7.0039 len=1923
error length < 2000bp
all-k141_5048024 flag=3 multi=5.0028 len=1917
error length < 2000bp
ZHd1-k141_99644 flag=3 multi=70.0000 len=1879
error length < 2000bp
ZHd1-k141_584318 flag=3 multi=8.0050 len=1750
error length < 2000bp
dCh1-k141_55015 flag=3 multi=13.0078 len=1807
error length < 2000bp
ZHr1-k141_12352 flag=1 multi=9.0000 len=1744
error length < 2000bp
ZHd1-k141_353091 flag=1 multi=16.0000 len=1993
error length < 2000bp
ZHd1-k141_457905 flag=3 multi=70.0000 len=1917
error length < 2000bp
ZHd1-k141_138406 flag=0 multi=12.8434 len=1967
error length < 2000bp
ZHd1-k141_570638 flag=3 multi=19.0137 len=1525
error length < 2000bp
ZHd1-k141_145359 flag=0 multi=1.9482 len=1841
error length < 2000bp
ZHd1-k141_578616 flag=3 multi=10.0063 len=1735
error length < 2000bp
ZHr1-k141_50163 flag=3 multi=12.0076 len=1723
error length < 2000bp
all-k141_1578589 flag=1 multi=7.3949 len=1883
error length < 2000bp
ZHd1-k141_569288 flag=3 multi=15.0081 len=1992
error length < 2000bp
ZHd1-k141_576512 flag=3 multi=13.0070 len=1995
error length < 2000bp
os1-k141_40449 flag=3 multi=8.0047 len=1838
error length < 2000bp
ZHd1-k141_63659 flag=0 multi=3.0000 len=1857
error length < 2000bp
ZHr1-k141_11012 flag=0 multi=3.0000 len=1879
error length < 2000bp
all-k141_8743 flag=1 multi=5.9407 len=1997
error length < 2000bp
ZHd1-k141_575819 flag=3 multi=7.7773 len=1771
error length < 2000bp
ZHr1-k141_49526 flag=3 multi=5.0029 len=1865
error length < 2000bp
os1-k141_666 flag=1 multi=4.8625 len=1799
error length < 2000bp
ZHd1-k141_219944 flag=1 multi=3.0000 len=1758
error length < 2000bp
ZHd1-k141_247242 flag=0 multi=40.5138 len=1951
error length < 2000bp
ZHd1-k141_592397 flag=3 multi=35.0202 len=1877
error length < 2000bp
ZHr1-k141_49599 flag=3 multi=6.0032 len=1999
error length < 2000bp
ZHd1-k141_566904 flag=3 multi=7.0044 len=1738
error length < 2000bp
ZHd1-k141_527421 flag=1 multi=3.0000 len=1581
error length < 2000bp
ZHd1-k141_220676 flag=0 multi=256.0000 len=1938
error length < 2000bp
ZHr1-k141_9912 flag=0 multi=13.6786 len=1706
error length < 2000bp
os1-k141_40538 flag=3 multi=33.0184 len=1939
error length < 2000bp
ZHd1-k141_383688 flag=3 multi=10.0000 len=1890
error length < 2000bp
all-k141_4969405 flag=3 multi=3.0018 len=1776
error length < 2000bp
ZHd1-k141_277341 flag=0 multi=19.6856 len=1833
error length < 2000bp
ZHr1-k141_4686 flag=1 multi=12.9705 len=1770
error length < 2000bp
ZHd1-k141_285937 flag=0 multi=71.4108 len=1962
error length < 2000bp
ZHd1-k141_78786 flag=1 multi=5.0000 len=1869
error length < 2000bp
all-k141_1746361 flag=1 multi=4.0000 len=1749
error length < 2000bp
ZHr1-k141_38140 flag=1 multi=4.0000 len=1649
error length < 2000bp
ZHd1-k141_86158 flag=3 multi=11.0000 len=1961
error length < 2000bp
ZHd1-k141_499741 flag=3 multi=25.0000 len=1748
error length < 2000bp
all-k141_2435307 flag=1 multi=12.0374 len=1986
error length < 2000bp
os1-k141_40446 flag=3 multi=8.0047 len=1856
error length < 2000bp
ZHd1-k141_566576 flag=3 multi=9.0049 len=1994
error length < 2000bp
ZHr1-k141_50061 flag=3 multi=19.0111 len=1856
error length < 2000bp
ZHd1-k141_568159 flag=3 multi=7.0039 len=1929
error length < 2000bp
ZHr1-k141_4795 flag=1 multi=7.0000 len=1819
error length < 2000bp
ZHd1-k141_427970 flag=0 multi=4.9381 len=1918
error length < 2000bp
ZHr1-k141_49663 flag=3 multi=4.0022 len=1955
error length < 2000bp
ZHd1-k141_578421 flag=3 multi=4.0027 len=1618
error length < 2000bp
ZHr1-k141_49392 flag=3 multi=21.0125 len=1817
error length < 2000bp
ZHd1-k141_454293 flag=3 multi=10.0000 len=1923
error length < 2000bp
dCh1-k141_55014 flag=3 multi=4.0025 len=1736
error length < 2000bp
ZHr1-k141_29929 flag=1 multi=7.5674 len=1840
error length < 2000bp
ZHr1-k141_50598 flag=3 multi=6.0034 len=1908
error length < 2000bp
ZHd1-k141_576834 flag=3 multi=11.0071 len=1683
error length < 2000bp
ZHd1-k141_278253 flag=1 multi=9.5475 len=1825
error length < 2000bp
ZHr1-k141_49206 flag=3 multi=23.0143 len=1753
error length < 2000bp
all-k141_1986633 flag=1 multi=18.3407 len=1943
error length < 2000bp
ZHd1-k141_155831 flag=1 multi=3.0000 len=1960
error length < 2000bp
all-k141_2374914 flag=1 multi=8.0000 len=1989
error length < 2000bp
all-k141_1689420 flag=1 multi=10.7489 len=1981
error length < 2000bp
ZHr1-k141_12138 flag=1 multi=4.6925 len=1754
error length < 2000bp
ZHd1-k141_411423 flag=3 multi=159.0000 len=1229
error length < 2000bp
os1-k141_31563 flag=1 multi=10.0000 len=1946
error length < 2000bp
ZHd1-k141_474042 flag=1 multi=9.3586 len=1909
error length < 2000bp
ZHr1-k141_5035 flag=1 multi=2.9713 len=1746
error length < 2000bp
ZHd1-k141_569444 flag=3 multi=42.0234 len=1939
error length < 2000bp
ZHd1-k141_572637 flag=3 multi=7.0040 len=1886
error length < 2000bp
ZHd1-k141_570032 flag=3 multi=11.0068 len=1765
error length < 2000bp
ZHd1-k141_527418 flag=3 multi=80.0000 len=1808
error length < 2000bp
ZHd1-k141_254825 flag=0 multi=10.0000 len=1966
error length < 2000bp
ZHd1-k141_573959 flag=3 multi=7.0040 len=1870
error length < 2000bp
ZHd1-k141_107898 flag=3 multi=232.0000 len=1871
error length < 2000bp
ZHd1-k141_291666 flag=0 multi=11.0000 len=1864
error length < 2000bp
ZHd1-k141_21167 flag=3 multi=33.0000 len=1898
error length < 2000bp
ZHd1-k141_595430 flag=3 multi=10.0058 len=1866
error length < 2000bp
all-k141_3137706 flag=1 multi=64.8861 len=1791
error length < 2000bp
ZHd1-k141_2329 flag=3 multi=89.0000 len=1979
error length < 2000bp
ZHr1-k141_50614 flag=3 multi=6.0033 len=1957
error length < 2000bp
ZHd1-k141_436046 flag=1 multi=6.6435 len=1706
error length < 2000bp
ZHr1-k141_49159 flag=3 multi=43.0240 len=1933
error length < 2000bp
ZHd1-k141_228632 flag=3 multi=9.8232 len=1923
error length < 2000bp
ZHd1-k141_146155 flag=3 multi=81.0000 len=1909
error length < 2000bp
ZHr1-k141_12572 flag=0 multi=74.8934 len=1632
error length < 2000bp
ZHd1-k141_566465 flag=3 multi=7.0041 len=1848
error length < 2000bp
ZHd1-k141_198310 flag=3 multi=19.7477 len=1873
error length < 2000bp
ZHd1-k141_579263 flag=3 multi=3.0020 len=1638
error length < 2000bp
ZHd1-k141_46073 flag=1 multi=10.8250 len=1741
error length < 2000bp
ZHr1-k141_50372 flag=3 multi=13.0076 len=1860
error length < 2000bp
ZHr1-k141_49412 flag=3 multi=14.0085 len=1789
error length < 2000bp
ZHd1-k141_317177 flag=1 multi=18.9575 len=1834
error length < 2000bp
ZHd1-k141_14883 flag=1 multi=4.0000 len=1790
error length < 2000bp
ZHd1-k141_583020 flag=3 multi=20.7524 len=1833
error length < 2000bp
ZHr1-k141_49958 flag=3 multi=29.0174 len=1803
error length < 2000bp
ZHr1-k141_50292 flag=3 multi=31.0170 len=1960
error length < 2000bp
all-k141_2838368 flag=1 multi=2.0000 len=1437
error length < 2000bp
ZHr1-k141_49224 flag=3 multi=11.0067 len=1787
error length < 2000bp
ZHd1-k141_51766 flag=0 multi=32.0000 len=1614
error length < 2000bp
ZHd1-k141_571487 flag=3 multi=5.0027 len=1979
error length < 2000bp
ZHd1-k141_569692 flag=3 multi=44.0265 len=1800
error length < 2000bp
ZHd1-k141_451855 flag=1 multi=12.1291 len=1923
error length < 2000bp
ZHd1-k141_572644 flag=3 multi=8.2082 len=1966
error length < 2000bp
ZHr1-k141_49150 flag=3 multi=10.0055 len=1970
error length < 2000bp
os1-k141_23445 flag=1 multi=5.8532 len=1844
error length < 2000bp
ZHd1-k141_570781 flag=3 multi=7.0043 len=1788
error length < 2000bp
ZHd1-k141_568244 flag=3 multi=4.0022 len=1967
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_103.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_109.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_112.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_116.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_119.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_123.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_130.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_141.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_149.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_150.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_151.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_153.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_156.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_160.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_163.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_164.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_167.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_169.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_170.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_171.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_175.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_176.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_177.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_184.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_187.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_189.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_192.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_194.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_195.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_20.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_208.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_210.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_219.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_221.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_226.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_232.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_233.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_234.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_237.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_249.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_251.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_254.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_256.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_26.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_260.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_263.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_268.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_269.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_271.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_28.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_285.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_287.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_295.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_296.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_302.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_311.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_316.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_319.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_32.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_320.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_327.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_33.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_336.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_34.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_340.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_341.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_342.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_345.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_347.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_355.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_356.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_365.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_368.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_369.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_37.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_372.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_375.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_377.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_379.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_383.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_390.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_394.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_395.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_396.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_40.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_402.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_406.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_417.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_421.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_424.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_429.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_430.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_436.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_44.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_440.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_443.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_450.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_452.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_456.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_463.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_48.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_481.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_483.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_484.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_485.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_508.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_511.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_512.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_513.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_514.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_517.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_523.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_524.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_525.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_527.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_529.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_531.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_536.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_537.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_54.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_541.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_543.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_544.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_545.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_546.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_547.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_548.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_549.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_55.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_551.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_552.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_553.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_554.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_556.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_557.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_559.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_560.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_564.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_565.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_566.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_568.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_570.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_573.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_578.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_580.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_581.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_582.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_584.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_585.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_586.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_587.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_588.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_589.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_590.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_591.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_592.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_593.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_594.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_615.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_616.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_619.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_621.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_63.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_632.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_636.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_637.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_639.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_644.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_647.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_653.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_655.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_666.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_668.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_669.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_675.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_676.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_689.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_692.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_693.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_695.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_698.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_7.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_701.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_702.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_705.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_712.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_715.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_719.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_72.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_722.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_724.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_73.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_84.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_86.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_87.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_92.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_95.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
code/create_dataset_compress.py:18: UserWarning: genfromtxt: Empty input file: "int_val/0_96.csv"
  read = np.genfromtxt(Load_path+name, delimiter=',')
run_CNN.py:132: UserWarning: genfromtxt: Empty input file: "dataset/0_103.csv"
  val = np.genfromtxt('dataset/'+name, delimiter=',')
folder Cyber_data/ exist... cleaning dictionary
Capturing compressed features
Traceback (most recent call last):
  File "run_CNN.py", line 133, in <module>
    val_label = val[:, -1]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
folder input exist... cleaning dictionary
folder pred exist... cleaning dictionary
folder Split_files exist... cleaning dictionary
Pre-trained CNN Error for file contig_0
Pre-trained CNN Error for file contig_1
Pre-trained CNN Error for file contig_2
Pre-trained CNN Error for file contig_3
Pre-trained CNN Error for file contig_4
Pre-trained CNN Error for file contig_5
Pre-trained CNN Error for file contig_6
Pre-trained CNN Error for file contig_7
Pre-trained CNN Error for file contig_8
Pre-trained CNN Error for file contig_9
Pre-trained CNN Error for file contig_10
Pre-trained CNN Error for file contig_11
Pre-trained CNN Error for file contig_13
Pre-trained CNN Error for file contig_14
Pre-trained CNN Error for file contig_15
Pre-trained CNN Error for file contig_16
Pre-trained CNN Error for file contig_17
Pre-trained CNN Error for file contig_18
Pre-trained CNN Error for file contig_25
Pre-trained CNN Error for file contig_27
Pre-trained CNN Error for file contig_31
Pre-trained CNN Error for file contig_37
Pre-trained CNN Error for file contig_38
Pre-trained CNN Error for file contig_39
Pre-trained CNN Error for file contig_40
Pre-trained CNN Error for file contig_41
Pre-trained CNN Error for file contig_42
Pre-trained CNN Error for file contig_43
Pre-trained CNN Error for file contig_44
Pre-trained CNN Error for file contig_45
Pre-trained CNN Error for file contig_46
Pre-trained CNN Error for file contig_47
Pre-trained CNN Error for file contig_48
Pre-trained CNN Error for file contig_49
Pre-trained CNN Error for file contig_50
Pre-trained CNN Error for file contig_51
Pre-trained CNN Error for file contig_52
Pre-trained CNN Error for file contig_53
Pre-trained CNN Error for file contig_54
Pre-trained CNN Error for file contig_55
